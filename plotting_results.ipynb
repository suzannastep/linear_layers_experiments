{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b21cba",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#imports\" data-toc-modified-id=\"imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>imports</a></span></li><li><span><a href=\"#load-data\" data-toc-modified-id=\"load-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#read-in-the-files\" data-toc-modified-id=\"read-in-the-files-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>read in the files</a></span><ul class=\"toc-item\"><li><span><a href=\"#data-from-linear-and-relu-activations\" data-toc-modified-id=\"data-from-linear-and-relu-activations-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>data from linear and relu activations</a></span></li></ul></li><li><span><a href=\"#create-pandas-table\" data-toc-modified-id=\"create-pandas-table-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>create pandas table</a></span></li></ul></li><li><span><a href=\"#filter-out-bad-training-losses\" data-toc-modified-id=\"filter-out-bad-training-losses-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>filter out bad training losses</a></span></li><li><span><a href=\"#determine-the-lambda-parameter-that-gets-the-best-validation-MSE-for-each-(r,n,L)\" data-toc-modified-id=\"determine-the-lambda-parameter-that-gets-the-best-test-MSE-for-each-(r,n,L)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>determine the lambda parameter that gets the best test MSE for each (r,n,L)</a></span></li><li><span><a href=\"#Generalization-MSE\" data-toc-modified-id=\"Generalization-MSE-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Generalization MSE</a></span><ul class=\"toc-item\"><li><span><a href=\"#generate-data\" data-toc-modified-id=\"generate-data-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>generate data</a></span></li><li><span><a href=\"#compute-MSE\" data-toc-modified-id=\"compute-MSE-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>compute MSE</a></span></li></ul></li><li><span><a href=\"#Out-of-Distribution-MSE\" data-toc-modified-id=\"Out-of-Distribution-MSE-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Out of Distribution MSE</a></span><ul class=\"toc-item\"><li><span><a href=\"#generate-data\" data-toc-modified-id=\"generate-data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>generate data</a></span></li><li><span><a href=\"#compute-MSE\" data-toc-modified-id=\"compute-MSE-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>compute MSE</a></span></li></ul></li><li><span><a href=\"#Active-Subspace\" data-toc-modified-id=\"Active-Subspace-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Active Subspace</a></span><ul class=\"toc-item\"><li><span><a href=\"#evaluate-gradients-and-compute-singular-values-and-active-subspaces\" data-toc-modified-id=\"evaluate-gradients-and-compute-singular-values-and-active-subspaces-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>evaluate gradients and compute singular values and active subspaces</a></span></li><li><span><a href=\"#plot-of-singular-values\" data-toc-modified-id=\"plot-of-singular-values-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>plot of singular values</a></span></li></ul></li><li><span><a href=\"#determine-the-L-parameter-that-gets-the-best-test-MSE-for-each-(r,n)\" data-toc-modified-id=\"determine-the-L-parameter-that-gets-the-best-test-MSE-for-each-(r,n)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>determine the L parameter that gets the best test MSE for each (r,n)</a></span></li><li><span><a href=\"#Plots-of-L-vs-Test-error-and-n-vs-Generalization-metrics-with/without-linear-layers\" data-toc-modified-id=\"Plots-of-L-vs-Validation-error-and-n-vs-Generalization-metrics-with/without-linear-layers-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Plots of L vs Test error and n vs Generalization metrics with/without linear layers</a></span></li><li><span><a href=\"#Final-Table\" data-toc-modified-id=\"Final-Table-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Final Table</a></span></li><li><span><a href=\"#Training-Time-Plots\" data-toc-modified-id=\"Training-Time-Plots-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Training Time Plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-MSE-v-Epoch\" data-toc-modified-id=\"Train-MSE-v-Epoch-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Train MSE v Epoch</a></span></li><li><span><a href=\"#Weight-Decay-v-Epoch\" data-toc-modified-id=\"Weight-Decay-v-Epoch-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Weight Decay v Epoch</a></span></li><li><span><a href=\"#learning-rates\" data-toc-modified-id=\"learning-rates-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>learning rates</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bF8A36cglU4",
   "metadata": {
    "id": "4bF8A36cglU4"
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2cca449a",
   "metadata": {
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1701128286405,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "2cca449a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.stats import linregress\n",
    "from scipy import linalg as la\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import sem\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21725609",
   "metadata": {
    "id": "21725609"
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aHdpcWWgi3RZ",
   "metadata": {
    "id": "aHdpcWWgi3RZ"
   },
   "source": [
    "## read in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "s2Q_uTMFlmlR",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "s2Q_uTMFlmlR"
   },
   "outputs": [],
   "source": [
    "rnvals = [(1,64),(1,128),(1,256),(1,512),(1,1024),(1,2048),\n",
    "          (2,64),(2,128),(2,256),(2,512),(2,1024),(2,2048)]\n",
    "Ls = [2,3,4,5,6,7,8,9]\n",
    "rs = [1,2]\n",
    "wds = [1e-3,1e-4,1e-5]\n",
    "labelnoise = [0,0.25,0.5,1]\n",
    "epochs = 60100\n",
    "job_name = \"new_targets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d588a3c2",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "d588a3c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testMSEs = {}\n",
    "trainMSEs = {}\n",
    "weightdecays = {}\n",
    "learningrates = {}\n",
    "files_found_list = []\n",
    "for r,n in rnvals:\n",
    "    for L in Ls:\n",
    "        for wd in wds:\n",
    "            for ln in labelnoise:\n",
    "                paramname = job_name+f\"_labelnoise{ln}\"+f\"/N{n}_L{L}_r{r}_wd{wd}_epochs{epochs}\"\n",
    "                if os.path.exists(paramname+\"testMSE.npy\"):\n",
    "                    testMSEs[r,n,L,wd,ln] = np.load(paramname+\"testMSE.npy\",allow_pickle=True).item()\n",
    "                    trainMSEs[r,n,L,wd,ln] = np.load(paramname+\"trainMSEs.npy\",allow_pickle=True)\n",
    "                    weightdecays[r,n,L,wd,ln] = np.load(paramname+\"weightdecays.npy\",allow_pickle=True)\n",
    "                    learningrates[r,n,L,wd,ln] = np.load(paramname+\"learningrates.npy\",allow_pickle=True)\n",
    "                    files_found_list.append((r,n,L,wd,ln))\n",
    "                else:\n",
    "                    print(f\"{paramname+'testMSE.npy'} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a9ca93e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 64, 2, 0.001, 0),\n",
       " (1, 64, 2, 0.001, 0.25),\n",
       " (1, 64, 2, 0.001, 0.5),\n",
       " (1, 64, 2, 0.001, 1),\n",
       " (1, 64, 2, 0.0001, 0),\n",
       " (1, 64, 2, 0.0001, 0.25),\n",
       " (1, 64, 2, 0.0001, 0.5),\n",
       " (1, 64, 2, 0.0001, 1),\n",
       " (1, 64, 2, 1e-05, 0),\n",
       " (1, 64, 2, 1e-05, 0.25),\n",
       " (1, 64, 2, 1e-05, 0.5),\n",
       " (1, 64, 2, 1e-05, 1),\n",
       " (1, 64, 3, 0.001, 0),\n",
       " (1, 64, 3, 0.001, 0.25),\n",
       " (1, 64, 3, 0.001, 0.5),\n",
       " (1, 64, 3, 0.001, 1),\n",
       " (1, 64, 3, 0.0001, 0),\n",
       " (1, 64, 3, 0.0001, 0.25),\n",
       " (1, 64, 3, 0.0001, 0.5),\n",
       " (1, 64, 3, 0.0001, 1),\n",
       " (1, 64, 3, 1e-05, 0),\n",
       " (1, 64, 3, 1e-05, 0.25),\n",
       " (1, 64, 3, 1e-05, 0.5),\n",
       " (1, 64, 3, 1e-05, 1),\n",
       " (1, 64, 4, 0.001, 0),\n",
       " (1, 64, 4, 0.001, 0.25),\n",
       " (1, 64, 4, 0.001, 0.5),\n",
       " (1, 64, 4, 0.001, 1),\n",
       " (1, 64, 4, 0.0001, 0),\n",
       " (1, 64, 4, 0.0001, 0.25),\n",
       " (1, 64, 4, 0.0001, 0.5),\n",
       " (1, 64, 4, 0.0001, 1),\n",
       " (1, 64, 4, 1e-05, 0),\n",
       " (1, 64, 4, 1e-05, 0.25),\n",
       " (1, 64, 4, 1e-05, 0.5),\n",
       " (1, 64, 4, 1e-05, 1),\n",
       " (1, 64, 5, 0.001, 0),\n",
       " (1, 64, 5, 0.001, 0.25),\n",
       " (1, 64, 5, 0.001, 0.5),\n",
       " (1, 64, 5, 0.001, 1),\n",
       " (1, 64, 5, 0.0001, 0),\n",
       " (1, 64, 5, 0.0001, 0.25),\n",
       " (1, 64, 5, 0.0001, 0.5),\n",
       " (1, 64, 5, 0.0001, 1),\n",
       " (1, 64, 5, 1e-05, 0),\n",
       " (1, 64, 5, 1e-05, 0.25),\n",
       " (1, 64, 5, 1e-05, 0.5),\n",
       " (1, 64, 5, 1e-05, 1),\n",
       " (1, 64, 6, 0.001, 0),\n",
       " (1, 64, 6, 0.001, 0.25),\n",
       " (1, 64, 6, 0.001, 0.5),\n",
       " (1, 64, 6, 0.001, 1),\n",
       " (1, 64, 6, 0.0001, 0),\n",
       " (1, 64, 6, 0.0001, 0.25),\n",
       " (1, 64, 6, 0.0001, 0.5),\n",
       " (1, 64, 6, 0.0001, 1),\n",
       " (1, 64, 6, 1e-05, 0),\n",
       " (1, 64, 6, 1e-05, 0.25),\n",
       " (1, 64, 6, 1e-05, 0.5),\n",
       " (1, 64, 6, 1e-05, 1),\n",
       " (1, 64, 7, 0.001, 0),\n",
       " (1, 64, 7, 0.001, 0.25),\n",
       " (1, 64, 7, 0.001, 0.5),\n",
       " (1, 64, 7, 0.001, 1),\n",
       " (1, 64, 7, 0.0001, 0),\n",
       " (1, 64, 7, 0.0001, 0.25),\n",
       " (1, 64, 7, 0.0001, 0.5),\n",
       " (1, 64, 7, 0.0001, 1),\n",
       " (1, 64, 7, 1e-05, 0),\n",
       " (1, 64, 7, 1e-05, 0.25),\n",
       " (1, 64, 7, 1e-05, 0.5),\n",
       " (1, 64, 7, 1e-05, 1),\n",
       " (1, 64, 8, 0.001, 0),\n",
       " (1, 64, 8, 0.001, 0.25),\n",
       " (1, 64, 8, 0.001, 0.5),\n",
       " (1, 64, 8, 0.001, 1),\n",
       " (1, 64, 8, 0.0001, 0),\n",
       " (1, 64, 8, 0.0001, 0.25),\n",
       " (1, 64, 8, 0.0001, 0.5),\n",
       " (1, 64, 8, 0.0001, 1),\n",
       " (1, 64, 8, 1e-05, 0),\n",
       " (1, 64, 8, 1e-05, 0.25),\n",
       " (1, 64, 8, 1e-05, 0.5),\n",
       " (1, 64, 8, 1e-05, 1),\n",
       " (1, 64, 9, 0.001, 0),\n",
       " (1, 64, 9, 0.001, 0.25),\n",
       " (1, 64, 9, 0.001, 0.5),\n",
       " (1, 64, 9, 0.001, 1),\n",
       " (1, 64, 9, 0.0001, 0),\n",
       " (1, 64, 9, 0.0001, 0.25),\n",
       " (1, 64, 9, 0.0001, 0.5),\n",
       " (1, 64, 9, 0.0001, 1),\n",
       " (1, 64, 9, 1e-05, 0),\n",
       " (1, 64, 9, 1e-05, 0.25),\n",
       " (1, 64, 9, 1e-05, 0.5),\n",
       " (1, 64, 9, 1e-05, 1),\n",
       " (1, 128, 2, 0.001, 0),\n",
       " (1, 128, 2, 0.001, 0.25),\n",
       " (1, 128, 2, 0.001, 0.5),\n",
       " (1, 128, 2, 0.001, 1),\n",
       " (1, 128, 2, 0.0001, 0),\n",
       " (1, 128, 2, 0.0001, 0.25),\n",
       " (1, 128, 2, 0.0001, 0.5),\n",
       " (1, 128, 2, 0.0001, 1),\n",
       " (1, 128, 2, 1e-05, 0),\n",
       " (1, 128, 2, 1e-05, 0.25),\n",
       " (1, 128, 2, 1e-05, 0.5),\n",
       " (1, 128, 2, 1e-05, 1),\n",
       " (1, 128, 3, 0.001, 0),\n",
       " (1, 128, 3, 0.001, 0.25),\n",
       " (1, 128, 3, 0.001, 0.5),\n",
       " (1, 128, 3, 0.001, 1),\n",
       " (1, 128, 3, 0.0001, 0),\n",
       " (1, 128, 3, 0.0001, 0.25),\n",
       " (1, 128, 3, 0.0001, 0.5),\n",
       " (1, 128, 3, 0.0001, 1),\n",
       " (1, 128, 3, 1e-05, 0),\n",
       " (1, 128, 3, 1e-05, 0.25),\n",
       " (1, 128, 3, 1e-05, 0.5),\n",
       " (1, 128, 3, 1e-05, 1),\n",
       " (1, 128, 4, 0.001, 0),\n",
       " (1, 128, 4, 0.001, 0.25),\n",
       " (1, 128, 4, 0.001, 0.5),\n",
       " (1, 128, 4, 0.001, 1),\n",
       " (1, 128, 4, 0.0001, 0),\n",
       " (1, 128, 4, 0.0001, 0.25),\n",
       " (1, 128, 4, 0.0001, 0.5),\n",
       " (1, 128, 4, 0.0001, 1),\n",
       " (1, 128, 4, 1e-05, 0),\n",
       " (1, 128, 4, 1e-05, 0.25),\n",
       " (1, 128, 4, 1e-05, 0.5),\n",
       " (1, 128, 4, 1e-05, 1),\n",
       " (1, 128, 5, 0.001, 0),\n",
       " (1, 128, 5, 0.001, 0.25),\n",
       " (1, 128, 5, 0.001, 0.5),\n",
       " (1, 128, 5, 0.001, 1),\n",
       " (1, 128, 5, 0.0001, 0),\n",
       " (1, 128, 5, 0.0001, 0.25),\n",
       " (1, 128, 5, 0.0001, 0.5),\n",
       " (1, 128, 5, 0.0001, 1),\n",
       " (1, 128, 5, 1e-05, 0),\n",
       " (1, 128, 5, 1e-05, 0.25),\n",
       " (1, 128, 5, 1e-05, 0.5),\n",
       " (1, 128, 5, 1e-05, 1),\n",
       " (1, 128, 6, 0.001, 0),\n",
       " (1, 128, 6, 0.001, 0.25),\n",
       " (1, 128, 6, 0.001, 0.5),\n",
       " (1, 128, 6, 0.001, 1),\n",
       " (1, 128, 6, 0.0001, 0),\n",
       " (1, 128, 6, 0.0001, 0.25),\n",
       " (1, 128, 6, 0.0001, 0.5),\n",
       " (1, 128, 6, 0.0001, 1),\n",
       " (1, 128, 6, 1e-05, 0),\n",
       " (1, 128, 6, 1e-05, 0.25),\n",
       " (1, 128, 6, 1e-05, 0.5),\n",
       " (1, 128, 6, 1e-05, 1),\n",
       " (1, 128, 7, 0.001, 0),\n",
       " (1, 128, 7, 0.001, 0.25),\n",
       " (1, 128, 7, 0.001, 0.5),\n",
       " (1, 128, 7, 0.001, 1),\n",
       " (1, 128, 7, 0.0001, 0),\n",
       " (1, 128, 7, 0.0001, 0.25),\n",
       " (1, 128, 7, 0.0001, 0.5),\n",
       " (1, 128, 7, 0.0001, 1),\n",
       " (1, 128, 7, 1e-05, 0),\n",
       " (1, 128, 7, 1e-05, 0.25),\n",
       " (1, 128, 7, 1e-05, 0.5),\n",
       " (1, 128, 7, 1e-05, 1),\n",
       " (1, 128, 8, 0.001, 0),\n",
       " (1, 128, 8, 0.001, 0.25),\n",
       " (1, 128, 8, 0.001, 0.5),\n",
       " (1, 128, 8, 0.001, 1),\n",
       " (1, 128, 8, 0.0001, 0),\n",
       " (1, 128, 8, 0.0001, 0.25),\n",
       " (1, 128, 8, 0.0001, 0.5),\n",
       " (1, 128, 8, 0.0001, 1),\n",
       " (1, 128, 8, 1e-05, 0),\n",
       " (1, 128, 8, 1e-05, 0.25),\n",
       " (1, 128, 8, 1e-05, 0.5),\n",
       " (1, 128, 8, 1e-05, 1),\n",
       " (1, 128, 9, 0.001, 0),\n",
       " (1, 128, 9, 0.001, 0.25),\n",
       " (1, 128, 9, 0.001, 0.5),\n",
       " (1, 128, 9, 0.001, 1),\n",
       " (1, 128, 9, 0.0001, 0),\n",
       " (1, 128, 9, 0.0001, 0.25),\n",
       " (1, 128, 9, 0.0001, 0.5),\n",
       " (1, 128, 9, 0.0001, 1),\n",
       " (1, 128, 9, 1e-05, 0),\n",
       " (1, 128, 9, 1e-05, 0.25),\n",
       " (1, 128, 9, 1e-05, 0.5),\n",
       " (1, 128, 9, 1e-05, 1),\n",
       " (1, 256, 2, 0.001, 0),\n",
       " (1, 256, 2, 0.001, 0.25),\n",
       " (1, 256, 2, 0.001, 0.5),\n",
       " (1, 256, 2, 0.001, 1),\n",
       " (1, 256, 2, 0.0001, 0),\n",
       " (1, 256, 2, 0.0001, 0.25),\n",
       " (1, 256, 2, 0.0001, 0.5),\n",
       " (1, 256, 2, 0.0001, 1),\n",
       " (1, 256, 2, 1e-05, 0),\n",
       " (1, 256, 2, 1e-05, 0.25),\n",
       " (1, 256, 2, 1e-05, 0.5),\n",
       " (1, 256, 2, 1e-05, 1),\n",
       " (1, 256, 3, 0.001, 0),\n",
       " (1, 256, 3, 0.001, 0.25),\n",
       " (1, 256, 3, 0.001, 0.5),\n",
       " (1, 256, 3, 0.001, 1),\n",
       " (1, 256, 3, 0.0001, 0),\n",
       " (1, 256, 3, 0.0001, 0.25),\n",
       " (1, 256, 3, 0.0001, 0.5),\n",
       " (1, 256, 3, 0.0001, 1),\n",
       " (1, 256, 3, 1e-05, 0),\n",
       " (1, 256, 3, 1e-05, 0.25),\n",
       " (1, 256, 3, 1e-05, 0.5),\n",
       " (1, 256, 3, 1e-05, 1),\n",
       " (1, 256, 4, 0.001, 0),\n",
       " (1, 256, 4, 0.001, 0.25),\n",
       " (1, 256, 4, 0.001, 0.5),\n",
       " (1, 256, 4, 0.001, 1),\n",
       " (1, 256, 4, 0.0001, 0),\n",
       " (1, 256, 4, 0.0001, 0.25),\n",
       " (1, 256, 4, 0.0001, 0.5),\n",
       " (1, 256, 4, 0.0001, 1),\n",
       " (1, 256, 4, 1e-05, 0),\n",
       " (1, 256, 4, 1e-05, 0.25),\n",
       " (1, 256, 4, 1e-05, 0.5),\n",
       " (1, 256, 4, 1e-05, 1),\n",
       " (1, 256, 5, 0.001, 0),\n",
       " (1, 256, 5, 0.001, 0.25),\n",
       " (1, 256, 5, 0.001, 0.5),\n",
       " (1, 256, 5, 0.001, 1),\n",
       " (1, 256, 5, 0.0001, 0),\n",
       " (1, 256, 5, 0.0001, 0.25),\n",
       " (1, 256, 5, 0.0001, 0.5),\n",
       " (1, 256, 5, 0.0001, 1),\n",
       " (1, 256, 5, 1e-05, 0),\n",
       " (1, 256, 5, 1e-05, 0.25),\n",
       " (1, 256, 5, 1e-05, 0.5),\n",
       " (1, 256, 5, 1e-05, 1),\n",
       " (1, 256, 6, 0.001, 0),\n",
       " (1, 256, 6, 0.001, 0.25),\n",
       " (1, 256, 6, 0.001, 0.5),\n",
       " (1, 256, 6, 0.001, 1),\n",
       " (1, 256, 6, 0.0001, 0),\n",
       " (1, 256, 6, 0.0001, 0.25),\n",
       " (1, 256, 6, 0.0001, 0.5),\n",
       " (1, 256, 6, 0.0001, 1),\n",
       " (1, 256, 6, 1e-05, 0),\n",
       " (1, 256, 6, 1e-05, 0.25),\n",
       " (1, 256, 6, 1e-05, 0.5),\n",
       " (1, 256, 6, 1e-05, 1),\n",
       " (1, 256, 7, 0.001, 0),\n",
       " (1, 256, 7, 0.001, 0.25),\n",
       " (1, 256, 7, 0.001, 0.5),\n",
       " (1, 256, 7, 0.001, 1),\n",
       " (1, 256, 7, 0.0001, 0),\n",
       " (1, 256, 7, 0.0001, 0.25),\n",
       " (1, 256, 7, 0.0001, 0.5),\n",
       " (1, 256, 7, 0.0001, 1),\n",
       " (1, 256, 7, 1e-05, 0),\n",
       " (1, 256, 7, 1e-05, 0.25),\n",
       " (1, 256, 7, 1e-05, 0.5),\n",
       " (1, 256, 7, 1e-05, 1),\n",
       " (1, 256, 8, 0.001, 0),\n",
       " (1, 256, 8, 0.001, 0.25),\n",
       " (1, 256, 8, 0.001, 0.5),\n",
       " (1, 256, 8, 0.001, 1),\n",
       " (1, 256, 8, 0.0001, 0),\n",
       " (1, 256, 8, 0.0001, 0.25),\n",
       " (1, 256, 8, 0.0001, 0.5),\n",
       " (1, 256, 8, 0.0001, 1),\n",
       " (1, 256, 8, 1e-05, 0),\n",
       " (1, 256, 8, 1e-05, 0.25),\n",
       " (1, 256, 8, 1e-05, 0.5),\n",
       " (1, 256, 8, 1e-05, 1),\n",
       " (1, 256, 9, 0.001, 0),\n",
       " (1, 256, 9, 0.001, 0.25),\n",
       " (1, 256, 9, 0.001, 0.5),\n",
       " (1, 256, 9, 0.001, 1),\n",
       " (1, 256, 9, 0.0001, 0),\n",
       " (1, 256, 9, 0.0001, 0.25),\n",
       " (1, 256, 9, 0.0001, 0.5),\n",
       " (1, 256, 9, 0.0001, 1),\n",
       " (1, 256, 9, 1e-05, 0),\n",
       " (1, 256, 9, 1e-05, 0.25),\n",
       " (1, 256, 9, 1e-05, 0.5),\n",
       " (1, 256, 9, 1e-05, 1),\n",
       " (1, 512, 2, 0.001, 0),\n",
       " (1, 512, 2, 0.001, 0.25),\n",
       " (1, 512, 2, 0.001, 0.5),\n",
       " (1, 512, 2, 0.001, 1),\n",
       " (1, 512, 2, 0.0001, 0),\n",
       " (1, 512, 2, 0.0001, 0.25),\n",
       " (1, 512, 2, 0.0001, 0.5),\n",
       " (1, 512, 2, 0.0001, 1),\n",
       " (1, 512, 2, 1e-05, 0),\n",
       " (1, 512, 2, 1e-05, 0.25),\n",
       " (1, 512, 2, 1e-05, 0.5),\n",
       " (1, 512, 2, 1e-05, 1),\n",
       " (1, 512, 3, 0.001, 0),\n",
       " (1, 512, 3, 0.001, 0.25),\n",
       " (1, 512, 3, 0.001, 0.5),\n",
       " (1, 512, 3, 0.001, 1),\n",
       " (1, 512, 3, 0.0001, 0),\n",
       " (1, 512, 3, 0.0001, 0.25),\n",
       " (1, 512, 3, 0.0001, 0.5),\n",
       " (1, 512, 3, 0.0001, 1),\n",
       " (1, 512, 3, 1e-05, 0),\n",
       " (1, 512, 3, 1e-05, 0.25),\n",
       " (1, 512, 3, 1e-05, 0.5),\n",
       " (1, 512, 3, 1e-05, 1),\n",
       " (1, 512, 4, 0.001, 0),\n",
       " (1, 512, 4, 0.001, 0.25),\n",
       " (1, 512, 4, 0.001, 0.5),\n",
       " (1, 512, 4, 0.001, 1),\n",
       " (1, 512, 4, 0.0001, 0),\n",
       " (1, 512, 4, 0.0001, 0.25),\n",
       " (1, 512, 4, 0.0001, 0.5),\n",
       " (1, 512, 4, 0.0001, 1),\n",
       " (1, 512, 4, 1e-05, 0),\n",
       " (1, 512, 4, 1e-05, 0.25),\n",
       " (1, 512, 4, 1e-05, 0.5),\n",
       " (1, 512, 4, 1e-05, 1),\n",
       " (1, 512, 5, 0.001, 0),\n",
       " (1, 512, 5, 0.001, 0.25),\n",
       " (1, 512, 5, 0.001, 0.5),\n",
       " (1, 512, 5, 0.001, 1),\n",
       " (1, 512, 5, 0.0001, 0),\n",
       " (1, 512, 5, 0.0001, 0.25),\n",
       " (1, 512, 5, 0.0001, 0.5),\n",
       " (1, 512, 5, 0.0001, 1),\n",
       " (1, 512, 5, 1e-05, 0),\n",
       " (1, 512, 5, 1e-05, 0.25),\n",
       " (1, 512, 5, 1e-05, 0.5),\n",
       " (1, 512, 5, 1e-05, 1),\n",
       " (1, 512, 6, 0.001, 0),\n",
       " (1, 512, 6, 0.001, 0.25),\n",
       " (1, 512, 6, 0.001, 0.5),\n",
       " (1, 512, 6, 0.001, 1),\n",
       " (1, 512, 6, 0.0001, 0),\n",
       " (1, 512, 6, 0.0001, 0.25),\n",
       " (1, 512, 6, 0.0001, 0.5),\n",
       " (1, 512, 6, 0.0001, 1),\n",
       " (1, 512, 6, 1e-05, 0),\n",
       " (1, 512, 6, 1e-05, 0.25),\n",
       " (1, 512, 6, 1e-05, 0.5),\n",
       " (1, 512, 6, 1e-05, 1),\n",
       " (1, 512, 7, 0.001, 0),\n",
       " (1, 512, 7, 0.001, 0.25),\n",
       " (1, 512, 7, 0.001, 0.5),\n",
       " (1, 512, 7, 0.001, 1),\n",
       " (1, 512, 7, 0.0001, 0),\n",
       " (1, 512, 7, 0.0001, 0.25),\n",
       " (1, 512, 7, 0.0001, 0.5),\n",
       " (1, 512, 7, 0.0001, 1),\n",
       " (1, 512, 7, 1e-05, 0),\n",
       " (1, 512, 7, 1e-05, 0.25),\n",
       " (1, 512, 7, 1e-05, 0.5),\n",
       " (1, 512, 7, 1e-05, 1),\n",
       " (1, 512, 8, 0.001, 0),\n",
       " (1, 512, 8, 0.001, 0.25),\n",
       " (1, 512, 8, 0.001, 0.5),\n",
       " (1, 512, 8, 0.001, 1),\n",
       " (1, 512, 8, 0.0001, 0),\n",
       " (1, 512, 8, 0.0001, 0.25),\n",
       " (1, 512, 8, 0.0001, 0.5),\n",
       " (1, 512, 8, 0.0001, 1),\n",
       " (1, 512, 8, 1e-05, 0),\n",
       " (1, 512, 8, 1e-05, 0.25),\n",
       " (1, 512, 8, 1e-05, 0.5),\n",
       " (1, 512, 8, 1e-05, 1),\n",
       " (1, 512, 9, 0.001, 0),\n",
       " (1, 512, 9, 0.001, 0.25),\n",
       " (1, 512, 9, 0.001, 0.5),\n",
       " (1, 512, 9, 0.001, 1),\n",
       " (1, 512, 9, 0.0001, 0),\n",
       " (1, 512, 9, 0.0001, 0.25),\n",
       " (1, 512, 9, 0.0001, 0.5),\n",
       " (1, 512, 9, 0.0001, 1),\n",
       " (1, 512, 9, 1e-05, 0),\n",
       " (1, 512, 9, 1e-05, 0.25),\n",
       " (1, 512, 9, 1e-05, 0.5),\n",
       " (1, 512, 9, 1e-05, 1),\n",
       " (1, 1024, 2, 0.001, 0),\n",
       " (1, 1024, 2, 0.001, 0.25),\n",
       " (1, 1024, 2, 0.001, 0.5),\n",
       " (1, 1024, 2, 0.001, 1),\n",
       " (1, 1024, 2, 0.0001, 0),\n",
       " (1, 1024, 2, 0.0001, 0.25),\n",
       " (1, 1024, 2, 0.0001, 0.5),\n",
       " (1, 1024, 2, 0.0001, 1),\n",
       " (1, 1024, 2, 1e-05, 0),\n",
       " (1, 1024, 2, 1e-05, 0.25),\n",
       " (1, 1024, 2, 1e-05, 0.5),\n",
       " (1, 1024, 2, 1e-05, 1),\n",
       " (1, 1024, 3, 0.001, 0),\n",
       " (1, 1024, 3, 0.001, 0.25),\n",
       " (1, 1024, 3, 0.001, 0.5),\n",
       " (1, 1024, 3, 0.001, 1),\n",
       " (1, 1024, 3, 0.0001, 0),\n",
       " (1, 1024, 3, 0.0001, 0.25),\n",
       " (1, 1024, 3, 0.0001, 0.5),\n",
       " (1, 1024, 3, 0.0001, 1),\n",
       " (1, 1024, 3, 1e-05, 0),\n",
       " (1, 1024, 3, 1e-05, 0.25),\n",
       " (1, 1024, 3, 1e-05, 0.5),\n",
       " (1, 1024, 3, 1e-05, 1),\n",
       " (1, 1024, 4, 0.001, 0),\n",
       " (1, 1024, 4, 0.001, 0.25),\n",
       " (1, 1024, 4, 0.001, 0.5),\n",
       " (1, 1024, 4, 0.001, 1),\n",
       " (1, 1024, 4, 0.0001, 0),\n",
       " (1, 1024, 4, 0.0001, 0.25),\n",
       " (1, 1024, 4, 0.0001, 0.5),\n",
       " (1, 1024, 4, 0.0001, 1),\n",
       " (1, 1024, 4, 1e-05, 0),\n",
       " (1, 1024, 4, 1e-05, 0.25),\n",
       " (1, 1024, 4, 1e-05, 0.5),\n",
       " (1, 1024, 4, 1e-05, 1),\n",
       " (1, 1024, 5, 0.001, 0),\n",
       " (1, 1024, 5, 0.001, 0.25),\n",
       " (1, 1024, 5, 0.001, 0.5),\n",
       " (1, 1024, 5, 0.001, 1),\n",
       " (1, 1024, 5, 0.0001, 0),\n",
       " (1, 1024, 5, 0.0001, 0.25),\n",
       " (1, 1024, 5, 0.0001, 0.5),\n",
       " (1, 1024, 5, 0.0001, 1),\n",
       " (1, 1024, 5, 1e-05, 0),\n",
       " (1, 1024, 5, 1e-05, 0.25),\n",
       " (1, 1024, 5, 1e-05, 0.5),\n",
       " (1, 1024, 5, 1e-05, 1),\n",
       " (1, 1024, 6, 0.001, 0),\n",
       " (1, 1024, 6, 0.001, 0.25),\n",
       " (1, 1024, 6, 0.001, 0.5),\n",
       " (1, 1024, 6, 0.001, 1),\n",
       " (1, 1024, 6, 0.0001, 0),\n",
       " (1, 1024, 6, 0.0001, 0.25),\n",
       " (1, 1024, 6, 0.0001, 0.5),\n",
       " (1, 1024, 6, 0.0001, 1),\n",
       " (1, 1024, 6, 1e-05, 0),\n",
       " (1, 1024, 6, 1e-05, 0.25),\n",
       " (1, 1024, 6, 1e-05, 0.5),\n",
       " (1, 1024, 6, 1e-05, 1),\n",
       " (1, 1024, 7, 0.001, 0),\n",
       " (1, 1024, 7, 0.001, 0.25),\n",
       " (1, 1024, 7, 0.001, 0.5),\n",
       " (1, 1024, 7, 0.001, 1),\n",
       " (1, 1024, 7, 0.0001, 0),\n",
       " (1, 1024, 7, 0.0001, 0.25),\n",
       " (1, 1024, 7, 0.0001, 0.5),\n",
       " (1, 1024, 7, 0.0001, 1),\n",
       " (1, 1024, 7, 1e-05, 0),\n",
       " (1, 1024, 7, 1e-05, 0.25),\n",
       " (1, 1024, 7, 1e-05, 0.5),\n",
       " (1, 1024, 7, 1e-05, 1),\n",
       " (1, 1024, 8, 0.001, 0),\n",
       " (1, 1024, 8, 0.001, 0.25),\n",
       " (1, 1024, 8, 0.001, 0.5),\n",
       " (1, 1024, 8, 0.001, 1),\n",
       " (1, 1024, 8, 0.0001, 0),\n",
       " (1, 1024, 8, 0.0001, 0.25),\n",
       " (1, 1024, 8, 0.0001, 0.5),\n",
       " (1, 1024, 8, 0.0001, 1),\n",
       " (1, 1024, 8, 1e-05, 0),\n",
       " (1, 1024, 8, 1e-05, 0.25),\n",
       " (1, 1024, 8, 1e-05, 0.5),\n",
       " (1, 1024, 8, 1e-05, 1),\n",
       " (1, 1024, 9, 0.001, 0),\n",
       " (1, 1024, 9, 0.001, 0.25),\n",
       " (1, 1024, 9, 0.001, 0.5),\n",
       " (1, 1024, 9, 0.001, 1),\n",
       " (1, 1024, 9, 0.0001, 0),\n",
       " (1, 1024, 9, 0.0001, 0.25),\n",
       " (1, 1024, 9, 0.0001, 0.5),\n",
       " (1, 1024, 9, 0.0001, 1),\n",
       " (1, 1024, 9, 1e-05, 0),\n",
       " (1, 1024, 9, 1e-05, 0.25),\n",
       " (1, 1024, 9, 1e-05, 0.5),\n",
       " (1, 1024, 9, 1e-05, 1),\n",
       " (1, 2048, 2, 0.001, 0),\n",
       " (1, 2048, 2, 0.001, 0.25),\n",
       " (1, 2048, 2, 0.001, 0.5),\n",
       " (1, 2048, 2, 0.001, 1),\n",
       " (1, 2048, 2, 0.0001, 0),\n",
       " (1, 2048, 2, 0.0001, 0.25),\n",
       " (1, 2048, 2, 0.0001, 0.5),\n",
       " (1, 2048, 2, 0.0001, 1),\n",
       " (1, 2048, 2, 1e-05, 0),\n",
       " (1, 2048, 2, 1e-05, 0.25),\n",
       " (1, 2048, 2, 1e-05, 0.5),\n",
       " (1, 2048, 2, 1e-05, 1),\n",
       " (1, 2048, 3, 0.001, 0),\n",
       " (1, 2048, 3, 0.001, 0.25),\n",
       " (1, 2048, 3, 0.001, 0.5),\n",
       " (1, 2048, 3, 0.001, 1),\n",
       " (1, 2048, 3, 0.0001, 0),\n",
       " (1, 2048, 3, 0.0001, 0.25),\n",
       " (1, 2048, 3, 0.0001, 0.5),\n",
       " (1, 2048, 3, 0.0001, 1),\n",
       " (1, 2048, 3, 1e-05, 0),\n",
       " (1, 2048, 3, 1e-05, 0.25),\n",
       " (1, 2048, 3, 1e-05, 0.5),\n",
       " (1, 2048, 3, 1e-05, 1),\n",
       " (1, 2048, 4, 0.001, 0),\n",
       " (1, 2048, 4, 0.001, 0.25),\n",
       " (1, 2048, 4, 0.001, 0.5),\n",
       " (1, 2048, 4, 0.001, 1),\n",
       " (1, 2048, 4, 0.0001, 0),\n",
       " (1, 2048, 4, 0.0001, 0.25),\n",
       " (1, 2048, 4, 0.0001, 0.5),\n",
       " (1, 2048, 4, 0.0001, 1),\n",
       " (1, 2048, 4, 1e-05, 0),\n",
       " (1, 2048, 4, 1e-05, 0.25),\n",
       " (1, 2048, 4, 1e-05, 0.5),\n",
       " (1, 2048, 4, 1e-05, 1),\n",
       " (1, 2048, 5, 0.001, 0),\n",
       " (1, 2048, 5, 0.001, 0.25),\n",
       " (1, 2048, 5, 0.001, 0.5),\n",
       " (1, 2048, 5, 0.001, 1),\n",
       " (1, 2048, 5, 0.0001, 0),\n",
       " (1, 2048, 5, 0.0001, 0.25),\n",
       " (1, 2048, 5, 0.0001, 0.5),\n",
       " (1, 2048, 5, 0.0001, 1),\n",
       " (1, 2048, 5, 1e-05, 0),\n",
       " (1, 2048, 5, 1e-05, 0.25),\n",
       " (1, 2048, 5, 1e-05, 0.5),\n",
       " (1, 2048, 5, 1e-05, 1),\n",
       " (1, 2048, 6, 0.001, 0),\n",
       " (1, 2048, 6, 0.001, 0.25),\n",
       " (1, 2048, 6, 0.001, 0.5),\n",
       " (1, 2048, 6, 0.001, 1),\n",
       " (1, 2048, 6, 0.0001, 0),\n",
       " (1, 2048, 6, 0.0001, 0.25),\n",
       " (1, 2048, 6, 0.0001, 0.5),\n",
       " (1, 2048, 6, 0.0001, 1),\n",
       " (1, 2048, 6, 1e-05, 0),\n",
       " (1, 2048, 6, 1e-05, 0.25),\n",
       " (1, 2048, 6, 1e-05, 0.5),\n",
       " (1, 2048, 6, 1e-05, 1),\n",
       " (1, 2048, 7, 0.001, 0),\n",
       " (1, 2048, 7, 0.001, 0.25),\n",
       " (1, 2048, 7, 0.001, 0.5),\n",
       " (1, 2048, 7, 0.001, 1),\n",
       " (1, 2048, 7, 0.0001, 0),\n",
       " (1, 2048, 7, 0.0001, 0.25),\n",
       " (1, 2048, 7, 0.0001, 0.5),\n",
       " (1, 2048, 7, 0.0001, 1),\n",
       " (1, 2048, 7, 1e-05, 0),\n",
       " (1, 2048, 7, 1e-05, 0.25),\n",
       " (1, 2048, 7, 1e-05, 0.5),\n",
       " (1, 2048, 7, 1e-05, 1),\n",
       " (1, 2048, 8, 0.001, 0),\n",
       " (1, 2048, 8, 0.001, 0.25),\n",
       " (1, 2048, 8, 0.001, 0.5),\n",
       " (1, 2048, 8, 0.001, 1),\n",
       " (1, 2048, 8, 0.0001, 0),\n",
       " (1, 2048, 8, 0.0001, 0.25),\n",
       " (1, 2048, 8, 0.0001, 0.5),\n",
       " (1, 2048, 8, 0.0001, 1),\n",
       " (1, 2048, 8, 1e-05, 0),\n",
       " (1, 2048, 8, 1e-05, 0.25),\n",
       " (1, 2048, 8, 1e-05, 0.5),\n",
       " (1, 2048, 8, 1e-05, 1),\n",
       " (1, 2048, 9, 0.001, 0),\n",
       " (1, 2048, 9, 0.001, 0.25),\n",
       " (1, 2048, 9, 0.001, 0.5),\n",
       " (1, 2048, 9, 0.001, 1),\n",
       " (1, 2048, 9, 0.0001, 0),\n",
       " (1, 2048, 9, 0.0001, 0.25),\n",
       " (1, 2048, 9, 0.0001, 0.5),\n",
       " (1, 2048, 9, 0.0001, 1),\n",
       " (1, 2048, 9, 1e-05, 0),\n",
       " (1, 2048, 9, 1e-05, 0.25),\n",
       " (1, 2048, 9, 1e-05, 0.5),\n",
       " (1, 2048, 9, 1e-05, 1),\n",
       " (2, 64, 2, 0.001, 0),\n",
       " (2, 64, 2, 0.001, 0.25),\n",
       " (2, 64, 2, 0.001, 0.5),\n",
       " (2, 64, 2, 0.001, 1),\n",
       " (2, 64, 2, 0.0001, 0),\n",
       " (2, 64, 2, 0.0001, 0.25),\n",
       " (2, 64, 2, 0.0001, 0.5),\n",
       " (2, 64, 2, 0.0001, 1),\n",
       " (2, 64, 2, 1e-05, 0),\n",
       " (2, 64, 2, 1e-05, 0.25),\n",
       " (2, 64, 2, 1e-05, 0.5),\n",
       " (2, 64, 2, 1e-05, 1),\n",
       " (2, 64, 3, 0.001, 0),\n",
       " (2, 64, 3, 0.001, 0.25),\n",
       " (2, 64, 3, 0.001, 0.5),\n",
       " (2, 64, 3, 0.001, 1),\n",
       " (2, 64, 3, 0.0001, 0),\n",
       " (2, 64, 3, 0.0001, 0.25),\n",
       " (2, 64, 3, 0.0001, 0.5),\n",
       " (2, 64, 3, 0.0001, 1),\n",
       " (2, 64, 3, 1e-05, 0),\n",
       " (2, 64, 3, 1e-05, 0.25),\n",
       " (2, 64, 3, 1e-05, 0.5),\n",
       " (2, 64, 3, 1e-05, 1),\n",
       " (2, 64, 4, 0.001, 0),\n",
       " (2, 64, 4, 0.001, 0.25),\n",
       " (2, 64, 4, 0.001, 0.5),\n",
       " (2, 64, 4, 0.001, 1),\n",
       " (2, 64, 4, 0.0001, 0),\n",
       " (2, 64, 4, 0.0001, 0.25),\n",
       " (2, 64, 4, 0.0001, 0.5),\n",
       " (2, 64, 4, 0.0001, 1),\n",
       " (2, 64, 4, 1e-05, 0),\n",
       " (2, 64, 4, 1e-05, 0.25),\n",
       " (2, 64, 4, 1e-05, 0.5),\n",
       " (2, 64, 4, 1e-05, 1),\n",
       " (2, 64, 5, 0.001, 0),\n",
       " (2, 64, 5, 0.001, 0.25),\n",
       " (2, 64, 5, 0.001, 0.5),\n",
       " (2, 64, 5, 0.001, 1),\n",
       " (2, 64, 5, 0.0001, 0),\n",
       " (2, 64, 5, 0.0001, 0.25),\n",
       " (2, 64, 5, 0.0001, 0.5),\n",
       " (2, 64, 5, 0.0001, 1),\n",
       " (2, 64, 5, 1e-05, 0),\n",
       " (2, 64, 5, 1e-05, 0.25),\n",
       " (2, 64, 5, 1e-05, 0.5),\n",
       " (2, 64, 5, 1e-05, 1),\n",
       " (2, 64, 6, 0.001, 0),\n",
       " (2, 64, 6, 0.001, 0.25),\n",
       " (2, 64, 6, 0.001, 0.5),\n",
       " (2, 64, 6, 0.001, 1),\n",
       " (2, 64, 6, 0.0001, 0),\n",
       " (2, 64, 6, 0.0001, 0.25),\n",
       " (2, 64, 6, 0.0001, 0.5),\n",
       " (2, 64, 6, 0.0001, 1),\n",
       " (2, 64, 6, 1e-05, 0),\n",
       " (2, 64, 6, 1e-05, 0.25),\n",
       " (2, 64, 6, 1e-05, 0.5),\n",
       " (2, 64, 6, 1e-05, 1),\n",
       " (2, 64, 7, 0.001, 0),\n",
       " (2, 64, 7, 0.001, 0.25),\n",
       " (2, 64, 7, 0.001, 0.5),\n",
       " (2, 64, 7, 0.001, 1),\n",
       " (2, 64, 7, 0.0001, 0),\n",
       " (2, 64, 7, 0.0001, 0.25),\n",
       " (2, 64, 7, 0.0001, 0.5),\n",
       " (2, 64, 7, 0.0001, 1),\n",
       " (2, 64, 7, 1e-05, 0),\n",
       " (2, 64, 7, 1e-05, 0.25),\n",
       " (2, 64, 7, 1e-05, 0.5),\n",
       " (2, 64, 7, 1e-05, 1),\n",
       " (2, 64, 8, 0.001, 0),\n",
       " (2, 64, 8, 0.001, 0.25),\n",
       " (2, 64, 8, 0.001, 0.5),\n",
       " (2, 64, 8, 0.001, 1),\n",
       " (2, 64, 8, 0.0001, 0),\n",
       " (2, 64, 8, 0.0001, 0.25),\n",
       " (2, 64, 8, 0.0001, 0.5),\n",
       " (2, 64, 8, 0.0001, 1),\n",
       " (2, 64, 8, 1e-05, 0),\n",
       " (2, 64, 8, 1e-05, 0.25),\n",
       " (2, 64, 8, 1e-05, 0.5),\n",
       " (2, 64, 8, 1e-05, 1),\n",
       " (2, 64, 9, 0.001, 0),\n",
       " (2, 64, 9, 0.001, 0.25),\n",
       " (2, 64, 9, 0.001, 0.5),\n",
       " (2, 64, 9, 0.001, 1),\n",
       " (2, 64, 9, 0.0001, 0),\n",
       " (2, 64, 9, 0.0001, 0.25),\n",
       " (2, 64, 9, 0.0001, 0.5),\n",
       " (2, 64, 9, 0.0001, 1),\n",
       " (2, 64, 9, 1e-05, 0),\n",
       " (2, 64, 9, 1e-05, 0.25),\n",
       " (2, 64, 9, 1e-05, 0.5),\n",
       " (2, 64, 9, 1e-05, 1),\n",
       " (2, 128, 2, 0.001, 0),\n",
       " (2, 128, 2, 0.001, 0.25),\n",
       " (2, 128, 2, 0.001, 0.5),\n",
       " (2, 128, 2, 0.001, 1),\n",
       " (2, 128, 2, 0.0001, 0),\n",
       " (2, 128, 2, 0.0001, 0.25),\n",
       " (2, 128, 2, 0.0001, 0.5),\n",
       " (2, 128, 2, 0.0001, 1),\n",
       " (2, 128, 2, 1e-05, 0),\n",
       " (2, 128, 2, 1e-05, 0.25),\n",
       " (2, 128, 2, 1e-05, 0.5),\n",
       " (2, 128, 2, 1e-05, 1),\n",
       " (2, 128, 3, 0.001, 0),\n",
       " (2, 128, 3, 0.001, 0.25),\n",
       " (2, 128, 3, 0.001, 0.5),\n",
       " (2, 128, 3, 0.001, 1),\n",
       " (2, 128, 3, 0.0001, 0),\n",
       " (2, 128, 3, 0.0001, 0.25),\n",
       " (2, 128, 3, 0.0001, 0.5),\n",
       " (2, 128, 3, 0.0001, 1),\n",
       " (2, 128, 3, 1e-05, 0),\n",
       " (2, 128, 3, 1e-05, 0.25),\n",
       " (2, 128, 3, 1e-05, 0.5),\n",
       " (2, 128, 3, 1e-05, 1),\n",
       " (2, 128, 4, 0.001, 0),\n",
       " (2, 128, 4, 0.001, 0.25),\n",
       " (2, 128, 4, 0.001, 0.5),\n",
       " (2, 128, 4, 0.001, 1),\n",
       " (2, 128, 4, 0.0001, 0),\n",
       " (2, 128, 4, 0.0001, 0.25),\n",
       " (2, 128, 4, 0.0001, 0.5),\n",
       " (2, 128, 4, 0.0001, 1),\n",
       " (2, 128, 4, 1e-05, 0),\n",
       " (2, 128, 4, 1e-05, 0.25),\n",
       " (2, 128, 4, 1e-05, 0.5),\n",
       " (2, 128, 4, 1e-05, 1),\n",
       " (2, 128, 5, 0.001, 0),\n",
       " (2, 128, 5, 0.001, 0.25),\n",
       " (2, 128, 5, 0.001, 0.5),\n",
       " (2, 128, 5, 0.001, 1),\n",
       " (2, 128, 5, 0.0001, 0),\n",
       " (2, 128, 5, 0.0001, 0.25),\n",
       " (2, 128, 5, 0.0001, 0.5),\n",
       " (2, 128, 5, 0.0001, 1),\n",
       " (2, 128, 5, 1e-05, 0),\n",
       " (2, 128, 5, 1e-05, 0.25),\n",
       " (2, 128, 5, 1e-05, 0.5),\n",
       " (2, 128, 5, 1e-05, 1),\n",
       " (2, 128, 6, 0.001, 0),\n",
       " (2, 128, 6, 0.001, 0.25),\n",
       " (2, 128, 6, 0.001, 0.5),\n",
       " (2, 128, 6, 0.001, 1),\n",
       " (2, 128, 6, 0.0001, 0),\n",
       " (2, 128, 6, 0.0001, 0.25),\n",
       " (2, 128, 6, 0.0001, 0.5),\n",
       " (2, 128, 6, 0.0001, 1),\n",
       " (2, 128, 6, 1e-05, 0),\n",
       " (2, 128, 6, 1e-05, 0.25),\n",
       " (2, 128, 6, 1e-05, 0.5),\n",
       " (2, 128, 6, 1e-05, 1),\n",
       " (2, 128, 7, 0.001, 0),\n",
       " (2, 128, 7, 0.001, 0.25),\n",
       " (2, 128, 7, 0.001, 0.5),\n",
       " (2, 128, 7, 0.001, 1),\n",
       " (2, 128, 7, 0.0001, 0),\n",
       " (2, 128, 7, 0.0001, 0.25),\n",
       " (2, 128, 7, 0.0001, 0.5),\n",
       " (2, 128, 7, 0.0001, 1),\n",
       " (2, 128, 7, 1e-05, 0),\n",
       " (2, 128, 7, 1e-05, 0.25),\n",
       " (2, 128, 7, 1e-05, 0.5),\n",
       " (2, 128, 7, 1e-05, 1),\n",
       " (2, 128, 8, 0.001, 0),\n",
       " (2, 128, 8, 0.001, 0.25),\n",
       " (2, 128, 8, 0.001, 0.5),\n",
       " (2, 128, 8, 0.001, 1),\n",
       " (2, 128, 8, 0.0001, 0),\n",
       " (2, 128, 8, 0.0001, 0.25),\n",
       " (2, 128, 8, 0.0001, 0.5),\n",
       " (2, 128, 8, 0.0001, 1),\n",
       " (2, 128, 8, 1e-05, 0),\n",
       " (2, 128, 8, 1e-05, 0.25),\n",
       " (2, 128, 8, 1e-05, 0.5),\n",
       " (2, 128, 8, 1e-05, 1),\n",
       " (2, 128, 9, 0.001, 0),\n",
       " (2, 128, 9, 0.001, 0.25),\n",
       " (2, 128, 9, 0.001, 0.5),\n",
       " (2, 128, 9, 0.001, 1),\n",
       " (2, 128, 9, 0.0001, 0),\n",
       " (2, 128, 9, 0.0001, 0.25),\n",
       " (2, 128, 9, 0.0001, 0.5),\n",
       " (2, 128, 9, 0.0001, 1),\n",
       " (2, 128, 9, 1e-05, 0),\n",
       " (2, 128, 9, 1e-05, 0.25),\n",
       " (2, 128, 9, 1e-05, 0.5),\n",
       " (2, 128, 9, 1e-05, 1),\n",
       " (2, 256, 2, 0.001, 0),\n",
       " (2, 256, 2, 0.001, 0.25),\n",
       " (2, 256, 2, 0.001, 0.5),\n",
       " (2, 256, 2, 0.001, 1),\n",
       " (2, 256, 2, 0.0001, 0),\n",
       " (2, 256, 2, 0.0001, 0.25),\n",
       " (2, 256, 2, 0.0001, 0.5),\n",
       " (2, 256, 2, 0.0001, 1),\n",
       " (2, 256, 2, 1e-05, 0),\n",
       " (2, 256, 2, 1e-05, 0.25),\n",
       " (2, 256, 2, 1e-05, 0.5),\n",
       " (2, 256, 2, 1e-05, 1),\n",
       " (2, 256, 3, 0.001, 0),\n",
       " (2, 256, 3, 0.001, 0.25),\n",
       " (2, 256, 3, 0.001, 0.5),\n",
       " (2, 256, 3, 0.001, 1),\n",
       " (2, 256, 3, 0.0001, 0),\n",
       " (2, 256, 3, 0.0001, 0.25),\n",
       " (2, 256, 3, 0.0001, 0.5),\n",
       " (2, 256, 3, 0.0001, 1),\n",
       " (2, 256, 3, 1e-05, 0),\n",
       " (2, 256, 3, 1e-05, 0.25),\n",
       " (2, 256, 3, 1e-05, 0.5),\n",
       " (2, 256, 3, 1e-05, 1),\n",
       " (2, 256, 4, 0.001, 0),\n",
       " (2, 256, 4, 0.001, 0.25),\n",
       " (2, 256, 4, 0.001, 0.5),\n",
       " (2, 256, 4, 0.001, 1),\n",
       " (2, 256, 4, 0.0001, 0),\n",
       " (2, 256, 4, 0.0001, 0.25),\n",
       " (2, 256, 4, 0.0001, 0.5),\n",
       " (2, 256, 4, 0.0001, 1),\n",
       " (2, 256, 4, 1e-05, 0),\n",
       " (2, 256, 4, 1e-05, 0.25),\n",
       " (2, 256, 4, 1e-05, 0.5),\n",
       " (2, 256, 4, 1e-05, 1),\n",
       " (2, 256, 5, 0.001, 0),\n",
       " (2, 256, 5, 0.001, 0.25),\n",
       " (2, 256, 5, 0.001, 0.5),\n",
       " (2, 256, 5, 0.001, 1),\n",
       " (2, 256, 5, 0.0001, 0),\n",
       " (2, 256, 5, 0.0001, 0.25),\n",
       " (2, 256, 5, 0.0001, 0.5),\n",
       " (2, 256, 5, 0.0001, 1),\n",
       " (2, 256, 5, 1e-05, 0),\n",
       " (2, 256, 5, 1e-05, 0.25),\n",
       " (2, 256, 5, 1e-05, 0.5),\n",
       " (2, 256, 5, 1e-05, 1),\n",
       " (2, 256, 6, 0.001, 0),\n",
       " (2, 256, 6, 0.001, 0.25),\n",
       " (2, 256, 6, 0.001, 0.5),\n",
       " (2, 256, 6, 0.001, 1),\n",
       " (2, 256, 6, 0.0001, 0),\n",
       " (2, 256, 6, 0.0001, 0.25),\n",
       " (2, 256, 6, 0.0001, 0.5),\n",
       " (2, 256, 6, 0.0001, 1),\n",
       " (2, 256, 6, 1e-05, 0),\n",
       " (2, 256, 6, 1e-05, 0.25),\n",
       " (2, 256, 6, 1e-05, 0.5),\n",
       " (2, 256, 6, 1e-05, 1),\n",
       " (2, 256, 7, 0.001, 0),\n",
       " (2, 256, 7, 0.001, 0.25),\n",
       " (2, 256, 7, 0.001, 0.5),\n",
       " (2, 256, 7, 0.001, 1),\n",
       " (2, 256, 7, 0.0001, 0),\n",
       " (2, 256, 7, 0.0001, 0.25),\n",
       " (2, 256, 7, 0.0001, 0.5),\n",
       " (2, 256, 7, 0.0001, 1),\n",
       " (2, 256, 7, 1e-05, 0),\n",
       " (2, 256, 7, 1e-05, 0.25),\n",
       " (2, 256, 7, 1e-05, 0.5),\n",
       " (2, 256, 7, 1e-05, 1),\n",
       " (2, 256, 8, 0.001, 0),\n",
       " (2, 256, 8, 0.001, 0.25),\n",
       " (2, 256, 8, 0.001, 0.5),\n",
       " (2, 256, 8, 0.001, 1),\n",
       " (2, 256, 8, 0.0001, 0),\n",
       " (2, 256, 8, 0.0001, 0.25),\n",
       " (2, 256, 8, 0.0001, 0.5),\n",
       " (2, 256, 8, 0.0001, 1),\n",
       " (2, 256, 8, 1e-05, 0),\n",
       " (2, 256, 8, 1e-05, 0.25),\n",
       " (2, 256, 8, 1e-05, 0.5),\n",
       " (2, 256, 8, 1e-05, 1),\n",
       " (2, 256, 9, 0.001, 0),\n",
       " (2, 256, 9, 0.001, 0.25),\n",
       " (2, 256, 9, 0.001, 0.5),\n",
       " (2, 256, 9, 0.001, 1),\n",
       " (2, 256, 9, 0.0001, 0),\n",
       " (2, 256, 9, 0.0001, 0.25),\n",
       " (2, 256, 9, 0.0001, 0.5),\n",
       " (2, 256, 9, 0.0001, 1),\n",
       " (2, 256, 9, 1e-05, 0),\n",
       " (2, 256, 9, 1e-05, 0.25),\n",
       " (2, 256, 9, 1e-05, 0.5),\n",
       " (2, 256, 9, 1e-05, 1),\n",
       " (2, 512, 2, 0.001, 0),\n",
       " (2, 512, 2, 0.001, 0.25),\n",
       " (2, 512, 2, 0.001, 0.5),\n",
       " (2, 512, 2, 0.001, 1),\n",
       " (2, 512, 2, 0.0001, 0),\n",
       " (2, 512, 2, 0.0001, 0.25),\n",
       " (2, 512, 2, 0.0001, 0.5),\n",
       " (2, 512, 2, 0.0001, 1),\n",
       " (2, 512, 2, 1e-05, 0),\n",
       " (2, 512, 2, 1e-05, 0.25),\n",
       " (2, 512, 2, 1e-05, 0.5),\n",
       " (2, 512, 2, 1e-05, 1),\n",
       " (2, 512, 3, 0.001, 0),\n",
       " (2, 512, 3, 0.001, 0.25),\n",
       " (2, 512, 3, 0.001, 0.5),\n",
       " (2, 512, 3, 0.001, 1),\n",
       " (2, 512, 3, 0.0001, 0),\n",
       " (2, 512, 3, 0.0001, 0.25),\n",
       " (2, 512, 3, 0.0001, 0.5),\n",
       " (2, 512, 3, 0.0001, 1),\n",
       " (2, 512, 3, 1e-05, 0),\n",
       " (2, 512, 3, 1e-05, 0.25),\n",
       " (2, 512, 3, 1e-05, 0.5),\n",
       " (2, 512, 3, 1e-05, 1),\n",
       " (2, 512, 4, 0.001, 0),\n",
       " (2, 512, 4, 0.001, 0.25),\n",
       " (2, 512, 4, 0.001, 0.5),\n",
       " (2, 512, 4, 0.001, 1),\n",
       " (2, 512, 4, 0.0001, 0),\n",
       " (2, 512, 4, 0.0001, 0.25),\n",
       " (2, 512, 4, 0.0001, 0.5),\n",
       " (2, 512, 4, 0.0001, 1),\n",
       " (2, 512, 4, 1e-05, 0),\n",
       " (2, 512, 4, 1e-05, 0.25),\n",
       " (2, 512, 4, 1e-05, 0.5),\n",
       " (2, 512, 4, 1e-05, 1),\n",
       " (2, 512, 5, 0.001, 0),\n",
       " (2, 512, 5, 0.001, 0.25),\n",
       " (2, 512, 5, 0.001, 0.5),\n",
       " (2, 512, 5, 0.001, 1),\n",
       " (2, 512, 5, 0.0001, 0),\n",
       " (2, 512, 5, 0.0001, 0.25),\n",
       " (2, 512, 5, 0.0001, 0.5),\n",
       " (2, 512, 5, 0.0001, 1),\n",
       " (2, 512, 5, 1e-05, 0),\n",
       " (2, 512, 5, 1e-05, 0.25),\n",
       " (2, 512, 5, 1e-05, 0.5),\n",
       " (2, 512, 5, 1e-05, 1),\n",
       " (2, 512, 6, 0.001, 0),\n",
       " (2, 512, 6, 0.001, 0.25),\n",
       " (2, 512, 6, 0.001, 0.5),\n",
       " (2, 512, 6, 0.001, 1),\n",
       " (2, 512, 6, 0.0001, 0),\n",
       " (2, 512, 6, 0.0001, 0.25),\n",
       " (2, 512, 6, 0.0001, 0.5),\n",
       " (2, 512, 6, 0.0001, 1),\n",
       " (2, 512, 6, 1e-05, 0),\n",
       " (2, 512, 6, 1e-05, 0.25),\n",
       " (2, 512, 6, 1e-05, 0.5),\n",
       " (2, 512, 6, 1e-05, 1),\n",
       " (2, 512, 7, 0.001, 0),\n",
       " (2, 512, 7, 0.001, 0.25),\n",
       " (2, 512, 7, 0.001, 0.5),\n",
       " (2, 512, 7, 0.001, 1),\n",
       " (2, 512, 7, 0.0001, 0),\n",
       " (2, 512, 7, 0.0001, 0.25),\n",
       " (2, 512, 7, 0.0001, 0.5),\n",
       " (2, 512, 7, 0.0001, 1),\n",
       " (2, 512, 7, 1e-05, 0),\n",
       " (2, 512, 7, 1e-05, 0.25),\n",
       " (2, 512, 7, 1e-05, 0.5),\n",
       " (2, 512, 7, 1e-05, 1),\n",
       " (2, 512, 8, 0.001, 0),\n",
       " (2, 512, 8, 0.001, 0.25),\n",
       " (2, 512, 8, 0.001, 0.5),\n",
       " (2, 512, 8, 0.001, 1),\n",
       " (2, 512, 8, 0.0001, 0),\n",
       " (2, 512, 8, 0.0001, 0.25),\n",
       " (2, 512, 8, 0.0001, 0.5),\n",
       " (2, 512, 8, 0.0001, 1),\n",
       " (2, 512, 8, 1e-05, 0),\n",
       " (2, 512, 8, 1e-05, 0.25),\n",
       " (2, 512, 8, 1e-05, 0.5),\n",
       " (2, 512, 8, 1e-05, 1),\n",
       " (2, 512, 9, 0.001, 0),\n",
       " (2, 512, 9, 0.001, 0.25),\n",
       " (2, 512, 9, 0.001, 0.5),\n",
       " (2, 512, 9, 0.001, 1),\n",
       " (2, 512, 9, 0.0001, 0),\n",
       " (2, 512, 9, 0.0001, 0.25),\n",
       " (2, 512, 9, 0.0001, 0.5),\n",
       " (2, 512, 9, 0.0001, 1),\n",
       " (2, 512, 9, 1e-05, 0),\n",
       " (2, 512, 9, 1e-05, 0.25),\n",
       " (2, 512, 9, 1e-05, 0.5),\n",
       " (2, 512, 9, 1e-05, 1),\n",
       " (2, 1024, 2, 0.001, 0),\n",
       " (2, 1024, 2, 0.001, 0.25),\n",
       " (2, 1024, 2, 0.001, 0.5),\n",
       " (2, 1024, 2, 0.001, 1),\n",
       " (2, 1024, 2, 0.0001, 0),\n",
       " (2, 1024, 2, 0.0001, 0.25),\n",
       " (2, 1024, 2, 0.0001, 0.5),\n",
       " (2, 1024, 2, 0.0001, 1),\n",
       " (2, 1024, 2, 1e-05, 0),\n",
       " (2, 1024, 2, 1e-05, 0.25),\n",
       " (2, 1024, 2, 1e-05, 0.5),\n",
       " (2, 1024, 2, 1e-05, 1),\n",
       " (2, 1024, 3, 0.001, 0),\n",
       " (2, 1024, 3, 0.001, 0.25),\n",
       " (2, 1024, 3, 0.001, 0.5),\n",
       " (2, 1024, 3, 0.001, 1),\n",
       " (2, 1024, 3, 0.0001, 0),\n",
       " (2, 1024, 3, 0.0001, 0.25),\n",
       " (2, 1024, 3, 0.0001, 0.5),\n",
       " (2, 1024, 3, 0.0001, 1),\n",
       " (2, 1024, 3, 1e-05, 0),\n",
       " (2, 1024, 3, 1e-05, 0.25),\n",
       " (2, 1024, 3, 1e-05, 0.5),\n",
       " (2, 1024, 3, 1e-05, 1),\n",
       " (2, 1024, 4, 0.001, 0),\n",
       " (2, 1024, 4, 0.001, 0.25),\n",
       " (2, 1024, 4, 0.001, 0.5),\n",
       " (2, 1024, 4, 0.001, 1),\n",
       " (2, 1024, 4, 0.0001, 0),\n",
       " (2, 1024, 4, 0.0001, 0.25),\n",
       " (2, 1024, 4, 0.0001, 0.5),\n",
       " (2, 1024, 4, 0.0001, 1),\n",
       " (2, 1024, 4, 1e-05, 0),\n",
       " (2, 1024, 4, 1e-05, 0.25),\n",
       " (2, 1024, 4, 1e-05, 0.5),\n",
       " (2, 1024, 4, 1e-05, 1),\n",
       " (2, 1024, 5, 0.001, 0),\n",
       " (2, 1024, 5, 0.001, 0.25),\n",
       " (2, 1024, 5, 0.001, 0.5),\n",
       " (2, 1024, 5, 0.001, 1),\n",
       " ...]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_found_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "g92mE6Ljkoq9",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "g92mE6Ljkoq9"
   },
   "outputs": [],
   "source": [
    "def Llayers(L,d=20,width=1000):\n",
    "    #construct L-1 linear layers; bias term only on last linear layer\n",
    "    if L < 2:\n",
    "        raise ValueError(\"L must be at least 2\")\n",
    "    if L == 2:\n",
    "        linear_layers = [nn.Linear(d,width,bias=True)]\n",
    "    if L > 2:\n",
    "        linear_layers = [nn.Linear(d,width,bias=False)]\n",
    "        for l in range(L-3):\n",
    "            linear_layers.append(nn.Linear(width,width,bias=False))\n",
    "        linear_layers.append(nn.Linear(width,width,bias=True))\n",
    "\n",
    "    relu = nn.ReLU()\n",
    "\n",
    "    last_layer = nn.Linear(width,1)\n",
    "\n",
    "    layers = linear_layers + [relu,last_layer]\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "jl4Oh8Vp4QIb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "jl4Oh8Vp4QIb",
    "outputId": "8514097d-0db9-4bcd-baf4-42b4faad166a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "4G22AjiIkYNE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1701128292778,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "4G22AjiIkYNE",
    "outputId": "4147ad49-f1a8-410f-c9bb-6822320eed52",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files_found_list = []\n",
    "models = {}\n",
    "for r,n in rnvals:\n",
    "    for L in Ls:\n",
    "        for wd in wds:\n",
    "            for ln in labelnoise:\n",
    "                paramname = job_name+f\"_labelnoise{ln}/N{n}_L{L}_r{r}_wd{wd}_epochs{epochs}\"\n",
    "                if os.path.exists(paramname+\"model.pt\"):\n",
    "                    models[r,n,L,wd,ln] = Llayers(L,width=1000)\n",
    "                    models[r,n,L,wd,ln].to(device)\n",
    "                    if torch.cuda.is_available():\n",
    "                        models[r,n,L,wd,ln].load_state_dict(torch.load(paramname+\"model.pt\"))\n",
    "                    else:\n",
    "                        models[r,n,L,wd,ln].load_state_dict(torch.load(paramname+\"model.pt\"),map_location=torch.device('cpu'))\n",
    "                    models[r,n,L,wd,ln].eval()\n",
    "                    files_found_list.append((r,n,L,wd,ln))\n",
    "                else:\n",
    "                    print(paramname+\"model.pt\",\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "101bd89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 64, 2, 0.001, 0),\n",
       " (1, 64, 2, 0.001, 0.25),\n",
       " (1, 64, 2, 0.001, 0.5),\n",
       " (1, 64, 2, 0.001, 1),\n",
       " (1, 64, 2, 0.0001, 0),\n",
       " (1, 64, 2, 0.0001, 0.25),\n",
       " (1, 64, 2, 0.0001, 0.5),\n",
       " (1, 64, 2, 0.0001, 1),\n",
       " (1, 64, 2, 1e-05, 0),\n",
       " (1, 64, 2, 1e-05, 0.25),\n",
       " (1, 64, 2, 1e-05, 0.5),\n",
       " (1, 64, 2, 1e-05, 1),\n",
       " (1, 64, 3, 0.001, 0),\n",
       " (1, 64, 3, 0.001, 0.25),\n",
       " (1, 64, 3, 0.001, 0.5),\n",
       " (1, 64, 3, 0.001, 1),\n",
       " (1, 64, 3, 0.0001, 0),\n",
       " (1, 64, 3, 0.0001, 0.25),\n",
       " (1, 64, 3, 0.0001, 0.5),\n",
       " (1, 64, 3, 0.0001, 1),\n",
       " (1, 64, 3, 1e-05, 0),\n",
       " (1, 64, 3, 1e-05, 0.25),\n",
       " (1, 64, 3, 1e-05, 0.5),\n",
       " (1, 64, 3, 1e-05, 1),\n",
       " (1, 64, 4, 0.001, 0),\n",
       " (1, 64, 4, 0.001, 0.25),\n",
       " (1, 64, 4, 0.001, 0.5),\n",
       " (1, 64, 4, 0.001, 1),\n",
       " (1, 64, 4, 0.0001, 0),\n",
       " (1, 64, 4, 0.0001, 0.25),\n",
       " (1, 64, 4, 0.0001, 0.5),\n",
       " (1, 64, 4, 0.0001, 1),\n",
       " (1, 64, 4, 1e-05, 0),\n",
       " (1, 64, 4, 1e-05, 0.25),\n",
       " (1, 64, 4, 1e-05, 0.5),\n",
       " (1, 64, 4, 1e-05, 1),\n",
       " (1, 64, 5, 0.001, 0),\n",
       " (1, 64, 5, 0.001, 0.25),\n",
       " (1, 64, 5, 0.001, 0.5),\n",
       " (1, 64, 5, 0.001, 1),\n",
       " (1, 64, 5, 0.0001, 0),\n",
       " (1, 64, 5, 0.0001, 0.25),\n",
       " (1, 64, 5, 0.0001, 0.5),\n",
       " (1, 64, 5, 0.0001, 1),\n",
       " (1, 64, 5, 1e-05, 0),\n",
       " (1, 64, 5, 1e-05, 0.25),\n",
       " (1, 64, 5, 1e-05, 0.5),\n",
       " (1, 64, 5, 1e-05, 1),\n",
       " (1, 64, 6, 0.001, 0),\n",
       " (1, 64, 6, 0.001, 0.25),\n",
       " (1, 64, 6, 0.001, 0.5),\n",
       " (1, 64, 6, 0.001, 1),\n",
       " (1, 64, 6, 0.0001, 0),\n",
       " (1, 64, 6, 0.0001, 0.25),\n",
       " (1, 64, 6, 0.0001, 0.5),\n",
       " (1, 64, 6, 0.0001, 1),\n",
       " (1, 64, 6, 1e-05, 0),\n",
       " (1, 64, 6, 1e-05, 0.25),\n",
       " (1, 64, 6, 1e-05, 0.5),\n",
       " (1, 64, 6, 1e-05, 1),\n",
       " (1, 64, 7, 0.001, 0),\n",
       " (1, 64, 7, 0.001, 0.25),\n",
       " (1, 64, 7, 0.001, 0.5),\n",
       " (1, 64, 7, 0.001, 1),\n",
       " (1, 64, 7, 0.0001, 0),\n",
       " (1, 64, 7, 0.0001, 0.25),\n",
       " (1, 64, 7, 0.0001, 0.5),\n",
       " (1, 64, 7, 0.0001, 1),\n",
       " (1, 64, 7, 1e-05, 0),\n",
       " (1, 64, 7, 1e-05, 0.25),\n",
       " (1, 64, 7, 1e-05, 0.5),\n",
       " (1, 64, 7, 1e-05, 1),\n",
       " (1, 64, 8, 0.001, 0),\n",
       " (1, 64, 8, 0.001, 0.25),\n",
       " (1, 64, 8, 0.001, 0.5),\n",
       " (1, 64, 8, 0.001, 1),\n",
       " (1, 64, 8, 0.0001, 0),\n",
       " (1, 64, 8, 0.0001, 0.25),\n",
       " (1, 64, 8, 0.0001, 0.5),\n",
       " (1, 64, 8, 0.0001, 1),\n",
       " (1, 64, 8, 1e-05, 0),\n",
       " (1, 64, 8, 1e-05, 0.25),\n",
       " (1, 64, 8, 1e-05, 0.5),\n",
       " (1, 64, 8, 1e-05, 1),\n",
       " (1, 64, 9, 0.001, 0),\n",
       " (1, 64, 9, 0.001, 0.25),\n",
       " (1, 64, 9, 0.001, 0.5),\n",
       " (1, 64, 9, 0.001, 1),\n",
       " (1, 64, 9, 0.0001, 0),\n",
       " (1, 64, 9, 0.0001, 0.25),\n",
       " (1, 64, 9, 0.0001, 0.5),\n",
       " (1, 64, 9, 0.0001, 1),\n",
       " (1, 64, 9, 1e-05, 0),\n",
       " (1, 64, 9, 1e-05, 0.25),\n",
       " (1, 64, 9, 1e-05, 0.5),\n",
       " (1, 64, 9, 1e-05, 1),\n",
       " (1, 128, 2, 0.001, 0),\n",
       " (1, 128, 2, 0.001, 0.25),\n",
       " (1, 128, 2, 0.001, 0.5),\n",
       " (1, 128, 2, 0.001, 1),\n",
       " (1, 128, 2, 0.0001, 0),\n",
       " (1, 128, 2, 0.0001, 0.25),\n",
       " (1, 128, 2, 0.0001, 0.5),\n",
       " (1, 128, 2, 0.0001, 1),\n",
       " (1, 128, 2, 1e-05, 0),\n",
       " (1, 128, 2, 1e-05, 0.25),\n",
       " (1, 128, 2, 1e-05, 0.5),\n",
       " (1, 128, 2, 1e-05, 1),\n",
       " (1, 128, 3, 0.001, 0),\n",
       " (1, 128, 3, 0.001, 0.25),\n",
       " (1, 128, 3, 0.001, 0.5),\n",
       " (1, 128, 3, 0.001, 1),\n",
       " (1, 128, 3, 0.0001, 0),\n",
       " (1, 128, 3, 0.0001, 0.25),\n",
       " (1, 128, 3, 0.0001, 0.5),\n",
       " (1, 128, 3, 0.0001, 1),\n",
       " (1, 128, 3, 1e-05, 0),\n",
       " (1, 128, 3, 1e-05, 0.25),\n",
       " (1, 128, 3, 1e-05, 0.5),\n",
       " (1, 128, 3, 1e-05, 1),\n",
       " (1, 128, 4, 0.001, 0),\n",
       " (1, 128, 4, 0.001, 0.25),\n",
       " (1, 128, 4, 0.001, 0.5),\n",
       " (1, 128, 4, 0.001, 1),\n",
       " (1, 128, 4, 0.0001, 0),\n",
       " (1, 128, 4, 0.0001, 0.25),\n",
       " (1, 128, 4, 0.0001, 0.5),\n",
       " (1, 128, 4, 0.0001, 1),\n",
       " (1, 128, 4, 1e-05, 0),\n",
       " (1, 128, 4, 1e-05, 0.25),\n",
       " (1, 128, 4, 1e-05, 0.5),\n",
       " (1, 128, 4, 1e-05, 1),\n",
       " (1, 128, 5, 0.001, 0),\n",
       " (1, 128, 5, 0.001, 0.25),\n",
       " (1, 128, 5, 0.001, 0.5),\n",
       " (1, 128, 5, 0.001, 1),\n",
       " (1, 128, 5, 0.0001, 0),\n",
       " (1, 128, 5, 0.0001, 0.25),\n",
       " (1, 128, 5, 0.0001, 0.5),\n",
       " (1, 128, 5, 0.0001, 1),\n",
       " (1, 128, 5, 1e-05, 0),\n",
       " (1, 128, 5, 1e-05, 0.25),\n",
       " (1, 128, 5, 1e-05, 0.5),\n",
       " (1, 128, 5, 1e-05, 1),\n",
       " (1, 128, 6, 0.001, 0),\n",
       " (1, 128, 6, 0.001, 0.25),\n",
       " (1, 128, 6, 0.001, 0.5),\n",
       " (1, 128, 6, 0.001, 1),\n",
       " (1, 128, 6, 0.0001, 0),\n",
       " (1, 128, 6, 0.0001, 0.25),\n",
       " (1, 128, 6, 0.0001, 0.5),\n",
       " (1, 128, 6, 0.0001, 1),\n",
       " (1, 128, 6, 1e-05, 0),\n",
       " (1, 128, 6, 1e-05, 0.25),\n",
       " (1, 128, 6, 1e-05, 0.5),\n",
       " (1, 128, 6, 1e-05, 1),\n",
       " (1, 128, 7, 0.001, 0),\n",
       " (1, 128, 7, 0.001, 0.25),\n",
       " (1, 128, 7, 0.001, 0.5),\n",
       " (1, 128, 7, 0.001, 1),\n",
       " (1, 128, 7, 0.0001, 0),\n",
       " (1, 128, 7, 0.0001, 0.25),\n",
       " (1, 128, 7, 0.0001, 0.5),\n",
       " (1, 128, 7, 0.0001, 1),\n",
       " (1, 128, 7, 1e-05, 0),\n",
       " (1, 128, 7, 1e-05, 0.25),\n",
       " (1, 128, 7, 1e-05, 0.5),\n",
       " (1, 128, 7, 1e-05, 1),\n",
       " (1, 128, 8, 0.001, 0),\n",
       " (1, 128, 8, 0.001, 0.25),\n",
       " (1, 128, 8, 0.001, 0.5),\n",
       " (1, 128, 8, 0.001, 1),\n",
       " (1, 128, 8, 0.0001, 0),\n",
       " (1, 128, 8, 0.0001, 0.25),\n",
       " (1, 128, 8, 0.0001, 0.5),\n",
       " (1, 128, 8, 0.0001, 1),\n",
       " (1, 128, 8, 1e-05, 0),\n",
       " (1, 128, 8, 1e-05, 0.25),\n",
       " (1, 128, 8, 1e-05, 0.5),\n",
       " (1, 128, 8, 1e-05, 1),\n",
       " (1, 128, 9, 0.001, 0),\n",
       " (1, 128, 9, 0.001, 0.25),\n",
       " (1, 128, 9, 0.001, 0.5),\n",
       " (1, 128, 9, 0.001, 1),\n",
       " (1, 128, 9, 0.0001, 0),\n",
       " (1, 128, 9, 0.0001, 0.25),\n",
       " (1, 128, 9, 0.0001, 0.5),\n",
       " (1, 128, 9, 0.0001, 1),\n",
       " (1, 128, 9, 1e-05, 0),\n",
       " (1, 128, 9, 1e-05, 0.25),\n",
       " (1, 128, 9, 1e-05, 0.5),\n",
       " (1, 128, 9, 1e-05, 1),\n",
       " (1, 256, 2, 0.001, 0),\n",
       " (1, 256, 2, 0.001, 0.25),\n",
       " (1, 256, 2, 0.001, 0.5),\n",
       " (1, 256, 2, 0.001, 1),\n",
       " (1, 256, 2, 0.0001, 0),\n",
       " (1, 256, 2, 0.0001, 0.25),\n",
       " (1, 256, 2, 0.0001, 0.5),\n",
       " (1, 256, 2, 0.0001, 1),\n",
       " (1, 256, 2, 1e-05, 0),\n",
       " (1, 256, 2, 1e-05, 0.25),\n",
       " (1, 256, 2, 1e-05, 0.5),\n",
       " (1, 256, 2, 1e-05, 1),\n",
       " (1, 256, 3, 0.001, 0),\n",
       " (1, 256, 3, 0.001, 0.25),\n",
       " (1, 256, 3, 0.001, 0.5),\n",
       " (1, 256, 3, 0.001, 1),\n",
       " (1, 256, 3, 0.0001, 0),\n",
       " (1, 256, 3, 0.0001, 0.25),\n",
       " (1, 256, 3, 0.0001, 0.5),\n",
       " (1, 256, 3, 0.0001, 1),\n",
       " (1, 256, 3, 1e-05, 0),\n",
       " (1, 256, 3, 1e-05, 0.25),\n",
       " (1, 256, 3, 1e-05, 0.5),\n",
       " (1, 256, 3, 1e-05, 1),\n",
       " (1, 256, 4, 0.001, 0),\n",
       " (1, 256, 4, 0.001, 0.25),\n",
       " (1, 256, 4, 0.001, 0.5),\n",
       " (1, 256, 4, 0.001, 1),\n",
       " (1, 256, 4, 0.0001, 0),\n",
       " (1, 256, 4, 0.0001, 0.25),\n",
       " (1, 256, 4, 0.0001, 0.5),\n",
       " (1, 256, 4, 0.0001, 1),\n",
       " (1, 256, 4, 1e-05, 0),\n",
       " (1, 256, 4, 1e-05, 0.25),\n",
       " (1, 256, 4, 1e-05, 0.5),\n",
       " (1, 256, 4, 1e-05, 1),\n",
       " (1, 256, 5, 0.001, 0),\n",
       " (1, 256, 5, 0.001, 0.25),\n",
       " (1, 256, 5, 0.001, 0.5),\n",
       " (1, 256, 5, 0.001, 1),\n",
       " (1, 256, 5, 0.0001, 0),\n",
       " (1, 256, 5, 0.0001, 0.25),\n",
       " (1, 256, 5, 0.0001, 0.5),\n",
       " (1, 256, 5, 0.0001, 1),\n",
       " (1, 256, 5, 1e-05, 0),\n",
       " (1, 256, 5, 1e-05, 0.25),\n",
       " (1, 256, 5, 1e-05, 0.5),\n",
       " (1, 256, 5, 1e-05, 1),\n",
       " (1, 256, 6, 0.001, 0),\n",
       " (1, 256, 6, 0.001, 0.25),\n",
       " (1, 256, 6, 0.001, 0.5),\n",
       " (1, 256, 6, 0.001, 1),\n",
       " (1, 256, 6, 0.0001, 0),\n",
       " (1, 256, 6, 0.0001, 0.25),\n",
       " (1, 256, 6, 0.0001, 0.5),\n",
       " (1, 256, 6, 0.0001, 1),\n",
       " (1, 256, 6, 1e-05, 0),\n",
       " (1, 256, 6, 1e-05, 0.25),\n",
       " (1, 256, 6, 1e-05, 0.5),\n",
       " (1, 256, 6, 1e-05, 1),\n",
       " (1, 256, 7, 0.001, 0),\n",
       " (1, 256, 7, 0.001, 0.25),\n",
       " (1, 256, 7, 0.001, 0.5),\n",
       " (1, 256, 7, 0.001, 1),\n",
       " (1, 256, 7, 0.0001, 0),\n",
       " (1, 256, 7, 0.0001, 0.25),\n",
       " (1, 256, 7, 0.0001, 0.5),\n",
       " (1, 256, 7, 0.0001, 1),\n",
       " (1, 256, 7, 1e-05, 0),\n",
       " (1, 256, 7, 1e-05, 0.25),\n",
       " (1, 256, 7, 1e-05, 0.5),\n",
       " (1, 256, 7, 1e-05, 1),\n",
       " (1, 256, 8, 0.001, 0),\n",
       " (1, 256, 8, 0.001, 0.25),\n",
       " (1, 256, 8, 0.001, 0.5),\n",
       " (1, 256, 8, 0.001, 1),\n",
       " (1, 256, 8, 0.0001, 0),\n",
       " (1, 256, 8, 0.0001, 0.25),\n",
       " (1, 256, 8, 0.0001, 0.5),\n",
       " (1, 256, 8, 0.0001, 1),\n",
       " (1, 256, 8, 1e-05, 0),\n",
       " (1, 256, 8, 1e-05, 0.25),\n",
       " (1, 256, 8, 1e-05, 0.5),\n",
       " (1, 256, 8, 1e-05, 1),\n",
       " (1, 256, 9, 0.001, 0),\n",
       " (1, 256, 9, 0.001, 0.25),\n",
       " (1, 256, 9, 0.001, 0.5),\n",
       " (1, 256, 9, 0.001, 1),\n",
       " (1, 256, 9, 0.0001, 0),\n",
       " (1, 256, 9, 0.0001, 0.25),\n",
       " (1, 256, 9, 0.0001, 0.5),\n",
       " (1, 256, 9, 0.0001, 1),\n",
       " (1, 256, 9, 1e-05, 0),\n",
       " (1, 256, 9, 1e-05, 0.25),\n",
       " (1, 256, 9, 1e-05, 0.5),\n",
       " (1, 256, 9, 1e-05, 1),\n",
       " (1, 512, 2, 0.001, 0),\n",
       " (1, 512, 2, 0.001, 0.25),\n",
       " (1, 512, 2, 0.001, 0.5),\n",
       " (1, 512, 2, 0.001, 1),\n",
       " (1, 512, 2, 0.0001, 0),\n",
       " (1, 512, 2, 0.0001, 0.25),\n",
       " (1, 512, 2, 0.0001, 0.5),\n",
       " (1, 512, 2, 0.0001, 1),\n",
       " (1, 512, 2, 1e-05, 0),\n",
       " (1, 512, 2, 1e-05, 0.25),\n",
       " (1, 512, 2, 1e-05, 0.5),\n",
       " (1, 512, 2, 1e-05, 1),\n",
       " (1, 512, 3, 0.001, 0),\n",
       " (1, 512, 3, 0.001, 0.25),\n",
       " (1, 512, 3, 0.001, 0.5),\n",
       " (1, 512, 3, 0.001, 1),\n",
       " (1, 512, 3, 0.0001, 0),\n",
       " (1, 512, 3, 0.0001, 0.25),\n",
       " (1, 512, 3, 0.0001, 0.5),\n",
       " (1, 512, 3, 0.0001, 1),\n",
       " (1, 512, 3, 1e-05, 0),\n",
       " (1, 512, 3, 1e-05, 0.25),\n",
       " (1, 512, 3, 1e-05, 0.5),\n",
       " (1, 512, 3, 1e-05, 1),\n",
       " (1, 512, 4, 0.001, 0),\n",
       " (1, 512, 4, 0.001, 0.25),\n",
       " (1, 512, 4, 0.001, 0.5),\n",
       " (1, 512, 4, 0.001, 1),\n",
       " (1, 512, 4, 0.0001, 0),\n",
       " (1, 512, 4, 0.0001, 0.25),\n",
       " (1, 512, 4, 0.0001, 0.5),\n",
       " (1, 512, 4, 0.0001, 1),\n",
       " (1, 512, 4, 1e-05, 0),\n",
       " (1, 512, 4, 1e-05, 0.25),\n",
       " (1, 512, 4, 1e-05, 0.5),\n",
       " (1, 512, 4, 1e-05, 1),\n",
       " (1, 512, 5, 0.001, 0),\n",
       " (1, 512, 5, 0.001, 0.25),\n",
       " (1, 512, 5, 0.001, 0.5),\n",
       " (1, 512, 5, 0.001, 1),\n",
       " (1, 512, 5, 0.0001, 0),\n",
       " (1, 512, 5, 0.0001, 0.25),\n",
       " (1, 512, 5, 0.0001, 0.5),\n",
       " (1, 512, 5, 0.0001, 1),\n",
       " (1, 512, 5, 1e-05, 0),\n",
       " (1, 512, 5, 1e-05, 0.25),\n",
       " (1, 512, 5, 1e-05, 0.5),\n",
       " (1, 512, 5, 1e-05, 1),\n",
       " (1, 512, 6, 0.001, 0),\n",
       " (1, 512, 6, 0.001, 0.25),\n",
       " (1, 512, 6, 0.001, 0.5),\n",
       " (1, 512, 6, 0.001, 1),\n",
       " (1, 512, 6, 0.0001, 0),\n",
       " (1, 512, 6, 0.0001, 0.25),\n",
       " (1, 512, 6, 0.0001, 0.5),\n",
       " (1, 512, 6, 0.0001, 1),\n",
       " (1, 512, 6, 1e-05, 0),\n",
       " (1, 512, 6, 1e-05, 0.25),\n",
       " (1, 512, 6, 1e-05, 0.5),\n",
       " (1, 512, 6, 1e-05, 1),\n",
       " (1, 512, 7, 0.001, 0),\n",
       " (1, 512, 7, 0.001, 0.25),\n",
       " (1, 512, 7, 0.001, 0.5),\n",
       " (1, 512, 7, 0.001, 1),\n",
       " (1, 512, 7, 0.0001, 0),\n",
       " (1, 512, 7, 0.0001, 0.25),\n",
       " (1, 512, 7, 0.0001, 0.5),\n",
       " (1, 512, 7, 0.0001, 1),\n",
       " (1, 512, 7, 1e-05, 0),\n",
       " (1, 512, 7, 1e-05, 0.25),\n",
       " (1, 512, 7, 1e-05, 0.5),\n",
       " (1, 512, 7, 1e-05, 1),\n",
       " (1, 512, 8, 0.001, 0),\n",
       " (1, 512, 8, 0.001, 0.25),\n",
       " (1, 512, 8, 0.001, 0.5),\n",
       " (1, 512, 8, 0.001, 1),\n",
       " (1, 512, 8, 0.0001, 0),\n",
       " (1, 512, 8, 0.0001, 0.25),\n",
       " (1, 512, 8, 0.0001, 0.5),\n",
       " (1, 512, 8, 0.0001, 1),\n",
       " (1, 512, 8, 1e-05, 0),\n",
       " (1, 512, 8, 1e-05, 0.25),\n",
       " (1, 512, 8, 1e-05, 0.5),\n",
       " (1, 512, 8, 1e-05, 1),\n",
       " (1, 512, 9, 0.001, 0),\n",
       " (1, 512, 9, 0.001, 0.25),\n",
       " (1, 512, 9, 0.001, 0.5),\n",
       " (1, 512, 9, 0.001, 1),\n",
       " (1, 512, 9, 0.0001, 0),\n",
       " (1, 512, 9, 0.0001, 0.25),\n",
       " (1, 512, 9, 0.0001, 0.5),\n",
       " (1, 512, 9, 0.0001, 1),\n",
       " (1, 512, 9, 1e-05, 0),\n",
       " (1, 512, 9, 1e-05, 0.25),\n",
       " (1, 512, 9, 1e-05, 0.5),\n",
       " (1, 512, 9, 1e-05, 1),\n",
       " (1, 1024, 2, 0.001, 0),\n",
       " (1, 1024, 2, 0.001, 0.25),\n",
       " (1, 1024, 2, 0.001, 0.5),\n",
       " (1, 1024, 2, 0.001, 1),\n",
       " (1, 1024, 2, 0.0001, 0),\n",
       " (1, 1024, 2, 0.0001, 0.25),\n",
       " (1, 1024, 2, 0.0001, 0.5),\n",
       " (1, 1024, 2, 0.0001, 1),\n",
       " (1, 1024, 2, 1e-05, 0),\n",
       " (1, 1024, 2, 1e-05, 0.25),\n",
       " (1, 1024, 2, 1e-05, 0.5),\n",
       " (1, 1024, 2, 1e-05, 1),\n",
       " (1, 1024, 3, 0.001, 0),\n",
       " (1, 1024, 3, 0.001, 0.25),\n",
       " (1, 1024, 3, 0.001, 0.5),\n",
       " (1, 1024, 3, 0.001, 1),\n",
       " (1, 1024, 3, 0.0001, 0),\n",
       " (1, 1024, 3, 0.0001, 0.25),\n",
       " (1, 1024, 3, 0.0001, 0.5),\n",
       " (1, 1024, 3, 0.0001, 1),\n",
       " (1, 1024, 3, 1e-05, 0),\n",
       " (1, 1024, 3, 1e-05, 0.25),\n",
       " (1, 1024, 3, 1e-05, 0.5),\n",
       " (1, 1024, 3, 1e-05, 1),\n",
       " (1, 1024, 4, 0.001, 0),\n",
       " (1, 1024, 4, 0.001, 0.25),\n",
       " (1, 1024, 4, 0.001, 0.5),\n",
       " (1, 1024, 4, 0.001, 1),\n",
       " (1, 1024, 4, 0.0001, 0),\n",
       " (1, 1024, 4, 0.0001, 0.25),\n",
       " (1, 1024, 4, 0.0001, 0.5),\n",
       " (1, 1024, 4, 0.0001, 1),\n",
       " (1, 1024, 4, 1e-05, 0),\n",
       " (1, 1024, 4, 1e-05, 0.25),\n",
       " (1, 1024, 4, 1e-05, 0.5),\n",
       " (1, 1024, 4, 1e-05, 1),\n",
       " (1, 1024, 5, 0.001, 0),\n",
       " (1, 1024, 5, 0.001, 0.25),\n",
       " (1, 1024, 5, 0.001, 0.5),\n",
       " (1, 1024, 5, 0.001, 1),\n",
       " (1, 1024, 5, 0.0001, 0),\n",
       " (1, 1024, 5, 0.0001, 0.25),\n",
       " (1, 1024, 5, 0.0001, 0.5),\n",
       " (1, 1024, 5, 0.0001, 1),\n",
       " (1, 1024, 5, 1e-05, 0),\n",
       " (1, 1024, 5, 1e-05, 0.25),\n",
       " (1, 1024, 5, 1e-05, 0.5),\n",
       " (1, 1024, 5, 1e-05, 1),\n",
       " (1, 1024, 6, 0.001, 0),\n",
       " (1, 1024, 6, 0.001, 0.25),\n",
       " (1, 1024, 6, 0.001, 0.5),\n",
       " (1, 1024, 6, 0.001, 1),\n",
       " (1, 1024, 6, 0.0001, 0),\n",
       " (1, 1024, 6, 0.0001, 0.25),\n",
       " (1, 1024, 6, 0.0001, 0.5),\n",
       " (1, 1024, 6, 0.0001, 1),\n",
       " (1, 1024, 6, 1e-05, 0),\n",
       " (1, 1024, 6, 1e-05, 0.25),\n",
       " (1, 1024, 6, 1e-05, 0.5),\n",
       " (1, 1024, 6, 1e-05, 1),\n",
       " (1, 1024, 7, 0.001, 0),\n",
       " (1, 1024, 7, 0.001, 0.25),\n",
       " (1, 1024, 7, 0.001, 0.5),\n",
       " (1, 1024, 7, 0.001, 1),\n",
       " (1, 1024, 7, 0.0001, 0),\n",
       " (1, 1024, 7, 0.0001, 0.25),\n",
       " (1, 1024, 7, 0.0001, 0.5),\n",
       " (1, 1024, 7, 0.0001, 1),\n",
       " (1, 1024, 7, 1e-05, 0),\n",
       " (1, 1024, 7, 1e-05, 0.25),\n",
       " (1, 1024, 7, 1e-05, 0.5),\n",
       " (1, 1024, 7, 1e-05, 1),\n",
       " (1, 1024, 8, 0.001, 0),\n",
       " (1, 1024, 8, 0.001, 0.25),\n",
       " (1, 1024, 8, 0.001, 0.5),\n",
       " (1, 1024, 8, 0.001, 1),\n",
       " (1, 1024, 8, 0.0001, 0),\n",
       " (1, 1024, 8, 0.0001, 0.25),\n",
       " (1, 1024, 8, 0.0001, 0.5),\n",
       " (1, 1024, 8, 0.0001, 1),\n",
       " (1, 1024, 8, 1e-05, 0),\n",
       " (1, 1024, 8, 1e-05, 0.25),\n",
       " (1, 1024, 8, 1e-05, 0.5),\n",
       " (1, 1024, 8, 1e-05, 1),\n",
       " (1, 1024, 9, 0.001, 0),\n",
       " (1, 1024, 9, 0.001, 0.25),\n",
       " (1, 1024, 9, 0.001, 0.5),\n",
       " (1, 1024, 9, 0.001, 1),\n",
       " (1, 1024, 9, 0.0001, 0),\n",
       " (1, 1024, 9, 0.0001, 0.25),\n",
       " (1, 1024, 9, 0.0001, 0.5),\n",
       " (1, 1024, 9, 0.0001, 1),\n",
       " (1, 1024, 9, 1e-05, 0),\n",
       " (1, 1024, 9, 1e-05, 0.25),\n",
       " (1, 1024, 9, 1e-05, 0.5),\n",
       " (1, 1024, 9, 1e-05, 1),\n",
       " (1, 2048, 2, 0.001, 0),\n",
       " (1, 2048, 2, 0.001, 0.25),\n",
       " (1, 2048, 2, 0.001, 0.5),\n",
       " (1, 2048, 2, 0.001, 1),\n",
       " (1, 2048, 2, 0.0001, 0),\n",
       " (1, 2048, 2, 0.0001, 0.25),\n",
       " (1, 2048, 2, 0.0001, 0.5),\n",
       " (1, 2048, 2, 0.0001, 1),\n",
       " (1, 2048, 2, 1e-05, 0),\n",
       " (1, 2048, 2, 1e-05, 0.25),\n",
       " (1, 2048, 2, 1e-05, 0.5),\n",
       " (1, 2048, 2, 1e-05, 1),\n",
       " (1, 2048, 3, 0.001, 0),\n",
       " (1, 2048, 3, 0.001, 0.25),\n",
       " (1, 2048, 3, 0.001, 0.5),\n",
       " (1, 2048, 3, 0.001, 1),\n",
       " (1, 2048, 3, 0.0001, 0),\n",
       " (1, 2048, 3, 0.0001, 0.25),\n",
       " (1, 2048, 3, 0.0001, 0.5),\n",
       " (1, 2048, 3, 0.0001, 1),\n",
       " (1, 2048, 3, 1e-05, 0),\n",
       " (1, 2048, 3, 1e-05, 0.25),\n",
       " (1, 2048, 3, 1e-05, 0.5),\n",
       " (1, 2048, 3, 1e-05, 1),\n",
       " (1, 2048, 4, 0.001, 0),\n",
       " (1, 2048, 4, 0.001, 0.25),\n",
       " (1, 2048, 4, 0.001, 0.5),\n",
       " (1, 2048, 4, 0.001, 1),\n",
       " (1, 2048, 4, 0.0001, 0),\n",
       " (1, 2048, 4, 0.0001, 0.25),\n",
       " (1, 2048, 4, 0.0001, 0.5),\n",
       " (1, 2048, 4, 0.0001, 1),\n",
       " (1, 2048, 4, 1e-05, 0),\n",
       " (1, 2048, 4, 1e-05, 0.25),\n",
       " (1, 2048, 4, 1e-05, 0.5),\n",
       " (1, 2048, 4, 1e-05, 1),\n",
       " (1, 2048, 5, 0.001, 0),\n",
       " (1, 2048, 5, 0.001, 0.25),\n",
       " (1, 2048, 5, 0.001, 0.5),\n",
       " (1, 2048, 5, 0.001, 1),\n",
       " (1, 2048, 5, 0.0001, 0),\n",
       " (1, 2048, 5, 0.0001, 0.25),\n",
       " (1, 2048, 5, 0.0001, 0.5),\n",
       " (1, 2048, 5, 0.0001, 1),\n",
       " (1, 2048, 5, 1e-05, 0),\n",
       " (1, 2048, 5, 1e-05, 0.25),\n",
       " (1, 2048, 5, 1e-05, 0.5),\n",
       " (1, 2048, 5, 1e-05, 1),\n",
       " (1, 2048, 6, 0.001, 0),\n",
       " (1, 2048, 6, 0.001, 0.25),\n",
       " (1, 2048, 6, 0.001, 0.5),\n",
       " (1, 2048, 6, 0.001, 1),\n",
       " (1, 2048, 6, 0.0001, 0),\n",
       " (1, 2048, 6, 0.0001, 0.25),\n",
       " (1, 2048, 6, 0.0001, 0.5),\n",
       " (1, 2048, 6, 0.0001, 1),\n",
       " (1, 2048, 6, 1e-05, 0),\n",
       " (1, 2048, 6, 1e-05, 0.25),\n",
       " (1, 2048, 6, 1e-05, 0.5),\n",
       " (1, 2048, 6, 1e-05, 1),\n",
       " (1, 2048, 7, 0.001, 0),\n",
       " (1, 2048, 7, 0.001, 0.25),\n",
       " (1, 2048, 7, 0.001, 0.5),\n",
       " (1, 2048, 7, 0.001, 1),\n",
       " (1, 2048, 7, 0.0001, 0),\n",
       " (1, 2048, 7, 0.0001, 0.25),\n",
       " (1, 2048, 7, 0.0001, 0.5),\n",
       " (1, 2048, 7, 0.0001, 1),\n",
       " (1, 2048, 7, 1e-05, 0),\n",
       " (1, 2048, 7, 1e-05, 0.25),\n",
       " (1, 2048, 7, 1e-05, 0.5),\n",
       " (1, 2048, 7, 1e-05, 1),\n",
       " (1, 2048, 8, 0.001, 0),\n",
       " (1, 2048, 8, 0.001, 0.25),\n",
       " (1, 2048, 8, 0.001, 0.5),\n",
       " (1, 2048, 8, 0.001, 1),\n",
       " (1, 2048, 8, 0.0001, 0),\n",
       " (1, 2048, 8, 0.0001, 0.25),\n",
       " (1, 2048, 8, 0.0001, 0.5),\n",
       " (1, 2048, 8, 0.0001, 1),\n",
       " (1, 2048, 8, 1e-05, 0),\n",
       " (1, 2048, 8, 1e-05, 0.25),\n",
       " (1, 2048, 8, 1e-05, 0.5),\n",
       " (1, 2048, 8, 1e-05, 1),\n",
       " (1, 2048, 9, 0.001, 0),\n",
       " (1, 2048, 9, 0.001, 0.25),\n",
       " (1, 2048, 9, 0.001, 0.5),\n",
       " (1, 2048, 9, 0.001, 1),\n",
       " (1, 2048, 9, 0.0001, 0),\n",
       " (1, 2048, 9, 0.0001, 0.25),\n",
       " (1, 2048, 9, 0.0001, 0.5),\n",
       " (1, 2048, 9, 0.0001, 1),\n",
       " (1, 2048, 9, 1e-05, 0),\n",
       " (1, 2048, 9, 1e-05, 0.25),\n",
       " (1, 2048, 9, 1e-05, 0.5),\n",
       " (1, 2048, 9, 1e-05, 1),\n",
       " (2, 64, 2, 0.001, 0),\n",
       " (2, 64, 2, 0.001, 0.25),\n",
       " (2, 64, 2, 0.001, 0.5),\n",
       " (2, 64, 2, 0.001, 1),\n",
       " (2, 64, 2, 0.0001, 0),\n",
       " (2, 64, 2, 0.0001, 0.25),\n",
       " (2, 64, 2, 0.0001, 0.5),\n",
       " (2, 64, 2, 0.0001, 1),\n",
       " (2, 64, 2, 1e-05, 0),\n",
       " (2, 64, 2, 1e-05, 0.25),\n",
       " (2, 64, 2, 1e-05, 0.5),\n",
       " (2, 64, 2, 1e-05, 1),\n",
       " (2, 64, 3, 0.001, 0),\n",
       " (2, 64, 3, 0.001, 0.25),\n",
       " (2, 64, 3, 0.001, 0.5),\n",
       " (2, 64, 3, 0.001, 1),\n",
       " (2, 64, 3, 0.0001, 0),\n",
       " (2, 64, 3, 0.0001, 0.25),\n",
       " (2, 64, 3, 0.0001, 0.5),\n",
       " (2, 64, 3, 0.0001, 1),\n",
       " (2, 64, 3, 1e-05, 0),\n",
       " (2, 64, 3, 1e-05, 0.25),\n",
       " (2, 64, 3, 1e-05, 0.5),\n",
       " (2, 64, 3, 1e-05, 1),\n",
       " (2, 64, 4, 0.001, 0),\n",
       " (2, 64, 4, 0.001, 0.25),\n",
       " (2, 64, 4, 0.001, 0.5),\n",
       " (2, 64, 4, 0.001, 1),\n",
       " (2, 64, 4, 0.0001, 0),\n",
       " (2, 64, 4, 0.0001, 0.25),\n",
       " (2, 64, 4, 0.0001, 0.5),\n",
       " (2, 64, 4, 0.0001, 1),\n",
       " (2, 64, 4, 1e-05, 0),\n",
       " (2, 64, 4, 1e-05, 0.25),\n",
       " (2, 64, 4, 1e-05, 0.5),\n",
       " (2, 64, 4, 1e-05, 1),\n",
       " (2, 64, 5, 0.001, 0),\n",
       " (2, 64, 5, 0.001, 0.25),\n",
       " (2, 64, 5, 0.001, 0.5),\n",
       " (2, 64, 5, 0.001, 1),\n",
       " (2, 64, 5, 0.0001, 0),\n",
       " (2, 64, 5, 0.0001, 0.25),\n",
       " (2, 64, 5, 0.0001, 0.5),\n",
       " (2, 64, 5, 0.0001, 1),\n",
       " (2, 64, 5, 1e-05, 0),\n",
       " (2, 64, 5, 1e-05, 0.25),\n",
       " (2, 64, 5, 1e-05, 0.5),\n",
       " (2, 64, 5, 1e-05, 1),\n",
       " (2, 64, 6, 0.001, 0),\n",
       " (2, 64, 6, 0.001, 0.25),\n",
       " (2, 64, 6, 0.001, 0.5),\n",
       " (2, 64, 6, 0.001, 1),\n",
       " (2, 64, 6, 0.0001, 0),\n",
       " (2, 64, 6, 0.0001, 0.25),\n",
       " (2, 64, 6, 0.0001, 0.5),\n",
       " (2, 64, 6, 0.0001, 1),\n",
       " (2, 64, 6, 1e-05, 0),\n",
       " (2, 64, 6, 1e-05, 0.25),\n",
       " (2, 64, 6, 1e-05, 0.5),\n",
       " (2, 64, 6, 1e-05, 1),\n",
       " (2, 64, 7, 0.001, 0),\n",
       " (2, 64, 7, 0.001, 0.25),\n",
       " (2, 64, 7, 0.001, 0.5),\n",
       " (2, 64, 7, 0.001, 1),\n",
       " (2, 64, 7, 0.0001, 0),\n",
       " (2, 64, 7, 0.0001, 0.25),\n",
       " (2, 64, 7, 0.0001, 0.5),\n",
       " (2, 64, 7, 0.0001, 1),\n",
       " (2, 64, 7, 1e-05, 0),\n",
       " (2, 64, 7, 1e-05, 0.25),\n",
       " (2, 64, 7, 1e-05, 0.5),\n",
       " (2, 64, 7, 1e-05, 1),\n",
       " (2, 64, 8, 0.001, 0),\n",
       " (2, 64, 8, 0.001, 0.25),\n",
       " (2, 64, 8, 0.001, 0.5),\n",
       " (2, 64, 8, 0.001, 1),\n",
       " (2, 64, 8, 0.0001, 0),\n",
       " (2, 64, 8, 0.0001, 0.25),\n",
       " (2, 64, 8, 0.0001, 0.5),\n",
       " (2, 64, 8, 0.0001, 1),\n",
       " (2, 64, 8, 1e-05, 0),\n",
       " (2, 64, 8, 1e-05, 0.25),\n",
       " (2, 64, 8, 1e-05, 0.5),\n",
       " (2, 64, 8, 1e-05, 1),\n",
       " (2, 64, 9, 0.001, 0),\n",
       " (2, 64, 9, 0.001, 0.25),\n",
       " (2, 64, 9, 0.001, 0.5),\n",
       " (2, 64, 9, 0.001, 1),\n",
       " (2, 64, 9, 0.0001, 0),\n",
       " (2, 64, 9, 0.0001, 0.25),\n",
       " (2, 64, 9, 0.0001, 0.5),\n",
       " (2, 64, 9, 0.0001, 1),\n",
       " (2, 64, 9, 1e-05, 0),\n",
       " (2, 64, 9, 1e-05, 0.25),\n",
       " (2, 64, 9, 1e-05, 0.5),\n",
       " (2, 64, 9, 1e-05, 1),\n",
       " (2, 128, 2, 0.001, 0),\n",
       " (2, 128, 2, 0.001, 0.25),\n",
       " (2, 128, 2, 0.001, 0.5),\n",
       " (2, 128, 2, 0.001, 1),\n",
       " (2, 128, 2, 0.0001, 0),\n",
       " (2, 128, 2, 0.0001, 0.25),\n",
       " (2, 128, 2, 0.0001, 0.5),\n",
       " (2, 128, 2, 0.0001, 1),\n",
       " (2, 128, 2, 1e-05, 0),\n",
       " (2, 128, 2, 1e-05, 0.25),\n",
       " (2, 128, 2, 1e-05, 0.5),\n",
       " (2, 128, 2, 1e-05, 1),\n",
       " (2, 128, 3, 0.001, 0),\n",
       " (2, 128, 3, 0.001, 0.25),\n",
       " (2, 128, 3, 0.001, 0.5),\n",
       " (2, 128, 3, 0.001, 1),\n",
       " (2, 128, 3, 0.0001, 0),\n",
       " (2, 128, 3, 0.0001, 0.25),\n",
       " (2, 128, 3, 0.0001, 0.5),\n",
       " (2, 128, 3, 0.0001, 1),\n",
       " (2, 128, 3, 1e-05, 0),\n",
       " (2, 128, 3, 1e-05, 0.25),\n",
       " (2, 128, 3, 1e-05, 0.5),\n",
       " (2, 128, 3, 1e-05, 1),\n",
       " (2, 128, 4, 0.001, 0),\n",
       " (2, 128, 4, 0.001, 0.25),\n",
       " (2, 128, 4, 0.001, 0.5),\n",
       " (2, 128, 4, 0.001, 1),\n",
       " (2, 128, 4, 0.0001, 0),\n",
       " (2, 128, 4, 0.0001, 0.25),\n",
       " (2, 128, 4, 0.0001, 0.5),\n",
       " (2, 128, 4, 0.0001, 1),\n",
       " (2, 128, 4, 1e-05, 0),\n",
       " (2, 128, 4, 1e-05, 0.25),\n",
       " (2, 128, 4, 1e-05, 0.5),\n",
       " (2, 128, 4, 1e-05, 1),\n",
       " (2, 128, 5, 0.001, 0),\n",
       " (2, 128, 5, 0.001, 0.25),\n",
       " (2, 128, 5, 0.001, 0.5),\n",
       " (2, 128, 5, 0.001, 1),\n",
       " (2, 128, 5, 0.0001, 0),\n",
       " (2, 128, 5, 0.0001, 0.25),\n",
       " (2, 128, 5, 0.0001, 0.5),\n",
       " (2, 128, 5, 0.0001, 1),\n",
       " (2, 128, 5, 1e-05, 0),\n",
       " (2, 128, 5, 1e-05, 0.25),\n",
       " (2, 128, 5, 1e-05, 0.5),\n",
       " (2, 128, 5, 1e-05, 1),\n",
       " (2, 128, 6, 0.001, 0),\n",
       " (2, 128, 6, 0.001, 0.25),\n",
       " (2, 128, 6, 0.001, 0.5),\n",
       " (2, 128, 6, 0.001, 1),\n",
       " (2, 128, 6, 0.0001, 0),\n",
       " (2, 128, 6, 0.0001, 0.25),\n",
       " (2, 128, 6, 0.0001, 0.5),\n",
       " (2, 128, 6, 0.0001, 1),\n",
       " (2, 128, 6, 1e-05, 0),\n",
       " (2, 128, 6, 1e-05, 0.25),\n",
       " (2, 128, 6, 1e-05, 0.5),\n",
       " (2, 128, 6, 1e-05, 1),\n",
       " (2, 128, 7, 0.001, 0),\n",
       " (2, 128, 7, 0.001, 0.25),\n",
       " (2, 128, 7, 0.001, 0.5),\n",
       " (2, 128, 7, 0.001, 1),\n",
       " (2, 128, 7, 0.0001, 0),\n",
       " (2, 128, 7, 0.0001, 0.25),\n",
       " (2, 128, 7, 0.0001, 0.5),\n",
       " (2, 128, 7, 0.0001, 1),\n",
       " (2, 128, 7, 1e-05, 0),\n",
       " (2, 128, 7, 1e-05, 0.25),\n",
       " (2, 128, 7, 1e-05, 0.5),\n",
       " (2, 128, 7, 1e-05, 1),\n",
       " (2, 128, 8, 0.001, 0),\n",
       " (2, 128, 8, 0.001, 0.25),\n",
       " (2, 128, 8, 0.001, 0.5),\n",
       " (2, 128, 8, 0.001, 1),\n",
       " (2, 128, 8, 0.0001, 0),\n",
       " (2, 128, 8, 0.0001, 0.25),\n",
       " (2, 128, 8, 0.0001, 0.5),\n",
       " (2, 128, 8, 0.0001, 1),\n",
       " (2, 128, 8, 1e-05, 0),\n",
       " (2, 128, 8, 1e-05, 0.25),\n",
       " (2, 128, 8, 1e-05, 0.5),\n",
       " (2, 128, 8, 1e-05, 1),\n",
       " (2, 128, 9, 0.001, 0),\n",
       " (2, 128, 9, 0.001, 0.25),\n",
       " (2, 128, 9, 0.001, 0.5),\n",
       " (2, 128, 9, 0.001, 1),\n",
       " (2, 128, 9, 0.0001, 0),\n",
       " (2, 128, 9, 0.0001, 0.25),\n",
       " (2, 128, 9, 0.0001, 0.5),\n",
       " (2, 128, 9, 0.0001, 1),\n",
       " (2, 128, 9, 1e-05, 0),\n",
       " (2, 128, 9, 1e-05, 0.25),\n",
       " (2, 128, 9, 1e-05, 0.5),\n",
       " (2, 128, 9, 1e-05, 1),\n",
       " (2, 256, 2, 0.001, 0),\n",
       " (2, 256, 2, 0.001, 0.25),\n",
       " (2, 256, 2, 0.001, 0.5),\n",
       " (2, 256, 2, 0.001, 1),\n",
       " (2, 256, 2, 0.0001, 0),\n",
       " (2, 256, 2, 0.0001, 0.25),\n",
       " (2, 256, 2, 0.0001, 0.5),\n",
       " (2, 256, 2, 0.0001, 1),\n",
       " (2, 256, 2, 1e-05, 0),\n",
       " (2, 256, 2, 1e-05, 0.25),\n",
       " (2, 256, 2, 1e-05, 0.5),\n",
       " (2, 256, 2, 1e-05, 1),\n",
       " (2, 256, 3, 0.001, 0),\n",
       " (2, 256, 3, 0.001, 0.25),\n",
       " (2, 256, 3, 0.001, 0.5),\n",
       " (2, 256, 3, 0.001, 1),\n",
       " (2, 256, 3, 0.0001, 0),\n",
       " (2, 256, 3, 0.0001, 0.25),\n",
       " (2, 256, 3, 0.0001, 0.5),\n",
       " (2, 256, 3, 0.0001, 1),\n",
       " (2, 256, 3, 1e-05, 0),\n",
       " (2, 256, 3, 1e-05, 0.25),\n",
       " (2, 256, 3, 1e-05, 0.5),\n",
       " (2, 256, 3, 1e-05, 1),\n",
       " (2, 256, 4, 0.001, 0),\n",
       " (2, 256, 4, 0.001, 0.25),\n",
       " (2, 256, 4, 0.001, 0.5),\n",
       " (2, 256, 4, 0.001, 1),\n",
       " (2, 256, 4, 0.0001, 0),\n",
       " (2, 256, 4, 0.0001, 0.25),\n",
       " (2, 256, 4, 0.0001, 0.5),\n",
       " (2, 256, 4, 0.0001, 1),\n",
       " (2, 256, 4, 1e-05, 0),\n",
       " (2, 256, 4, 1e-05, 0.25),\n",
       " (2, 256, 4, 1e-05, 0.5),\n",
       " (2, 256, 4, 1e-05, 1),\n",
       " (2, 256, 5, 0.001, 0),\n",
       " (2, 256, 5, 0.001, 0.25),\n",
       " (2, 256, 5, 0.001, 0.5),\n",
       " (2, 256, 5, 0.001, 1),\n",
       " (2, 256, 5, 0.0001, 0),\n",
       " (2, 256, 5, 0.0001, 0.25),\n",
       " (2, 256, 5, 0.0001, 0.5),\n",
       " (2, 256, 5, 0.0001, 1),\n",
       " (2, 256, 5, 1e-05, 0),\n",
       " (2, 256, 5, 1e-05, 0.25),\n",
       " (2, 256, 5, 1e-05, 0.5),\n",
       " (2, 256, 5, 1e-05, 1),\n",
       " (2, 256, 6, 0.001, 0),\n",
       " (2, 256, 6, 0.001, 0.25),\n",
       " (2, 256, 6, 0.001, 0.5),\n",
       " (2, 256, 6, 0.001, 1),\n",
       " (2, 256, 6, 0.0001, 0),\n",
       " (2, 256, 6, 0.0001, 0.25),\n",
       " (2, 256, 6, 0.0001, 0.5),\n",
       " (2, 256, 6, 0.0001, 1),\n",
       " (2, 256, 6, 1e-05, 0),\n",
       " (2, 256, 6, 1e-05, 0.25),\n",
       " (2, 256, 6, 1e-05, 0.5),\n",
       " (2, 256, 6, 1e-05, 1),\n",
       " (2, 256, 7, 0.001, 0),\n",
       " (2, 256, 7, 0.001, 0.25),\n",
       " (2, 256, 7, 0.001, 0.5),\n",
       " (2, 256, 7, 0.001, 1),\n",
       " (2, 256, 7, 0.0001, 0),\n",
       " (2, 256, 7, 0.0001, 0.25),\n",
       " (2, 256, 7, 0.0001, 0.5),\n",
       " (2, 256, 7, 0.0001, 1),\n",
       " (2, 256, 7, 1e-05, 0),\n",
       " (2, 256, 7, 1e-05, 0.25),\n",
       " (2, 256, 7, 1e-05, 0.5),\n",
       " (2, 256, 7, 1e-05, 1),\n",
       " (2, 256, 8, 0.001, 0),\n",
       " (2, 256, 8, 0.001, 0.25),\n",
       " (2, 256, 8, 0.001, 0.5),\n",
       " (2, 256, 8, 0.001, 1),\n",
       " (2, 256, 8, 0.0001, 0),\n",
       " (2, 256, 8, 0.0001, 0.25),\n",
       " (2, 256, 8, 0.0001, 0.5),\n",
       " (2, 256, 8, 0.0001, 1),\n",
       " (2, 256, 8, 1e-05, 0),\n",
       " (2, 256, 8, 1e-05, 0.25),\n",
       " (2, 256, 8, 1e-05, 0.5),\n",
       " (2, 256, 8, 1e-05, 1),\n",
       " (2, 256, 9, 0.001, 0),\n",
       " (2, 256, 9, 0.001, 0.25),\n",
       " (2, 256, 9, 0.001, 0.5),\n",
       " (2, 256, 9, 0.001, 1),\n",
       " (2, 256, 9, 0.0001, 0),\n",
       " (2, 256, 9, 0.0001, 0.25),\n",
       " (2, 256, 9, 0.0001, 0.5),\n",
       " (2, 256, 9, 0.0001, 1),\n",
       " (2, 256, 9, 1e-05, 0),\n",
       " (2, 256, 9, 1e-05, 0.25),\n",
       " (2, 256, 9, 1e-05, 0.5),\n",
       " (2, 256, 9, 1e-05, 1),\n",
       " (2, 512, 2, 0.001, 0),\n",
       " (2, 512, 2, 0.001, 0.25),\n",
       " (2, 512, 2, 0.001, 0.5),\n",
       " (2, 512, 2, 0.001, 1),\n",
       " (2, 512, 2, 0.0001, 0),\n",
       " (2, 512, 2, 0.0001, 0.25),\n",
       " (2, 512, 2, 0.0001, 0.5),\n",
       " (2, 512, 2, 0.0001, 1),\n",
       " (2, 512, 2, 1e-05, 0),\n",
       " (2, 512, 2, 1e-05, 0.25),\n",
       " (2, 512, 2, 1e-05, 0.5),\n",
       " (2, 512, 2, 1e-05, 1),\n",
       " (2, 512, 3, 0.001, 0),\n",
       " (2, 512, 3, 0.001, 0.25),\n",
       " (2, 512, 3, 0.001, 0.5),\n",
       " (2, 512, 3, 0.001, 1),\n",
       " (2, 512, 3, 0.0001, 0),\n",
       " (2, 512, 3, 0.0001, 0.25),\n",
       " (2, 512, 3, 0.0001, 0.5),\n",
       " (2, 512, 3, 0.0001, 1),\n",
       " (2, 512, 3, 1e-05, 0),\n",
       " (2, 512, 3, 1e-05, 0.25),\n",
       " (2, 512, 3, 1e-05, 0.5),\n",
       " (2, 512, 3, 1e-05, 1),\n",
       " (2, 512, 4, 0.001, 0),\n",
       " (2, 512, 4, 0.001, 0.25),\n",
       " (2, 512, 4, 0.001, 0.5),\n",
       " (2, 512, 4, 0.001, 1),\n",
       " (2, 512, 4, 0.0001, 0),\n",
       " (2, 512, 4, 0.0001, 0.25),\n",
       " (2, 512, 4, 0.0001, 0.5),\n",
       " (2, 512, 4, 0.0001, 1),\n",
       " (2, 512, 4, 1e-05, 0),\n",
       " (2, 512, 4, 1e-05, 0.25),\n",
       " (2, 512, 4, 1e-05, 0.5),\n",
       " (2, 512, 4, 1e-05, 1),\n",
       " (2, 512, 5, 0.001, 0),\n",
       " (2, 512, 5, 0.001, 0.25),\n",
       " (2, 512, 5, 0.001, 0.5),\n",
       " (2, 512, 5, 0.001, 1),\n",
       " (2, 512, 5, 0.0001, 0),\n",
       " (2, 512, 5, 0.0001, 0.25),\n",
       " (2, 512, 5, 0.0001, 0.5),\n",
       " (2, 512, 5, 0.0001, 1),\n",
       " (2, 512, 5, 1e-05, 0),\n",
       " (2, 512, 5, 1e-05, 0.25),\n",
       " (2, 512, 5, 1e-05, 0.5),\n",
       " (2, 512, 5, 1e-05, 1),\n",
       " (2, 512, 6, 0.001, 0),\n",
       " (2, 512, 6, 0.001, 0.25),\n",
       " (2, 512, 6, 0.001, 0.5),\n",
       " (2, 512, 6, 0.001, 1),\n",
       " (2, 512, 6, 0.0001, 0),\n",
       " (2, 512, 6, 0.0001, 0.25),\n",
       " (2, 512, 6, 0.0001, 0.5),\n",
       " (2, 512, 6, 0.0001, 1),\n",
       " (2, 512, 6, 1e-05, 0),\n",
       " (2, 512, 6, 1e-05, 0.25),\n",
       " (2, 512, 6, 1e-05, 0.5),\n",
       " (2, 512, 6, 1e-05, 1),\n",
       " (2, 512, 7, 0.001, 0),\n",
       " (2, 512, 7, 0.001, 0.25),\n",
       " (2, 512, 7, 0.001, 0.5),\n",
       " (2, 512, 7, 0.001, 1),\n",
       " (2, 512, 7, 0.0001, 0),\n",
       " (2, 512, 7, 0.0001, 0.25),\n",
       " (2, 512, 7, 0.0001, 0.5),\n",
       " (2, 512, 7, 0.0001, 1),\n",
       " (2, 512, 7, 1e-05, 0),\n",
       " (2, 512, 7, 1e-05, 0.25),\n",
       " (2, 512, 7, 1e-05, 0.5),\n",
       " (2, 512, 7, 1e-05, 1),\n",
       " (2, 512, 8, 0.001, 0),\n",
       " (2, 512, 8, 0.001, 0.25),\n",
       " (2, 512, 8, 0.001, 0.5),\n",
       " (2, 512, 8, 0.001, 1),\n",
       " (2, 512, 8, 0.0001, 0),\n",
       " (2, 512, 8, 0.0001, 0.25),\n",
       " (2, 512, 8, 0.0001, 0.5),\n",
       " (2, 512, 8, 0.0001, 1),\n",
       " (2, 512, 8, 1e-05, 0),\n",
       " (2, 512, 8, 1e-05, 0.25),\n",
       " (2, 512, 8, 1e-05, 0.5),\n",
       " (2, 512, 8, 1e-05, 1),\n",
       " (2, 512, 9, 0.001, 0),\n",
       " (2, 512, 9, 0.001, 0.25),\n",
       " (2, 512, 9, 0.001, 0.5),\n",
       " (2, 512, 9, 0.001, 1),\n",
       " (2, 512, 9, 0.0001, 0),\n",
       " (2, 512, 9, 0.0001, 0.25),\n",
       " (2, 512, 9, 0.0001, 0.5),\n",
       " (2, 512, 9, 0.0001, 1),\n",
       " (2, 512, 9, 1e-05, 0),\n",
       " (2, 512, 9, 1e-05, 0.25),\n",
       " (2, 512, 9, 1e-05, 0.5),\n",
       " (2, 512, 9, 1e-05, 1),\n",
       " (2, 1024, 2, 0.001, 0),\n",
       " (2, 1024, 2, 0.001, 0.25),\n",
       " (2, 1024, 2, 0.001, 0.5),\n",
       " (2, 1024, 2, 0.001, 1),\n",
       " (2, 1024, 2, 0.0001, 0),\n",
       " (2, 1024, 2, 0.0001, 0.25),\n",
       " (2, 1024, 2, 0.0001, 0.5),\n",
       " (2, 1024, 2, 0.0001, 1),\n",
       " (2, 1024, 2, 1e-05, 0),\n",
       " (2, 1024, 2, 1e-05, 0.25),\n",
       " (2, 1024, 2, 1e-05, 0.5),\n",
       " (2, 1024, 2, 1e-05, 1),\n",
       " (2, 1024, 3, 0.001, 0),\n",
       " (2, 1024, 3, 0.001, 0.25),\n",
       " (2, 1024, 3, 0.001, 0.5),\n",
       " (2, 1024, 3, 0.001, 1),\n",
       " (2, 1024, 3, 0.0001, 0),\n",
       " (2, 1024, 3, 0.0001, 0.25),\n",
       " (2, 1024, 3, 0.0001, 0.5),\n",
       " (2, 1024, 3, 0.0001, 1),\n",
       " (2, 1024, 3, 1e-05, 0),\n",
       " (2, 1024, 3, 1e-05, 0.25),\n",
       " (2, 1024, 3, 1e-05, 0.5),\n",
       " (2, 1024, 3, 1e-05, 1),\n",
       " (2, 1024, 4, 0.001, 0),\n",
       " (2, 1024, 4, 0.001, 0.25),\n",
       " (2, 1024, 4, 0.001, 0.5),\n",
       " (2, 1024, 4, 0.001, 1),\n",
       " (2, 1024, 4, 0.0001, 0),\n",
       " (2, 1024, 4, 0.0001, 0.25),\n",
       " (2, 1024, 4, 0.0001, 0.5),\n",
       " (2, 1024, 4, 0.0001, 1),\n",
       " (2, 1024, 4, 1e-05, 0),\n",
       " (2, 1024, 4, 1e-05, 0.25),\n",
       " (2, 1024, 4, 1e-05, 0.5),\n",
       " (2, 1024, 4, 1e-05, 1),\n",
       " (2, 1024, 5, 0.001, 0),\n",
       " (2, 1024, 5, 0.001, 0.25),\n",
       " (2, 1024, 5, 0.001, 0.5),\n",
       " (2, 1024, 5, 0.001, 1),\n",
       " ...]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_found_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wt9Aud_elI-7",
   "metadata": {
    "id": "Wt9Aud_elI-7"
   },
   "source": [
    "## create pandas table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8hubUl6NlIh3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "8hubUl6NlIh3",
    "outputId": "008a8a90-61cb-40fb-e1fe-d5a2df77ddb8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>sigma</th>\n",
       "      <th>n</th>\n",
       "      <th>L</th>\n",
       "      <th>lambda</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Weight Decay</th>\n",
       "      <th>Model</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Activations</th>\n",
       "      <th>Final Train MSE</th>\n",
       "      <th>Final Weight Decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.264137, 60.159626, 60.055336, 59.95126, 59...</td>\n",
       "      <td>[1340.8088, 1340.6824, 1340.5591, 1340.4393, 1...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.015114</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>1.341431e-05</td>\n",
       "      <td>224.599670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.19433, 60.09037, 59.986626, 59.883102, 59....</td>\n",
       "      <td>[1340.81, 1340.6852, 1340.564, 1340.4459, 1340...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.187302</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>5.777340e-06</td>\n",
       "      <td>232.962372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.23307, 60.129654, 60.026463, 59.923485, 59...</td>\n",
       "      <td>[1340.8153, 1340.6946, 1340.5769, 1340.463, 13...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.641077</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>2.933852e-06</td>\n",
       "      <td>243.854279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.6362, 60.53386, 60.43174, 60.32984, 60.228...</td>\n",
       "      <td>[1340.8146, 1340.6935, 1340.5748, 1340.4594, 1...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>2.281639</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>8.222866e-07</td>\n",
       "      <td>270.989899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.26413, 60.159615, 60.05532, 59.95124, 59.8...</td>\n",
       "      <td>[1340.8729, 1340.8097, 1340.7502, 1340.6937, 1...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.028895</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>5.553246e-08</td>\n",
       "      <td>226.878784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[4.843381, 4.1538687, 4.0898495, 4.112174, 4.0...</td>\n",
       "      <td>[29353.979, 29336.143, 29325.576, 29313.77, 29...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>1.051062</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>8.801726e-01</td>\n",
       "      <td>1423.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[3.9311483, 3.126759, 3.0239534, 3.2637768, 3....</td>\n",
       "      <td>[29418.832, 29405.988, 29402.477, 29401.318, 2...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>4.186356e-06</td>\n",
       "      <td>793.465820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[3.9806905, 3.2087455, 3.1139393, 3.2826471, 3...</td>\n",
       "      <td>[29418.32, 29405.709, 29402.227, 29400.758, 29...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.079065</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>4.259164e-02</td>\n",
       "      <td>2450.782715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[4.1494136, 3.4205222, 3.316286, 3.436524, 3.2...</td>\n",
       "      <td>[29417.41, 29405.28, 29401.941, 29399.887, 293...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.442928</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>1.006398e-01</td>\n",
       "      <td>4212.969727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[4.8435297, 4.1512017, 4.089573, 4.106273, 4.0...</td>\n",
       "      <td>[29415.23, 29403.904, 29401.18, 29397.674, 293...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>2.417259</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>3.620931e-04</td>\n",
       "      <td>6659.266113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      r  sigma     n  L   lambda  \\\n",
       "0     1   0.00    64  2  0.00100   \n",
       "1     1   0.25    64  2  0.00100   \n",
       "2     1   0.50    64  2  0.00100   \n",
       "3     1   1.00    64  2  0.00100   \n",
       "4     1   0.00    64  2  0.00010   \n",
       "...  ..    ...   ... ..      ...   \n",
       "1147  2   1.00  2048  9  0.00010   \n",
       "1148  2   0.00  2048  9  0.00001   \n",
       "1149  2   0.25  2048  9  0.00001   \n",
       "1150  2   0.50  2048  9  0.00001   \n",
       "1151  2   1.00  2048  9  0.00001   \n",
       "\n",
       "                                          Learning Rate  \\\n",
       "0     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "2     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "3     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "4     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "...                                                 ...   \n",
       "1147  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1148  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1149  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1150  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1151  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "\n",
       "                                              Train MSE  \\\n",
       "0     [60.264137, 60.159626, 60.055336, 59.95126, 59...   \n",
       "1     [60.19433, 60.09037, 59.986626, 59.883102, 59....   \n",
       "2     [60.23307, 60.129654, 60.026463, 59.923485, 59...   \n",
       "3     [60.6362, 60.53386, 60.43174, 60.32984, 60.228...   \n",
       "4     [60.26413, 60.159615, 60.05532, 59.95124, 59.8...   \n",
       "...                                                 ...   \n",
       "1147  [4.843381, 4.1538687, 4.0898495, 4.112174, 4.0...   \n",
       "1148  [3.9311483, 3.126759, 3.0239534, 3.2637768, 3....   \n",
       "1149  [3.9806905, 3.2087455, 3.1139393, 3.2826471, 3...   \n",
       "1150  [4.1494136, 3.4205222, 3.316286, 3.436524, 3.2...   \n",
       "1151  [4.8435297, 4.1512017, 4.089573, 4.106273, 4.0...   \n",
       "\n",
       "                                           Weight Decay  \\\n",
       "0     [1340.8088, 1340.6824, 1340.5591, 1340.4393, 1...   \n",
       "1     [1340.81, 1340.6852, 1340.564, 1340.4459, 1340...   \n",
       "2     [1340.8153, 1340.6946, 1340.5769, 1340.463, 13...   \n",
       "3     [1340.8146, 1340.6935, 1340.5748, 1340.4594, 1...   \n",
       "4     [1340.8729, 1340.8097, 1340.7502, 1340.6937, 1...   \n",
       "...                                                 ...   \n",
       "1147  [29353.979, 29336.143, 29325.576, 29313.77, 29...   \n",
       "1148  [29418.832, 29405.988, 29402.477, 29401.318, 2...   \n",
       "1149  [29418.32, 29405.709, 29402.227, 29400.758, 29...   \n",
       "1150  [29417.41, 29405.28, 29401.941, 29399.887, 293...   \n",
       "1151  [29415.23, 29403.904, 29401.18, 29397.674, 293...   \n",
       "\n",
       "                                                  Model  Test MSE  \\\n",
       "0     [Linear(in_features=20, out_features=1000, bia...  0.015114   \n",
       "1     [Linear(in_features=20, out_features=1000, bia...  0.187302   \n",
       "2     [Linear(in_features=20, out_features=1000, bia...  0.641077   \n",
       "3     [Linear(in_features=20, out_features=1000, bia...  2.281639   \n",
       "4     [Linear(in_features=20, out_features=1000, bia...  0.028895   \n",
       "...                                                 ...       ...   \n",
       "1147  [Linear(in_features=20, out_features=1000, bia...  1.051062   \n",
       "1148  [Linear(in_features=20, out_features=1000, bia...  0.000005   \n",
       "1149  [Linear(in_features=20, out_features=1000, bia...  0.079065   \n",
       "1150  [Linear(in_features=20, out_features=1000, bia...  0.442928   \n",
       "1151  [Linear(in_features=20, out_features=1000, bia...  2.417259   \n",
       "\n",
       "          Activations  Final Train MSE  Final Weight Decay  \n",
       "0     linear and relu     1.341431e-05          224.599670  \n",
       "1     linear and relu     5.777340e-06          232.962372  \n",
       "2     linear and relu     2.933852e-06          243.854279  \n",
       "3     linear and relu     8.222866e-07          270.989899  \n",
       "4     linear and relu     5.553246e-08          226.878784  \n",
       "...               ...              ...                 ...  \n",
       "1147  linear and relu     8.801726e-01         1423.843750  \n",
       "1148  linear and relu     4.186356e-06          793.465820  \n",
       "1149  linear and relu     4.259164e-02         2450.782715  \n",
       "1150  linear and relu     1.006398e-01         4212.969727  \n",
       "1151  linear and relu     3.620931e-04         6659.266113  \n",
       "\n",
       "[1152 rows x 13 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {\n",
    "  \"r\"                                    : [r                          for r,n,L,wd,ln in files_found_list],# + [r                         for wd in wds for r,n in relu_rnvals],\n",
    "  \"sigma\"                                : [ln                         for r,n,L,wd,ln in files_found_list],# + [ln                        for wd in wds for r,n in relu_rnvals],\n",
    "  \"n\"                                    : [n                          for r,n,L,wd,ln in files_found_list],# + [n                         for wd in wds for r,n in relu_rnvals],\n",
    "  \"L\"                                    : [L                          for r,n,L,wd,ln in files_found_list],# + [4                         for wd in wds for r,n in relu_rnvals],\n",
    "  \"lambda\"                               : [wd                         for r,n,L,wd,ln in files_found_list],# + [wd                        for wd in wds for r,n in relu_rnvals],\n",
    "  \"Learning Rate\"                        : [learningrates[r,n,L,wd,ln] for r,n,L,wd,ln in files_found_list],# + [RELUlearningrates[r,n][4][wd] for wd in wds for r,n in relu_rnvals],\n",
    "  \"Train MSE\"                            : [trainMSEs[r,n,L,wd,ln]     for r,n,L,wd,ln in files_found_list],# + [RELUtrainMSEs[r,n][4][wd]     for wd in wds for r,n in relu_rnvals],\n",
    "  \"Weight Decay\"                         : [weightdecays[r,n,L,wd,ln]  for r,n,L,wd,ln in files_found_list],# + [RELUweightdecays[r,n][4][wd]  for wd in wds for r,n in relu_rnvals],\n",
    "  \"Model\"                                : [models[r,n,L,wd,ln]        for r,n,L,wd,ln in files_found_list],# + [RELUmodels[r,n,4,wd]          for wd in wds for r,n in relu_rnvals],\n",
    "  \"Test MSE\"                             : [testMSEs[r,n,L,wd,ln]      for r,n,L,wd,ln in files_found_list],# + [RELUtestMSEs[r,n][4][wd].item()      for wd in wds for r,n in relu_rnvals],\n",
    "  \"Activations\"                          : [\"linear and relu\"          for r,n,L,wd,ln in files_found_list],# + [\"relu only\"         for wd in wds for r,n in relu_rnvals]\n",
    "}\n",
    "res = pd.DataFrame(res)\n",
    "res[\"Final Train MSE\"] = [r[-1] for r in res[\"Train MSE\"]]\n",
    "res[\"Final Weight Decay\"] = [r[-1] for r in res[\"Weight Decay\"]]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc089a26",
   "metadata": {},
   "source": [
    "# Initial Training Time Checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659b93a",
   "metadata": {},
   "source": [
    "## Train MSE v Epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "43f1d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ln in labelnoise:\n",
    "#     for lamb in wds:\n",
    "#         f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(20,10))#len(res.r.unique()),len(res.n.unique())\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for rownum,row in res.iterrows():\n",
    "#             if row[\"L\"] <= 4 and row[\"sigma\"] == ln and row[\"lambda\"] == lamb:\n",
    "#                 whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "#                 whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "#                 ax[whichrow,whichcol].semilogy(row[\"Train MSE\"],label=rf\"$L = {row['L']}$\",linewidth=1,alpha=0.5)\n",
    "#                 ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},n = {row['n']}$\")\n",
    "#                 ax[whichrow,whichcol].set_xlabel(\"Epoch\")\n",
    "#         ax[0,0].legend()\n",
    "#         f.suptitle(rf\"Train MSE v Epoch (label noise $N(0,{ln**2})$, weight decay $\\lambda$ = {lamb})\")\n",
    "#         f.savefig(job_name+f\"_labelnoise{ln}/trainmse234_lamb{lamb}.png\",dpi=300)\n",
    "#         plt.show()\n",
    "#         f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(20,10))#len(res.r.unique()),len(res.n.unique())\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for rownum,row in res.iterrows():\n",
    "#             if 4 < row[\"L\"] <= 7 and row[\"sigma\"] == ln and row[\"lambda\"] == lamb:\n",
    "#                 whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "#                 whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "#                 ax[whichrow,whichcol].semilogy(row[\"Train MSE\"],label=rf\"$L = {row['L']}$\",linewidth=1,alpha=0.5)\n",
    "#                 ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},n = {row['n']}$\")\n",
    "#                 ax[whichrow,whichcol].set_xlabel(\"Epoch\")\n",
    "#         ax[0,0].legend()\n",
    "#         f.suptitle(rf\"Train MSE v Epoch (label noise $N(0,{ln**2})$, weight decay $\\lambda$ = {lamb})\")\n",
    "#         f.savefig(job_name+f\"_labelnoise{ln}/trainmse567_lamb{lamb}.png\",dpi=300)\n",
    "#         plt.show()\n",
    "#         f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(20,10))\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for rownum,row in res.iterrows():\n",
    "#             if row[\"L\"] > 7 and row[\"sigma\"] == ln and row[\"lambda\"] == lamb:\n",
    "#                 whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "#                 whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "#                 ax[whichrow,whichcol].semilogy(row[\"Train MSE\"],label=rf\"$L = {row['L']}$\",linewidth=1,alpha=0.5)\n",
    "#                 ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},n = {row['n']}$\")\n",
    "#                 ax[whichrow,whichcol].set_xlabel(\"Epoch\")\n",
    "#         ax[0,0].legend()\n",
    "#         f.suptitle(rf\"Train MSE v Epoch (label noise $N(0,{ln**2})$, weight decay $\\lambda$ = {lamb})\")\n",
    "#         f.savefig(job_name+f\"_labelnoise{ln}/trainmse8910_lamb{lamb}.png\",dpi=300)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1613a4",
   "metadata": {},
   "source": [
    "## Weight Decay v Epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c619fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ln in labelnoise:\n",
    "#     for lamb in wds:\n",
    "#         f, ax = plt.subplots(len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(20,10))#len(res.r.unique()),len(res.n.unique())\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for rownum,row in res.iterrows():\n",
    "#             if row[\"sigma\"] == ln and row[\"lambda\"] == lamb:\n",
    "#                 whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "#                 whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "#                 ax[whichrow,whichcol].semilogy(row[\"Weight Decay\"],label=rf\"$L = {row['L']}$\",linewidth=1,alpha=0.7)\n",
    "#                 ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},n = {row['n']}$\")\n",
    "#                 ax[whichrow,whichcol].set_xlabel(\"Epoch\")\n",
    "#         ax[0,0].legend()\n",
    "#         f.suptitle(rf\"Weight Decay v Epoch (label noise $N(0,{ln**2})$, weight decay $\\lambda$ = {lamb})\")\n",
    "#         f.savefig(job_name+f\"_labelnoise{ln}/weightdecay_lamb{lamb}.png\",dpi=300)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605ff27",
   "metadata": {},
   "source": [
    "## learning rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "064707ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ln in labelnoise:\n",
    "#     for lamb in wds:\n",
    "#         f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(20,10))#len(res.r.unique()),len(res.n.unique())\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for rownum,row in res.iterrows():\n",
    "#             if row[\"sigma\"] == ln and row[\"lambda\"] == lamb:\n",
    "#                     whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "#                     whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "#                     ax[whichrow,whichcol].semilogy(row[\"Learning Rate\"],label=rf\"$L = {row['L']}$\",linewidth=1,alpha=0.7)\n",
    "#                     ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},n = {row['n']}$\")\n",
    "#                     ax[whichrow,whichcol].set_xlabel(\"Epoch\")\n",
    "#         ax[0,0].legend()\n",
    "#         f.suptitle(rf\"Learning Rate v Epoch (label noise $N(0,{ln**2})$, weight decay $\\lambda$ = {lamb})\")\n",
    "#         f.savefig(job_name+f\"_labelnoise{ln}/LearningRate_lamb{lamb}.png\",dpi=300)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2d3b4",
   "metadata": {},
   "source": [
    "# filter out bad training losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "c6b3c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMSE_threshold = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "166e6a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>sigma</th>\n",
       "      <th>n</th>\n",
       "      <th>L</th>\n",
       "      <th>lambda</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Weight Decay</th>\n",
       "      <th>Model</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Activations</th>\n",
       "      <th>Final Train MSE</th>\n",
       "      <th>Final Weight Decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [r, sigma, n, L, lambda, Learning Rate, Train MSE, Weight Decay, Model, Test MSE, Activations, Final Train MSE, Final Weight Decay]\n",
       "Index: []"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[res[\"Final Train MSE\"] >= trainMSE_threshold + res[\"sigma\"]] #TODO is this reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4aX7gRNo_xet",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "4aX7gRNo_xet"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>sigma</th>\n",
       "      <th>n</th>\n",
       "      <th>L</th>\n",
       "      <th>lambda</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Weight Decay</th>\n",
       "      <th>Model</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Activations</th>\n",
       "      <th>Final Train MSE</th>\n",
       "      <th>Final Weight Decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.264137, 60.159626, 60.055336, 59.95126, 59...</td>\n",
       "      <td>[1340.8088, 1340.6824, 1340.5591, 1340.4393, 1...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.015114</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>1.341431e-05</td>\n",
       "      <td>224.599670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.19433, 60.09037, 59.986626, 59.883102, 59....</td>\n",
       "      <td>[1340.81, 1340.6852, 1340.564, 1340.4459, 1340...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.187302</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>5.777340e-06</td>\n",
       "      <td>232.962372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.23307, 60.129654, 60.026463, 59.923485, 59...</td>\n",
       "      <td>[1340.8153, 1340.6946, 1340.5769, 1340.463, 13...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.641077</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>2.933852e-06</td>\n",
       "      <td>243.854279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.6362, 60.53386, 60.43174, 60.32984, 60.228...</td>\n",
       "      <td>[1340.8146, 1340.6935, 1340.5748, 1340.4594, 1...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>2.281639</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>8.222866e-07</td>\n",
       "      <td>270.989899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.26413, 60.159615, 60.05532, 59.95124, 59.8...</td>\n",
       "      <td>[1340.8729, 1340.8097, 1340.7502, 1340.6937, 1...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.028895</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>5.553246e-08</td>\n",
       "      <td>226.878784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[4.843381, 4.1538687, 4.0898495, 4.112174, 4.0...</td>\n",
       "      <td>[29353.979, 29336.143, 29325.576, 29313.77, 29...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>1.051062</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>8.801726e-01</td>\n",
       "      <td>1423.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[3.9311483, 3.126759, 3.0239534, 3.2637768, 3....</td>\n",
       "      <td>[29418.832, 29405.988, 29402.477, 29401.318, 2...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>4.186356e-06</td>\n",
       "      <td>793.465820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[3.9806905, 3.2087455, 3.1139393, 3.2826471, 3...</td>\n",
       "      <td>[29418.32, 29405.709, 29402.227, 29400.758, 29...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.079065</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>4.259164e-02</td>\n",
       "      <td>2450.782715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[4.1494136, 3.4205222, 3.316286, 3.436524, 3.2...</td>\n",
       "      <td>[29417.41, 29405.28, 29401.941, 29399.887, 293...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.442928</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>1.006398e-01</td>\n",
       "      <td>4212.969727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[4.8435297, 4.1512017, 4.089573, 4.106273, 4.0...</td>\n",
       "      <td>[29415.23, 29403.904, 29401.18, 29397.674, 293...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>2.417259</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>3.620931e-04</td>\n",
       "      <td>6659.266113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      r  sigma     n  L   lambda  \\\n",
       "0     1   0.00    64  2  0.00100   \n",
       "1     1   0.25    64  2  0.00100   \n",
       "2     1   0.50    64  2  0.00100   \n",
       "3     1   1.00    64  2  0.00100   \n",
       "4     1   0.00    64  2  0.00010   \n",
       "...  ..    ...   ... ..      ...   \n",
       "1147  2   1.00  2048  9  0.00010   \n",
       "1148  2   0.00  2048  9  0.00001   \n",
       "1149  2   0.25  2048  9  0.00001   \n",
       "1150  2   0.50  2048  9  0.00001   \n",
       "1151  2   1.00  2048  9  0.00001   \n",
       "\n",
       "                                          Learning Rate  \\\n",
       "0     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "2     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "3     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "4     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "...                                                 ...   \n",
       "1147  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1148  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1149  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1150  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1151  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "\n",
       "                                              Train MSE  \\\n",
       "0     [60.264137, 60.159626, 60.055336, 59.95126, 59...   \n",
       "1     [60.19433, 60.09037, 59.986626, 59.883102, 59....   \n",
       "2     [60.23307, 60.129654, 60.026463, 59.923485, 59...   \n",
       "3     [60.6362, 60.53386, 60.43174, 60.32984, 60.228...   \n",
       "4     [60.26413, 60.159615, 60.05532, 59.95124, 59.8...   \n",
       "...                                                 ...   \n",
       "1147  [4.843381, 4.1538687, 4.0898495, 4.112174, 4.0...   \n",
       "1148  [3.9311483, 3.126759, 3.0239534, 3.2637768, 3....   \n",
       "1149  [3.9806905, 3.2087455, 3.1139393, 3.2826471, 3...   \n",
       "1150  [4.1494136, 3.4205222, 3.316286, 3.436524, 3.2...   \n",
       "1151  [4.8435297, 4.1512017, 4.089573, 4.106273, 4.0...   \n",
       "\n",
       "                                           Weight Decay  \\\n",
       "0     [1340.8088, 1340.6824, 1340.5591, 1340.4393, 1...   \n",
       "1     [1340.81, 1340.6852, 1340.564, 1340.4459, 1340...   \n",
       "2     [1340.8153, 1340.6946, 1340.5769, 1340.463, 13...   \n",
       "3     [1340.8146, 1340.6935, 1340.5748, 1340.4594, 1...   \n",
       "4     [1340.8729, 1340.8097, 1340.7502, 1340.6937, 1...   \n",
       "...                                                 ...   \n",
       "1147  [29353.979, 29336.143, 29325.576, 29313.77, 29...   \n",
       "1148  [29418.832, 29405.988, 29402.477, 29401.318, 2...   \n",
       "1149  [29418.32, 29405.709, 29402.227, 29400.758, 29...   \n",
       "1150  [29417.41, 29405.28, 29401.941, 29399.887, 293...   \n",
       "1151  [29415.23, 29403.904, 29401.18, 29397.674, 293...   \n",
       "\n",
       "                                                  Model  Test MSE  \\\n",
       "0     [Linear(in_features=20, out_features=1000, bia...  0.015114   \n",
       "1     [Linear(in_features=20, out_features=1000, bia...  0.187302   \n",
       "2     [Linear(in_features=20, out_features=1000, bia...  0.641077   \n",
       "3     [Linear(in_features=20, out_features=1000, bia...  2.281639   \n",
       "4     [Linear(in_features=20, out_features=1000, bia...  0.028895   \n",
       "...                                                 ...       ...   \n",
       "1147  [Linear(in_features=20, out_features=1000, bia...  1.051062   \n",
       "1148  [Linear(in_features=20, out_features=1000, bia...  0.000005   \n",
       "1149  [Linear(in_features=20, out_features=1000, bia...  0.079065   \n",
       "1150  [Linear(in_features=20, out_features=1000, bia...  0.442928   \n",
       "1151  [Linear(in_features=20, out_features=1000, bia...  2.417259   \n",
       "\n",
       "          Activations  Final Train MSE  Final Weight Decay  \n",
       "0     linear and relu     1.341431e-05          224.599670  \n",
       "1     linear and relu     5.777340e-06          232.962372  \n",
       "2     linear and relu     2.933852e-06          243.854279  \n",
       "3     linear and relu     8.222866e-07          270.989899  \n",
       "4     linear and relu     5.553246e-08          226.878784  \n",
       "...               ...              ...                 ...  \n",
       "1147  linear and relu     8.801726e-01         1423.843750  \n",
       "1148  linear and relu     4.186356e-06          793.465820  \n",
       "1149  linear and relu     4.259164e-02         2450.782715  \n",
       "1150  linear and relu     1.006398e-01         4212.969727  \n",
       "1151  linear and relu     3.620931e-04         6659.266113  \n",
       "\n",
       "[1152 rows x 13 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = res[res[\"Final Train MSE\"] < trainMSE_threshold + res[\"sigma\"]]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96588e81",
   "metadata": {},
   "source": [
    "# generate data function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "55ad283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(datasetsize,r,seed,std,labelnoiseseed,trainsize=2**18,testsize=2**10,d=20,funcseed=42,verbose=False,ood=False):\n",
    "\n",
    "    ##Generate data with a true central subspaces of varying dimensions\n",
    "    #generate X values for training and test sets\n",
    "    np.random.seed(seed) #set seed for data generation\n",
    "    trainX = np.random.rand(d,trainsize).astype(np.float32)[:,:datasetsize] - 0.5 #distributed as U[-1/2, 1/2]\n",
    "    testX = np.random.rand(d,testsize).astype(np.float32) - 0.5 #distributed as U[-1/2, 1/2]\n",
    "    #out of distribution datagen\n",
    "    if ood:\n",
    "        trainX *= 2 #now distributed as U[-1, 1]\n",
    "        testX *= 2 #now distributed as U[-1, 1]\n",
    "    ##for each $r$ value create and store data-gen functions and $y$ evaluations\n",
    "    #geneate params for functions\n",
    "    k = d+1\n",
    "    U = np.load(job_name+f\"_labelnoise{std}/r{r}U.npy\")\n",
    "    Sigma = np.load(job_name+f\"_labelnoise{std}/r{r}Sigma.npy\")\n",
    "    V = np.load(job_name+f\"_labelnoise{std}/r{r}V.npy\")\n",
    "    A = np.load(job_name+f\"_labelnoise{std}/r{r}A.npy\")\n",
    "    B = np.load(job_name+f\"_labelnoise{std}/r{r}B.npy\")\n",
    "    #create functions\n",
    "    np.random.seed(labelnoiseseed) #set seed for data generation\n",
    "    def g(z): #active subspace function\n",
    "        hidden_layer = (U*Sigma)@z\n",
    "        hidden_layer = hidden_layer.T + B\n",
    "        hidden_layer = np.maximum(0,hidden_layer).T\n",
    "        return A@hidden_layer\n",
    "    def f(x): #teacher network\n",
    "        z = V.T@x    \n",
    "        eps = std*np.random.randn(x.shape[1])    \n",
    "        return g(z) + eps\n",
    "    #generate data\n",
    "    trainY = f(trainX).astype(np.float32)\n",
    "    testY = f(testX).astype(np.float32)\n",
    "    #move data to device\n",
    "    if verbose:\n",
    "        print(\"device: {}\".format(device))\n",
    "    trainX = torch.from_numpy(trainX).T.to(device)\n",
    "    trainY = torch.from_numpy(trainY).to(device)\n",
    "    testX = torch.from_numpy(testX).T.to(device)\n",
    "    testY = torch.from_numpy(testY).to(device)\n",
    "    if verbose:\n",
    "        print(\"trainX shape = {} trainY shape = {}\".format(\n",
    "            trainX.shape,\n",
    "            trainY.shape\n",
    "        ))\n",
    "    return trainX,trainY,testX,testY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ed4f3",
   "metadata": {},
   "source": [
    "# Validation MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf426e6",
   "metadata": {},
   "source": [
    "\n",
    "## generate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e36305fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation size = 2048 r = 1 label noise std = 0 label noise seed = 686\n",
      "validation size = 2048 r = 1 label noise std = 0.25 label noise seed = 687\n",
      "validation size = 2048 r = 1 label noise std = 0.5 label noise seed = 688\n",
      "validation size = 2048 r = 1 label noise std = 1 label noise seed = 689\n",
      "validation size = 2048 r = 2 label noise std = 0 label noise seed = 686\n",
      "validation size = 2048 r = 2 label noise std = 0.25 label noise seed = 687\n",
      "validation size = 2048 r = 2 label noise std = 0.5 label noise seed = 688\n",
      "validation size = 2048 r = 2 label noise std = 1 label noise seed = 689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(-0.5000, device='cuda:0'), tensor(0.5000, device='cuda:0'))"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationY = {}\n",
    "\n",
    "validationsize = 2048\n",
    "for r in rs:\n",
    "    for k,std in enumerate(labelnoise):\n",
    "        labelnoiseseed = 686 + k\n",
    "        datagenseed = 1107\n",
    "        print(\"validation size =\",validationsize,\"r =\",r,\"label noise std =\",std,\"label noise seed =\",labelnoiseseed)\n",
    "        validationX,validationY[r,std] = gen_data(datasetsize=validationsize,r=r,seed=datagenseed,std=std,labelnoiseseed=labelnoiseseed)[:2]\n",
    "validationX.min(),validationX.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e5647",
   "metadata": {},
   "source": [
    "## compute squared errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "c299da64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>sigma</th>\n",
       "      <th>n</th>\n",
       "      <th>L</th>\n",
       "      <th>lambda</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Weight Decay</th>\n",
       "      <th>Model</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Activations</th>\n",
       "      <th>Final Train MSE</th>\n",
       "      <th>Final Weight Decay</th>\n",
       "      <th>Validation MSE</th>\n",
       "      <th>Validation MSE$/\\sigma^2$</th>\n",
       "      <th>Validation Squared Errors</th>\n",
       "      <th>Validation SEM</th>\n",
       "      <th>Validation STD of Squared Errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.264137, 60.159626, 60.055336, 59.95126, 59...</td>\n",
       "      <td>[1340.8088, 1340.6824, 1340.5591, 1340.4393, 1...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.015114</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>1.341431e-05</td>\n",
       "      <td>224.599670</td>\n",
       "      <td>0.016170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.00016536887, 0.00055736996, 0.012135972, 0....</td>\n",
       "      <td>1.897142e-03</td>\n",
       "      <td>0.085834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.19433, 60.09037, 59.986626, 59.883102, 59....</td>\n",
       "      <td>[1340.81, 1340.6852, 1340.564, 1340.4459, 1340...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.187302</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>5.777340e-06</td>\n",
       "      <td>232.962372</td>\n",
       "      <td>0.196858</td>\n",
       "      <td>3.149720</td>\n",
       "      <td>[0.047511395, 0.42259312, 0.22228272, 0.204517...</td>\n",
       "      <td>6.768054e-03</td>\n",
       "      <td>0.306212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.23307, 60.129654, 60.026463, 59.923485, 59...</td>\n",
       "      <td>[1340.8153, 1340.6946, 1340.5769, 1340.463, 13...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.641077</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>2.933852e-06</td>\n",
       "      <td>243.854279</td>\n",
       "      <td>0.715122</td>\n",
       "      <td>2.860486</td>\n",
       "      <td>[1.5282708, 1.1073704, 0.4331878, 0.6494131, 0...</td>\n",
       "      <td>2.324565e-02</td>\n",
       "      <td>1.051721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.6362, 60.53386, 60.43174, 60.32984, 60.228...</td>\n",
       "      <td>[1340.8146, 1340.6935, 1340.5748, 1340.4594, 1...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>2.281639</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>8.222866e-07</td>\n",
       "      <td>270.989899</td>\n",
       "      <td>2.548462</td>\n",
       "      <td>2.548462</td>\n",
       "      <td>[0.20293018, 0.06598258, 0.0017223269, 0.55333...</td>\n",
       "      <td>8.663294e-02</td>\n",
       "      <td>3.919602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[60.26413, 60.159615, 60.05532, 59.95124, 59.8...</td>\n",
       "      <td>[1340.8729, 1340.8097, 1340.7502, 1340.6937, 1...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.028895</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>5.553246e-08</td>\n",
       "      <td>226.878784</td>\n",
       "      <td>0.030106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.00012912386, 0.0035010616, 0.03244231, 0.01...</td>\n",
       "      <td>2.302237e-03</td>\n",
       "      <td>0.104162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[4.843381, 4.1538687, 4.0898495, 4.112174, 4.0...</td>\n",
       "      <td>[29353.979, 29336.143, 29325.576, 29313.77, 29...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>1.051062</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>8.801726e-01</td>\n",
       "      <td>1423.843750</td>\n",
       "      <td>1.071401</td>\n",
       "      <td>1.071401</td>\n",
       "      <td>[0.26118207, 0.9932619, 0.35466397, 0.55603135...</td>\n",
       "      <td>3.355892e-02</td>\n",
       "      <td>1.518333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[3.9311483, 3.126759, 3.0239534, 3.2637768, 3....</td>\n",
       "      <td>[29418.832, 29405.988, 29402.477, 29401.318, 2...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>4.186356e-06</td>\n",
       "      <td>793.465820</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2.513002e-06, 3.7479367e-06, 1.0855331e-08, 2...</td>\n",
       "      <td>5.863767e-07</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[3.9806905, 3.2087455, 3.1139393, 3.2826471, 3...</td>\n",
       "      <td>[29418.32, 29405.709, 29402.227, 29400.758, 29...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.079065</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>4.259164e-02</td>\n",
       "      <td>2450.782715</td>\n",
       "      <td>0.077548</td>\n",
       "      <td>1.240771</td>\n",
       "      <td>[0.008216529, 0.041220017, 0.04544737, 0.03442...</td>\n",
       "      <td>2.484017e-03</td>\n",
       "      <td>0.112386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[4.1494136, 3.4205222, 3.316286, 3.436524, 3.2...</td>\n",
       "      <td>[29417.41, 29405.28, 29401.941, 29399.887, 293...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>0.442928</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>1.006398e-01</td>\n",
       "      <td>4212.969727</td>\n",
       "      <td>0.440972</td>\n",
       "      <td>1.763887</td>\n",
       "      <td>[1.560701, 0.104476415, 0.1401253, 0.031048845...</td>\n",
       "      <td>1.581635e-02</td>\n",
       "      <td>0.715592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>[1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...</td>\n",
       "      <td>[4.8435297, 4.1512017, 4.089573, 4.106273, 4.0...</td>\n",
       "      <td>[29415.23, 29403.904, 29401.18, 29397.674, 293...</td>\n",
       "      <td>[Linear(in_features=20, out_features=1000, bia...</td>\n",
       "      <td>2.417259</td>\n",
       "      <td>linear and relu</td>\n",
       "      <td>3.620931e-04</td>\n",
       "      <td>6659.266113</td>\n",
       "      <td>2.366597</td>\n",
       "      <td>2.366597</td>\n",
       "      <td>[0.3252172, 0.017183177, 4.964687, 0.003945299...</td>\n",
       "      <td>7.762085e-02</td>\n",
       "      <td>3.511861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      r  sigma     n  L   lambda  \\\n",
       "0     1   0.00    64  2  0.00100   \n",
       "1     1   0.25    64  2  0.00100   \n",
       "2     1   0.50    64  2  0.00100   \n",
       "3     1   1.00    64  2  0.00100   \n",
       "4     1   0.00    64  2  0.00010   \n",
       "...  ..    ...   ... ..      ...   \n",
       "1147  2   1.00  2048  9  0.00010   \n",
       "1148  2   0.00  2048  9  0.00001   \n",
       "1149  2   0.25  2048  9  0.00001   \n",
       "1150  2   0.50  2048  9  0.00001   \n",
       "1151  2   1.00  2048  9  0.00001   \n",
       "\n",
       "                                          Learning Rate  \\\n",
       "0     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "2     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "3     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "4     [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "...                                                 ...   \n",
       "1147  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1148  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1149  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1150  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "1151  [1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-...   \n",
       "\n",
       "                                              Train MSE  \\\n",
       "0     [60.264137, 60.159626, 60.055336, 59.95126, 59...   \n",
       "1     [60.19433, 60.09037, 59.986626, 59.883102, 59....   \n",
       "2     [60.23307, 60.129654, 60.026463, 59.923485, 59...   \n",
       "3     [60.6362, 60.53386, 60.43174, 60.32984, 60.228...   \n",
       "4     [60.26413, 60.159615, 60.05532, 59.95124, 59.8...   \n",
       "...                                                 ...   \n",
       "1147  [4.843381, 4.1538687, 4.0898495, 4.112174, 4.0...   \n",
       "1148  [3.9311483, 3.126759, 3.0239534, 3.2637768, 3....   \n",
       "1149  [3.9806905, 3.2087455, 3.1139393, 3.2826471, 3...   \n",
       "1150  [4.1494136, 3.4205222, 3.316286, 3.436524, 3.2...   \n",
       "1151  [4.8435297, 4.1512017, 4.089573, 4.106273, 4.0...   \n",
       "\n",
       "                                           Weight Decay  \\\n",
       "0     [1340.8088, 1340.6824, 1340.5591, 1340.4393, 1...   \n",
       "1     [1340.81, 1340.6852, 1340.564, 1340.4459, 1340...   \n",
       "2     [1340.8153, 1340.6946, 1340.5769, 1340.463, 13...   \n",
       "3     [1340.8146, 1340.6935, 1340.5748, 1340.4594, 1...   \n",
       "4     [1340.8729, 1340.8097, 1340.7502, 1340.6937, 1...   \n",
       "...                                                 ...   \n",
       "1147  [29353.979, 29336.143, 29325.576, 29313.77, 29...   \n",
       "1148  [29418.832, 29405.988, 29402.477, 29401.318, 2...   \n",
       "1149  [29418.32, 29405.709, 29402.227, 29400.758, 29...   \n",
       "1150  [29417.41, 29405.28, 29401.941, 29399.887, 293...   \n",
       "1151  [29415.23, 29403.904, 29401.18, 29397.674, 293...   \n",
       "\n",
       "                                                  Model  Test MSE  \\\n",
       "0     [Linear(in_features=20, out_features=1000, bia...  0.015114   \n",
       "1     [Linear(in_features=20, out_features=1000, bia...  0.187302   \n",
       "2     [Linear(in_features=20, out_features=1000, bia...  0.641077   \n",
       "3     [Linear(in_features=20, out_features=1000, bia...  2.281639   \n",
       "4     [Linear(in_features=20, out_features=1000, bia...  0.028895   \n",
       "...                                                 ...       ...   \n",
       "1147  [Linear(in_features=20, out_features=1000, bia...  1.051062   \n",
       "1148  [Linear(in_features=20, out_features=1000, bia...  0.000005   \n",
       "1149  [Linear(in_features=20, out_features=1000, bia...  0.079065   \n",
       "1150  [Linear(in_features=20, out_features=1000, bia...  0.442928   \n",
       "1151  [Linear(in_features=20, out_features=1000, bia...  2.417259   \n",
       "\n",
       "          Activations  Final Train MSE  Final Weight Decay  Validation MSE  \\\n",
       "0     linear and relu     1.341431e-05          224.599670        0.016170   \n",
       "1     linear and relu     5.777340e-06          232.962372        0.196858   \n",
       "2     linear and relu     2.933852e-06          243.854279        0.715122   \n",
       "3     linear and relu     8.222866e-07          270.989899        2.548462   \n",
       "4     linear and relu     5.553246e-08          226.878784        0.030106   \n",
       "...               ...              ...                 ...             ...   \n",
       "1147  linear and relu     8.801726e-01         1423.843750        1.071401   \n",
       "1148  linear and relu     4.186356e-06          793.465820        0.000004   \n",
       "1149  linear and relu     4.259164e-02         2450.782715        0.077548   \n",
       "1150  linear and relu     1.006398e-01         4212.969727        0.440972   \n",
       "1151  linear and relu     3.620931e-04         6659.266113        2.366597   \n",
       "\n",
       "      Validation MSE$/\\sigma^2$  \\\n",
       "0                           NaN   \n",
       "1                      3.149720   \n",
       "2                      2.860486   \n",
       "3                      2.548462   \n",
       "4                           NaN   \n",
       "...                         ...   \n",
       "1147                   1.071401   \n",
       "1148                        NaN   \n",
       "1149                   1.240771   \n",
       "1150                   1.763887   \n",
       "1151                   2.366597   \n",
       "\n",
       "                              Validation Squared Errors  Validation SEM  \\\n",
       "0     [0.00016536887, 0.00055736996, 0.012135972, 0....    1.897142e-03   \n",
       "1     [0.047511395, 0.42259312, 0.22228272, 0.204517...    6.768054e-03   \n",
       "2     [1.5282708, 1.1073704, 0.4331878, 0.6494131, 0...    2.324565e-02   \n",
       "3     [0.20293018, 0.06598258, 0.0017223269, 0.55333...    8.663294e-02   \n",
       "4     [0.00012912386, 0.0035010616, 0.03244231, 0.01...    2.302237e-03   \n",
       "...                                                 ...             ...   \n",
       "1147  [0.26118207, 0.9932619, 0.35466397, 0.55603135...    3.355892e-02   \n",
       "1148  [2.513002e-06, 3.7479367e-06, 1.0855331e-08, 2...    5.863767e-07   \n",
       "1149  [0.008216529, 0.041220017, 0.04544737, 0.03442...    2.484017e-03   \n",
       "1150  [1.560701, 0.104476415, 0.1401253, 0.031048845...    1.581635e-02   \n",
       "1151  [0.3252172, 0.017183177, 4.964687, 0.003945299...    7.762085e-02   \n",
       "\n",
       "      Validation STD of Squared Errors  \n",
       "0                             0.085834  \n",
       "1                             0.306212  \n",
       "2                             1.051721  \n",
       "3                             3.919602  \n",
       "4                             0.104162  \n",
       "...                                ...  \n",
       "1147                          1.518333  \n",
       "1148                          0.000027  \n",
       "1149                          0.112386  \n",
       "1150                          0.715592  \n",
       "1151                          3.511861  \n",
       "\n",
       "[1152 rows x 18 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    validation = []\n",
    "    normalized_validation = []\n",
    "    squared_errors = []\n",
    "    validation_sem = []\n",
    "    validation_std = []\n",
    "    for rownum, row in res.iterrows():\n",
    "        std = row[\"sigma\"]\n",
    "        predY = row[\"Model\"](validationX)\n",
    "        squared_err = (predY[:,0] - validationY[row[\"r\"],std])**2\n",
    "        squared_err = squared_err.cpu().numpy()\n",
    "        mse = nn.functional.mse_loss(predY[:,0],validationY[row[\"r\"],std]).item()\n",
    "        assert np.isclose(mse,np.mean(squared_err))\n",
    "        validation.append(mse)\n",
    "        sem_sqared_err = sem(squared_err)\n",
    "        validation_sem.append(sem_sqared_err)\n",
    "        std_sqared_err = np.std(squared_err)\n",
    "        validation_std.append(std_sqared_err)\n",
    "        if std > 0:\n",
    "            normalized_validation.append(mse/(std**2))\n",
    "        else:\n",
    "            normalized_validation.append(np.nan)\n",
    "        squared_errors.append(squared_err)\n",
    "    res[\"Validation MSE\"] = validation\n",
    "    res[\"Validation MSE$/\\sigma^2$\"] = normalized_validation\n",
    "    res[\"Validation Squared Errors\"] = squared_errors\n",
    "    res[\"Validation SEM\"] = validation_sem\n",
    "    res[\"Validation STD of Squared Errors\"] = validation_std\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f6014",
   "metadata": {},
   "source": [
    "# Generalization MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf53019",
   "metadata": {},
   "source": [
    "## generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "pNH8hcsX35vb",
   "metadata": {
    "id": "pNH8hcsX35vb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generalization size = 2048 r = 1 label noise std = 0 label noise seed = 743\n",
      "generalization size = 2048 r = 1 label noise std = 0.25 label noise seed = 744\n",
      "generalization size = 2048 r = 1 label noise std = 0.5 label noise seed = 745\n",
      "generalization size = 2048 r = 1 label noise std = 1 label noise seed = 746\n",
      "generalization size = 2048 r = 2 label noise std = 0 label noise seed = 743\n",
      "generalization size = 2048 r = 2 label noise std = 0.25 label noise seed = 744\n",
      "generalization size = 2048 r = 2 label noise std = 0.5 label noise seed = 745\n",
      "generalization size = 2048 r = 2 label noise std = 1 label noise seed = 746\n"
     ]
    }
   ],
   "source": [
    "generalizationY = {}\n",
    "generalizationsize = 2048\n",
    "for r in rs:\n",
    "    for k,std in enumerate(labelnoise):\n",
    "        labelnoiseseed = 743 + k\n",
    "        datagenseed = 555\n",
    "        print(\"generalization size =\",generalizationsize,\"r =\",r,\"label noise std =\",std,\"label noise seed =\",labelnoiseseed)\n",
    "        generalizationX,generalizationY[r,std] = gen_data(datasetsize=generalizationsize,r=r,seed=datagenseed,std=std,labelnoiseseed=labelnoiseseed)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "SGjV4q985lrM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1691614314236,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 300
    },
    "id": "SGjV4q985lrM",
    "outputId": "20b86ce8-f70f-4eec-d2cc-4c6313a4bfa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.5000, device='cuda:0'), tensor(0.5000, device='cuda:0'))"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalizationX.min(),generalizationX.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iHCTTCWw4l4s",
   "metadata": {
    "id": "iHCTTCWw4l4s"
   },
   "source": [
    "## compute squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ulWXCTcd2_Xo",
   "metadata": {
    "id": "ulWXCTcd2_Xo"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generalization = []\n",
    "    normalized_generalization = []\n",
    "    squared_errors = []\n",
    "    generalization_sem = []\n",
    "    generalization_std = []\n",
    "    for rownum, row in res.iterrows():\n",
    "        std = row[\"sigma\"]\n",
    "        predY = row[\"Model\"](generalizationX)\n",
    "        squared_err = (predY[:,0] - generalizationY[row[\"r\"],std])**2\n",
    "        squared_err = squared_err.cpu().numpy()\n",
    "        mse = nn.functional.mse_loss(predY[:,0],generalizationY[row[\"r\"],std]).item()\n",
    "        assert np.isclose(mse,np.mean(squared_err))\n",
    "        generalization.append(mse)\n",
    "        sem_sqared_err = sem(squared_err)\n",
    "        generalization_sem.append(sem_sqared_err)\n",
    "        std_sqared_err = np.std(squared_err)\n",
    "        generalization_std.append(std_sqared_err)\n",
    "        if std > 0:\n",
    "            normalized_generalization.append(mse/(std**2))\n",
    "        else:\n",
    "            normalized_generalization.append(np.nan)\n",
    "        squared_errors.append(squared_err)\n",
    "    res[\"Generalization MSE\"] = generalization\n",
    "    res[\"Generalization MSE$/\\sigma^2$\"] = normalized_generalization\n",
    "    res[\"Generalization Squared Errors\"] = squared_errors\n",
    "    res[\"Generalization SEM\"] = generalization_sem\n",
    "    res[\"Generalization STD of Squared Errors\"] = generalization_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LbkhZQcT8Z4S",
   "metadata": {
    "id": "LbkhZQcT8Z4S"
   },
   "source": [
    "# Out of Distribution MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GEplm-eG8g74",
   "metadata": {
    "id": "GEplm-eG8g74"
   },
   "source": [
    "## generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "24bc4f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ood size = 2048 r = 1 label noise std = 0 label noise seed = 235\n",
      "ood size = 2048 r = 1 label noise std = 0.25 label noise seed = 236\n",
      "ood size = 2048 r = 1 label noise std = 0.5 label noise seed = 237\n",
      "ood size = 2048 r = 1 label noise std = 1 label noise seed = 238\n",
      "ood size = 2048 r = 2 label noise std = 0 label noise seed = 235\n",
      "ood size = 2048 r = 2 label noise std = 0.25 label noise seed = 236\n",
      "ood size = 2048 r = 2 label noise std = 0.5 label noise seed = 237\n",
      "ood size = 2048 r = 2 label noise std = 1 label noise seed = 238\n"
     ]
    }
   ],
   "source": [
    "oodY = {}\n",
    "oodsize = 2048\n",
    "for r in rs:\n",
    "    for k,std in enumerate(labelnoise):\n",
    "        labelnoiseseed = 235 + k\n",
    "        datagenseed = 333\n",
    "        print(\"ood size =\",oodsize,\"r =\",r,\"label noise std =\",std,\"label noise seed =\",labelnoiseseed)\n",
    "        oodX,oodY[r,std] = gen_data(datasetsize=oodsize,r=r,seed=datagenseed,std=std,labelnoiseseed=labelnoiseseed,ood=True)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "SiXhFaOE8n4j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1691614317198,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 300
    },
    "id": "SiXhFaOE8n4j",
    "outputId": "cf4681fa-710e-4e13-b705-78939987d20c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.0000, device='cuda:0'), tensor(0.9999, device='cuda:0'))"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oodX.min(),oodX.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yt15AMGL84jk",
   "metadata": {
    "id": "yt15AMGL84jk"
   },
   "source": [
    "## compute squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "8d4d9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ood = []\n",
    "    normalized_ood = []\n",
    "    squared_errors = []\n",
    "    ood_sem = []\n",
    "    ood_std = []\n",
    "    for rownum, row in res.iterrows():\n",
    "        std = row[\"sigma\"]\n",
    "        predY = row[\"Model\"](oodX)\n",
    "        squared_err = (predY[:,0] - oodY[row[\"r\"],std])**2\n",
    "        squared_err = squared_err.cpu().numpy()\n",
    "        mse = nn.functional.mse_loss(predY[:,0],oodY[row[\"r\"],std]).item()\n",
    "        assert np.isclose(mse,np.mean(squared_err))\n",
    "        ood.append(mse)\n",
    "        sem_sqared_err = sem(squared_err)\n",
    "        ood_sem.append(sem_sqared_err)\n",
    "        std_sqared_err = np.std(squared_err)\n",
    "        ood_std.append(std_sqared_err)\n",
    "        if std > 0:\n",
    "            normalized_ood.append(mse/(std**2))\n",
    "        else:\n",
    "            normalized_ood.append(np.nan)\n",
    "        squared_errors.append(squared_err)\n",
    "    res[\"Out of Distribution MSE\"] = ood\n",
    "    res[\"Out of Distribution MSE$/\\sigma^2$\"] = normalized_ood\n",
    "    res[\"Out of Distribution Squared Errors\"] = squared_errors\n",
    "    res[\"Out of Distribution SEM\"] = ood_sem\n",
    "    res[\"Out of Distribution STD of Squared Errors\"] = ood_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c9fb6",
   "metadata": {},
   "source": [
    "# Check that most or all ReLU hyperplanes intersect the support of the distributions of the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "4a78a539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 1\n",
      "\n",
      "TOTALS:\n",
      "~~~~~~~\n",
      " R2-cost contribution          129.775677\n",
      "|b| / ||w||_1                   1.718445\n",
      "# training active,n=64        743.000000\n",
      "% training active,n=64         11.609375\n",
      "# training active,n=128      1468.000000\n",
      "% training active,n=128        11.468750\n",
      "# training active,n=256      2894.000000\n",
      "% training active,n=256        11.304688\n",
      "# training active,n=512      5796.000000\n",
      "% training active,n=512        11.320312\n",
      "# training active,n=1024    11592.000000\n",
      "% training active,n=1024       11.320312\n",
      "# training active,n=2048    23158.000000\n",
      "% training active,n=2048       11.307617\n",
      "# validation active         23138.000000\n",
      "% validation active            11.297852\n",
      "# generalization active     23137.000000\n",
      "% generalization active        11.297363\n",
      "# ood active                23299.000000\n",
      "% ood active                   11.376465\n",
      "dtype: float64\n",
      "\n",
      "unit-wise table:\n",
      "~~~~~~~\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R2-cost contribution</th>\n",
       "      <th>|b| / ||w||_1</th>\n",
       "      <th># training active,n=64</th>\n",
       "      <th>% training active,n=64</th>\n",
       "      <th># training active,n=128</th>\n",
       "      <th>% training active,n=128</th>\n",
       "      <th># training active,n=256</th>\n",
       "      <th>% training active,n=256</th>\n",
       "      <th># training active,n=512</th>\n",
       "      <th>% training active,n=512</th>\n",
       "      <th># training active,n=1024</th>\n",
       "      <th>% training active,n=1024</th>\n",
       "      <th># training active,n=2048</th>\n",
       "      <th>% training active,n=2048</th>\n",
       "      <th># validation active</th>\n",
       "      <th>% validation active</th>\n",
       "      <th># generalization active</th>\n",
       "      <th>% generalization active</th>\n",
       "      <th># ood active</th>\n",
       "      <th>% ood active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.239101</td>\n",
       "      <td>0.017410</td>\n",
       "      <td>18</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>45</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>105</td>\n",
       "      <td>0.410156</td>\n",
       "      <td>204</td>\n",
       "      <td>0.398438</td>\n",
       "      <td>425</td>\n",
       "      <td>0.415039</td>\n",
       "      <td>865</td>\n",
       "      <td>0.422363</td>\n",
       "      <td>894</td>\n",
       "      <td>0.436523</td>\n",
       "      <td>868</td>\n",
       "      <td>0.423828</td>\n",
       "      <td>956</td>\n",
       "      <td>0.466797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.780412</td>\n",
       "      <td>0.025086</td>\n",
       "      <td>46</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>84</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>156</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>318</td>\n",
       "      <td>0.621094</td>\n",
       "      <td>634</td>\n",
       "      <td>0.619141</td>\n",
       "      <td>1252</td>\n",
       "      <td>0.611328</td>\n",
       "      <td>1215</td>\n",
       "      <td>0.593262</td>\n",
       "      <td>1247</td>\n",
       "      <td>0.608887</td>\n",
       "      <td>1118</td>\n",
       "      <td>0.545898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.438060</td>\n",
       "      <td>0.113275</td>\n",
       "      <td>9</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>14</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>23</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>52</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>118</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>231</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>216</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>225</td>\n",
       "      <td>0.109863</td>\n",
       "      <td>563</td>\n",
       "      <td>0.274902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.485826</td>\n",
       "      <td>0.009074</td>\n",
       "      <td>45</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>79</td>\n",
       "      <td>0.617188</td>\n",
       "      <td>141</td>\n",
       "      <td>0.550781</td>\n",
       "      <td>288</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>561</td>\n",
       "      <td>0.547852</td>\n",
       "      <td>1108</td>\n",
       "      <td>0.541016</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.526367</td>\n",
       "      <td>1115</td>\n",
       "      <td>0.544434</td>\n",
       "      <td>1060</td>\n",
       "      <td>0.517578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.722474</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>37</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>66</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>126</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>260</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>514</td>\n",
       "      <td>0.501953</td>\n",
       "      <td>1028</td>\n",
       "      <td>0.501953</td>\n",
       "      <td>992</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>1029</td>\n",
       "      <td>0.502441</td>\n",
       "      <td>1027</td>\n",
       "      <td>0.501465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.581510</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>25</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>59</td>\n",
       "      <td>0.460938</td>\n",
       "      <td>127</td>\n",
       "      <td>0.496094</td>\n",
       "      <td>247</td>\n",
       "      <td>0.482422</td>\n",
       "      <td>502</td>\n",
       "      <td>0.490234</td>\n",
       "      <td>1002</td>\n",
       "      <td>0.489258</td>\n",
       "      <td>1045</td>\n",
       "      <td>0.510254</td>\n",
       "      <td>1003</td>\n",
       "      <td>0.489746</td>\n",
       "      <td>1013</td>\n",
       "      <td>0.494629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.344744</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>39</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>69</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>130</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>268</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>527</td>\n",
       "      <td>0.514648</td>\n",
       "      <td>1053</td>\n",
       "      <td>0.514160</td>\n",
       "      <td>1018</td>\n",
       "      <td>0.497070</td>\n",
       "      <td>1054</td>\n",
       "      <td>0.514648</td>\n",
       "      <td>1041</td>\n",
       "      <td>0.508301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11.159105</td>\n",
       "      <td>0.007599</td>\n",
       "      <td>44</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>75</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>137</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>283</td>\n",
       "      <td>0.552734</td>\n",
       "      <td>552</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.534668</td>\n",
       "      <td>1059</td>\n",
       "      <td>0.517090</td>\n",
       "      <td>1099</td>\n",
       "      <td>0.536621</td>\n",
       "      <td>1054</td>\n",
       "      <td>0.514648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.406547</td>\n",
       "      <td>0.019168</td>\n",
       "      <td>29</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>54</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>103</td>\n",
       "      <td>0.402344</td>\n",
       "      <td>217</td>\n",
       "      <td>0.423828</td>\n",
       "      <td>436</td>\n",
       "      <td>0.425781</td>\n",
       "      <td>855</td>\n",
       "      <td>0.417480</td>\n",
       "      <td>836</td>\n",
       "      <td>0.408203</td>\n",
       "      <td>883</td>\n",
       "      <td>0.431152</td>\n",
       "      <td>937</td>\n",
       "      <td>0.457520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.524434</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>40</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>71</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>132</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>271</td>\n",
       "      <td>0.529297</td>\n",
       "      <td>533</td>\n",
       "      <td>0.520508</td>\n",
       "      <td>1064</td>\n",
       "      <td>0.519531</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1066</td>\n",
       "      <td>0.520508</td>\n",
       "      <td>1042</td>\n",
       "      <td>0.508789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.352635</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>28</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>64</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>134</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>259</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>522</td>\n",
       "      <td>0.509766</td>\n",
       "      <td>1049</td>\n",
       "      <td>0.512207</td>\n",
       "      <td>1083</td>\n",
       "      <td>0.528809</td>\n",
       "      <td>1042</td>\n",
       "      <td>0.508789</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.183430</td>\n",
       "      <td>0.033264</td>\n",
       "      <td>16</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>40</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>89</td>\n",
       "      <td>0.347656</td>\n",
       "      <td>175</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>358</td>\n",
       "      <td>0.349609</td>\n",
       "      <td>727</td>\n",
       "      <td>0.354980</td>\n",
       "      <td>761</td>\n",
       "      <td>0.371582</td>\n",
       "      <td>735</td>\n",
       "      <td>0.358887</td>\n",
       "      <td>896</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23.132322</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>29</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>65</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>135</td>\n",
       "      <td>0.527344</td>\n",
       "      <td>261</td>\n",
       "      <td>0.509766</td>\n",
       "      <td>529</td>\n",
       "      <td>0.516602</td>\n",
       "      <td>1062</td>\n",
       "      <td>0.518555</td>\n",
       "      <td>1095</td>\n",
       "      <td>0.534668</td>\n",
       "      <td>1053</td>\n",
       "      <td>0.514160</td>\n",
       "      <td>1048</td>\n",
       "      <td>0.511719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.087554</td>\n",
       "      <td>0.898286</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2048</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.376140</td>\n",
       "      <td>0.013717</td>\n",
       "      <td>33</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>71</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>146</td>\n",
       "      <td>0.570312</td>\n",
       "      <td>283</td>\n",
       "      <td>0.552734</td>\n",
       "      <td>566</td>\n",
       "      <td>0.552734</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.560059</td>\n",
       "      <td>1165</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>1115</td>\n",
       "      <td>0.544434</td>\n",
       "      <td>1092</td>\n",
       "      <td>0.533203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.369441</td>\n",
       "      <td>0.515860</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2045</td>\n",
       "      <td>0.998535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.440889</td>\n",
       "      <td>0.013751</td>\n",
       "      <td>46</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>80</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>144</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>298</td>\n",
       "      <td>0.582031</td>\n",
       "      <td>577</td>\n",
       "      <td>0.563477</td>\n",
       "      <td>1142</td>\n",
       "      <td>0.557617</td>\n",
       "      <td>1114</td>\n",
       "      <td>0.543945</td>\n",
       "      <td>1146</td>\n",
       "      <td>0.559570</td>\n",
       "      <td>1079</td>\n",
       "      <td>0.526855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.567174</td>\n",
       "      <td>0.022309</td>\n",
       "      <td>46</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>84</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>153</td>\n",
       "      <td>0.597656</td>\n",
       "      <td>313</td>\n",
       "      <td>0.611328</td>\n",
       "      <td>618</td>\n",
       "      <td>0.603516</td>\n",
       "      <td>1224</td>\n",
       "      <td>0.597656</td>\n",
       "      <td>1193</td>\n",
       "      <td>0.582520</td>\n",
       "      <td>1223</td>\n",
       "      <td>0.597168</td>\n",
       "      <td>1109</td>\n",
       "      <td>0.541504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20.941614</td>\n",
       "      <td>0.004756</td>\n",
       "      <td>29</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>65</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>135</td>\n",
       "      <td>0.527344</td>\n",
       "      <td>260</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>527</td>\n",
       "      <td>0.514648</td>\n",
       "      <td>1059</td>\n",
       "      <td>0.517090</td>\n",
       "      <td>1087</td>\n",
       "      <td>0.530762</td>\n",
       "      <td>1049</td>\n",
       "      <td>0.512207</td>\n",
       "      <td>1047</td>\n",
       "      <td>0.511230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.470661</td>\n",
       "      <td>0.004930</td>\n",
       "      <td>29</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>65</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>135</td>\n",
       "      <td>0.527344</td>\n",
       "      <td>260</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>527</td>\n",
       "      <td>0.514648</td>\n",
       "      <td>1059</td>\n",
       "      <td>0.517090</td>\n",
       "      <td>1089</td>\n",
       "      <td>0.531738</td>\n",
       "      <td>1049</td>\n",
       "      <td>0.512207</td>\n",
       "      <td>1047</td>\n",
       "      <td>0.511230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.171602</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>27</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>62</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>131</td>\n",
       "      <td>0.511719</td>\n",
       "      <td>255</td>\n",
       "      <td>0.498047</td>\n",
       "      <td>518</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.526367</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>1037</td>\n",
       "      <td>0.506348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    R2-cost contribution  |b| / ||w||_1  # training active,n=64  \\\n",
       "0               5.239101       0.017410                      18   \n",
       "1               0.780412       0.025086                      46   \n",
       "2               0.438060       0.113275                       9   \n",
       "3               8.485826       0.009074                      45   \n",
       "4               5.722474       0.000186                      37   \n",
       "5               9.581510       0.001485                      25   \n",
       "6               9.344744       0.002883                      39   \n",
       "7              11.159105       0.007599                      44   \n",
       "8               0.406547       0.019168                      29   \n",
       "9               5.524434       0.003632                      40   \n",
       "10              3.352635       0.003558                      28   \n",
       "11              1.183430       0.033264                      16   \n",
       "12             23.132322       0.005514                      29   \n",
       "13              0.087554       0.898286                      64   \n",
       "14              0.376140       0.013717                      33   \n",
       "15              0.369441       0.515860                      64   \n",
       "16             10.440889       0.013751                      46   \n",
       "17              2.567174       0.022309                      46   \n",
       "18             20.941614       0.004756                      29   \n",
       "19             10.470661       0.004930                      29   \n",
       "20              0.171602       0.002702                      27   \n",
       "\n",
       "    % training active,n=64  # training active,n=128  % training active,n=128  \\\n",
       "0                 0.281250                       45                 0.351562   \n",
       "1                 0.718750                       84                 0.656250   \n",
       "2                 0.140625                       14                 0.109375   \n",
       "3                 0.703125                       79                 0.617188   \n",
       "4                 0.578125                       66                 0.515625   \n",
       "5                 0.390625                       59                 0.460938   \n",
       "6                 0.609375                       69                 0.539062   \n",
       "7                 0.687500                       75                 0.585938   \n",
       "8                 0.453125                       54                 0.421875   \n",
       "9                 0.625000                       71                 0.554688   \n",
       "10                0.437500                       64                 0.500000   \n",
       "11                0.250000                       40                 0.312500   \n",
       "12                0.453125                       65                 0.507812   \n",
       "13                1.000000                      128                 1.000000   \n",
       "14                0.515625                       71                 0.554688   \n",
       "15                1.000000                      128                 1.000000   \n",
       "16                0.718750                       80                 0.625000   \n",
       "17                0.718750                       84                 0.656250   \n",
       "18                0.453125                       65                 0.507812   \n",
       "19                0.453125                       65                 0.507812   \n",
       "20                0.421875                       62                 0.484375   \n",
       "\n",
       "    # training active,n=256  % training active,n=256  # training active,n=512  \\\n",
       "0                       105                 0.410156                      204   \n",
       "1                       156                 0.609375                      318   \n",
       "2                        23                 0.089844                       52   \n",
       "3                       141                 0.550781                      288   \n",
       "4                       126                 0.492188                      260   \n",
       "5                       127                 0.496094                      247   \n",
       "6                       130                 0.507812                      268   \n",
       "7                       137                 0.535156                      283   \n",
       "8                       103                 0.402344                      217   \n",
       "9                       132                 0.515625                      271   \n",
       "10                      134                 0.523438                      259   \n",
       "11                       89                 0.347656                      175   \n",
       "12                      135                 0.527344                      261   \n",
       "13                      256                 1.000000                      512   \n",
       "14                      146                 0.570312                      283   \n",
       "15                      256                 1.000000                      512   \n",
       "16                      144                 0.562500                      298   \n",
       "17                      153                 0.597656                      313   \n",
       "18                      135                 0.527344                      260   \n",
       "19                      135                 0.527344                      260   \n",
       "20                      131                 0.511719                      255   \n",
       "\n",
       "    % training active,n=512  # training active,n=1024  \\\n",
       "0                  0.398438                       425   \n",
       "1                  0.621094                       634   \n",
       "2                  0.101562                       118   \n",
       "3                  0.562500                       561   \n",
       "4                  0.507812                       514   \n",
       "5                  0.482422                       502   \n",
       "6                  0.523438                       527   \n",
       "7                  0.552734                       552   \n",
       "8                  0.423828                       436   \n",
       "9                  0.529297                       533   \n",
       "10                 0.505859                       522   \n",
       "11                 0.341797                       358   \n",
       "12                 0.509766                       529   \n",
       "13                 1.000000                      1024   \n",
       "14                 0.552734                       566   \n",
       "15                 1.000000                      1024   \n",
       "16                 0.582031                       577   \n",
       "17                 0.611328                       618   \n",
       "18                 0.507812                       527   \n",
       "19                 0.507812                       527   \n",
       "20                 0.498047                       518   \n",
       "\n",
       "    % training active,n=1024  # training active,n=2048  \\\n",
       "0                   0.415039                       865   \n",
       "1                   0.619141                      1252   \n",
       "2                   0.115234                       231   \n",
       "3                   0.547852                      1108   \n",
       "4                   0.501953                      1028   \n",
       "5                   0.490234                      1002   \n",
       "6                   0.514648                      1053   \n",
       "7                   0.539062                      1095   \n",
       "8                   0.425781                       855   \n",
       "9                   0.520508                      1064   \n",
       "10                  0.509766                      1049   \n",
       "11                  0.349609                       727   \n",
       "12                  0.516602                      1062   \n",
       "13                  1.000000                      2048   \n",
       "14                  0.552734                      1147   \n",
       "15                  1.000000                      2048   \n",
       "16                  0.563477                      1142   \n",
       "17                  0.603516                      1224   \n",
       "18                  0.514648                      1059   \n",
       "19                  0.514648                      1059   \n",
       "20                  0.505859                      1040   \n",
       "\n",
       "    % training active,n=2048  # validation active  % validation active  \\\n",
       "0                   0.422363                  894             0.436523   \n",
       "1                   0.611328                 1215             0.593262   \n",
       "2                   0.112793                  216             0.105469   \n",
       "3                   0.541016                 1078             0.526367   \n",
       "4                   0.501953                  992             0.484375   \n",
       "5                   0.489258                 1045             0.510254   \n",
       "6                   0.514160                 1018             0.497070   \n",
       "7                   0.534668                 1059             0.517090   \n",
       "8                   0.417480                  836             0.408203   \n",
       "9                   0.519531                 1024             0.500000   \n",
       "10                  0.512207                 1083             0.528809   \n",
       "11                  0.354980                  761             0.371582   \n",
       "12                  0.518555                 1095             0.534668   \n",
       "13                  1.000000                 2048             1.000000   \n",
       "14                  0.560059                 1165             0.568848   \n",
       "15                  1.000000                 2048             1.000000   \n",
       "16                  0.557617                 1114             0.543945   \n",
       "17                  0.597656                 1193             0.582520   \n",
       "18                  0.517090                 1087             0.530762   \n",
       "19                  0.517090                 1089             0.531738   \n",
       "20                  0.507812                 1078             0.526367   \n",
       "\n",
       "    # generalization active  % generalization active  # ood active  \\\n",
       "0                       868                 0.423828           956   \n",
       "1                      1247                 0.608887          1118   \n",
       "2                       225                 0.109863           563   \n",
       "3                      1115                 0.544434          1060   \n",
       "4                      1029                 0.502441          1027   \n",
       "5                      1003                 0.489746          1013   \n",
       "6                      1054                 0.514648          1041   \n",
       "7                      1099                 0.536621          1054   \n",
       "8                       883                 0.431152           937   \n",
       "9                      1066                 0.520508          1042   \n",
       "10                     1042                 0.508789          1040   \n",
       "11                      735                 0.358887           896   \n",
       "12                     1053                 0.514160          1048   \n",
       "13                     2048                 1.000000          2048   \n",
       "14                     1115                 0.544434          1092   \n",
       "15                     2048                 1.000000          2045   \n",
       "16                     1146                 0.559570          1079   \n",
       "17                     1223                 0.597168          1109   \n",
       "18                     1049                 0.512207          1047   \n",
       "19                     1049                 0.512207          1047   \n",
       "20                     1040                 0.507812          1037   \n",
       "\n",
       "    % ood active  \n",
       "0       0.466797  \n",
       "1       0.545898  \n",
       "2       0.274902  \n",
       "3       0.517578  \n",
       "4       0.501465  \n",
       "5       0.494629  \n",
       "6       0.508301  \n",
       "7       0.514648  \n",
       "8       0.457520  \n",
       "9       0.508789  \n",
       "10      0.507812  \n",
       "11      0.437500  \n",
       "12      0.511719  \n",
       "13      1.000000  \n",
       "14      0.533203  \n",
       "15      0.998535  \n",
       "16      0.526855  \n",
       "17      0.541504  \n",
       "18      0.511230  \n",
       "19      0.511230  \n",
       "20      0.506348  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 2\n",
      "\n",
      "TOTALS:\n",
      "~~~~~~~\n",
      " R2-cost contribution          223.147503\n",
      "|b| / ||w||_1                   0.230783\n",
      "# training active,n=64        672.000000\n",
      "% training active,n=64         10.500000\n",
      "# training active,n=128      1333.000000\n",
      "% training active,n=128        10.414062\n",
      "# training active,n=256      2674.000000\n",
      "% training active,n=256        10.445312\n",
      "# training active,n=512      5353.000000\n",
      "% training active,n=512        10.455078\n",
      "# training active,n=1024    10675.000000\n",
      "% training active,n=1024       10.424805\n",
      "# training active,n=2048    21379.000000\n",
      "% training active,n=2048       10.438965\n",
      "# validation active         21284.000000\n",
      "% validation active            10.392578\n",
      "# generalization active     21298.000000\n",
      "% generalization active        10.399414\n",
      "# ood active                21360.000000\n",
      "% ood active                   10.429688\n",
      "dtype: float64\n",
      "\n",
      "unit-wise table:\n",
      "~~~~~~~\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R2-cost contribution</th>\n",
       "      <th>|b| / ||w||_1</th>\n",
       "      <th># training active,n=64</th>\n",
       "      <th>% training active,n=64</th>\n",
       "      <th># training active,n=128</th>\n",
       "      <th>% training active,n=128</th>\n",
       "      <th># training active,n=256</th>\n",
       "      <th>% training active,n=256</th>\n",
       "      <th># training active,n=512</th>\n",
       "      <th>% training active,n=512</th>\n",
       "      <th># training active,n=1024</th>\n",
       "      <th>% training active,n=1024</th>\n",
       "      <th># training active,n=2048</th>\n",
       "      <th>% training active,n=2048</th>\n",
       "      <th># validation active</th>\n",
       "      <th>% validation active</th>\n",
       "      <th># generalization active</th>\n",
       "      <th>% generalization active</th>\n",
       "      <th># ood active</th>\n",
       "      <th>% ood active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.021811</td>\n",
       "      <td>0.021495</td>\n",
       "      <td>28</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>50</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>98</td>\n",
       "      <td>0.382812</td>\n",
       "      <td>206</td>\n",
       "      <td>0.402344</td>\n",
       "      <td>398</td>\n",
       "      <td>0.388672</td>\n",
       "      <td>775</td>\n",
       "      <td>0.378418</td>\n",
       "      <td>782</td>\n",
       "      <td>0.381836</td>\n",
       "      <td>784</td>\n",
       "      <td>0.382812</td>\n",
       "      <td>925</td>\n",
       "      <td>0.451660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.402139</td>\n",
       "      <td>0.043486</td>\n",
       "      <td>42</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>90</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>181</td>\n",
       "      <td>0.707031</td>\n",
       "      <td>357</td>\n",
       "      <td>0.697266</td>\n",
       "      <td>725</td>\n",
       "      <td>0.708008</td>\n",
       "      <td>1449</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>1446</td>\n",
       "      <td>0.706055</td>\n",
       "      <td>1453</td>\n",
       "      <td>0.709473</td>\n",
       "      <td>1220</td>\n",
       "      <td>0.595703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.278993</td>\n",
       "      <td>0.023220</td>\n",
       "      <td>27</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>52</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>107</td>\n",
       "      <td>0.417969</td>\n",
       "      <td>206</td>\n",
       "      <td>0.402344</td>\n",
       "      <td>384</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>808</td>\n",
       "      <td>0.394531</td>\n",
       "      <td>753</td>\n",
       "      <td>0.367676</td>\n",
       "      <td>787</td>\n",
       "      <td>0.384277</td>\n",
       "      <td>887</td>\n",
       "      <td>0.433105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.191544</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>29</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>56</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>124</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>232</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>483</td>\n",
       "      <td>0.471680</td>\n",
       "      <td>995</td>\n",
       "      <td>0.485840</td>\n",
       "      <td>976</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>968</td>\n",
       "      <td>0.472656</td>\n",
       "      <td>982</td>\n",
       "      <td>0.479492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.394932</td>\n",
       "      <td>0.013318</td>\n",
       "      <td>37</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>76</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>152</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>297</td>\n",
       "      <td>0.580078</td>\n",
       "      <td>578</td>\n",
       "      <td>0.564453</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.578613</td>\n",
       "      <td>1138</td>\n",
       "      <td>0.555664</td>\n",
       "      <td>1165</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.510742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.869071</td>\n",
       "      <td>0.007666</td>\n",
       "      <td>33</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>67</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>130</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>271</td>\n",
       "      <td>0.529297</td>\n",
       "      <td>558</td>\n",
       "      <td>0.544922</td>\n",
       "      <td>1082</td>\n",
       "      <td>0.528320</td>\n",
       "      <td>1126</td>\n",
       "      <td>0.549805</td>\n",
       "      <td>1102</td>\n",
       "      <td>0.538086</td>\n",
       "      <td>1080</td>\n",
       "      <td>0.527344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.596780</td>\n",
       "      <td>0.010253</td>\n",
       "      <td>36</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>72</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>147</td>\n",
       "      <td>0.574219</td>\n",
       "      <td>291</td>\n",
       "      <td>0.568359</td>\n",
       "      <td>568</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>1164</td>\n",
       "      <td>0.568359</td>\n",
       "      <td>1118</td>\n",
       "      <td>0.545898</td>\n",
       "      <td>1145</td>\n",
       "      <td>0.559082</td>\n",
       "      <td>1035</td>\n",
       "      <td>0.505371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.045827</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>33</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>71</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>146</td>\n",
       "      <td>0.570312</td>\n",
       "      <td>282</td>\n",
       "      <td>0.550781</td>\n",
       "      <td>573</td>\n",
       "      <td>0.559570</td>\n",
       "      <td>1148</td>\n",
       "      <td>0.560547</td>\n",
       "      <td>1108</td>\n",
       "      <td>0.541016</td>\n",
       "      <td>1102</td>\n",
       "      <td>0.538086</td>\n",
       "      <td>1045</td>\n",
       "      <td>0.510254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.535087</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>35</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>68</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>141</td>\n",
       "      <td>0.550781</td>\n",
       "      <td>271</td>\n",
       "      <td>0.529297</td>\n",
       "      <td>536</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>1102</td>\n",
       "      <td>0.538086</td>\n",
       "      <td>1089</td>\n",
       "      <td>0.531738</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.521484</td>\n",
       "      <td>1037</td>\n",
       "      <td>0.506348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.782236</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>29</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>63</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>128</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>259</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>522</td>\n",
       "      <td>0.509766</td>\n",
       "      <td>1071</td>\n",
       "      <td>0.522949</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.502930</td>\n",
       "      <td>1059</td>\n",
       "      <td>0.517090</td>\n",
       "      <td>1002</td>\n",
       "      <td>0.489258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.291021</td>\n",
       "      <td>0.010387</td>\n",
       "      <td>29</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>58</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>111</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>234</td>\n",
       "      <td>0.457031</td>\n",
       "      <td>474</td>\n",
       "      <td>0.462891</td>\n",
       "      <td>904</td>\n",
       "      <td>0.441406</td>\n",
       "      <td>916</td>\n",
       "      <td>0.447266</td>\n",
       "      <td>930</td>\n",
       "      <td>0.454102</td>\n",
       "      <td>973</td>\n",
       "      <td>0.475098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.097131</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>31</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>57</td>\n",
       "      <td>0.445312</td>\n",
       "      <td>111</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>226</td>\n",
       "      <td>0.441406</td>\n",
       "      <td>450</td>\n",
       "      <td>0.439453</td>\n",
       "      <td>877</td>\n",
       "      <td>0.428223</td>\n",
       "      <td>900</td>\n",
       "      <td>0.439453</td>\n",
       "      <td>879</td>\n",
       "      <td>0.429199</td>\n",
       "      <td>984</td>\n",
       "      <td>0.480469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.638483</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>35</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>65</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>125</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>247</td>\n",
       "      <td>0.482422</td>\n",
       "      <td>495</td>\n",
       "      <td>0.483398</td>\n",
       "      <td>970</td>\n",
       "      <td>0.473633</td>\n",
       "      <td>1015</td>\n",
       "      <td>0.495605</td>\n",
       "      <td>986</td>\n",
       "      <td>0.481445</td>\n",
       "      <td>1051</td>\n",
       "      <td>0.513184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.688879</td>\n",
       "      <td>0.008468</td>\n",
       "      <td>35</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>65</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>128</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>252</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>470</td>\n",
       "      <td>0.458984</td>\n",
       "      <td>972</td>\n",
       "      <td>0.474609</td>\n",
       "      <td>942</td>\n",
       "      <td>0.459961</td>\n",
       "      <td>946</td>\n",
       "      <td>0.461914</td>\n",
       "      <td>975</td>\n",
       "      <td>0.476074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.334482</td>\n",
       "      <td>0.009203</td>\n",
       "      <td>36</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>70</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>137</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>286</td>\n",
       "      <td>0.558594</td>\n",
       "      <td>548</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>1119</td>\n",
       "      <td>0.546387</td>\n",
       "      <td>1115</td>\n",
       "      <td>0.544434</td>\n",
       "      <td>1114</td>\n",
       "      <td>0.543945</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.526367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.066603</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>27</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>60</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>124</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>250</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>516</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>996</td>\n",
       "      <td>0.486328</td>\n",
       "      <td>1039</td>\n",
       "      <td>0.507324</td>\n",
       "      <td>1028</td>\n",
       "      <td>0.501953</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.507812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.356540</td>\n",
       "      <td>0.014601</td>\n",
       "      <td>26</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>56</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>116</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>224</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>447</td>\n",
       "      <td>0.436523</td>\n",
       "      <td>920</td>\n",
       "      <td>0.449219</td>\n",
       "      <td>880</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>917</td>\n",
       "      <td>0.447754</td>\n",
       "      <td>934</td>\n",
       "      <td>0.456055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.845111</td>\n",
       "      <td>0.020689</td>\n",
       "      <td>25</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>49</td>\n",
       "      <td>0.382812</td>\n",
       "      <td>106</td>\n",
       "      <td>0.414062</td>\n",
       "      <td>201</td>\n",
       "      <td>0.392578</td>\n",
       "      <td>431</td>\n",
       "      <td>0.420898</td>\n",
       "      <td>862</td>\n",
       "      <td>0.420898</td>\n",
       "      <td>852</td>\n",
       "      <td>0.416016</td>\n",
       "      <td>832</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>937</td>\n",
       "      <td>0.457520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>36.209156</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>35</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>66</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>124</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>255</td>\n",
       "      <td>0.498047</td>\n",
       "      <td>508</td>\n",
       "      <td>0.496094</td>\n",
       "      <td>994</td>\n",
       "      <td>0.485352</td>\n",
       "      <td>1022</td>\n",
       "      <td>0.499023</td>\n",
       "      <td>1029</td>\n",
       "      <td>0.502441</td>\n",
       "      <td>1048</td>\n",
       "      <td>0.511719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>29.604319</td>\n",
       "      <td>0.006739</td>\n",
       "      <td>32</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>61</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>120</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>254</td>\n",
       "      <td>0.496094</td>\n",
       "      <td>483</td>\n",
       "      <td>0.471680</td>\n",
       "      <td>962</td>\n",
       "      <td>0.469727</td>\n",
       "      <td>952</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>960</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.488770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>42.897360</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>32</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>61</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>118</td>\n",
       "      <td>0.460938</td>\n",
       "      <td>252</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>528</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.529785</td>\n",
       "      <td>1044</td>\n",
       "      <td>0.509766</td>\n",
       "      <td>1080</td>\n",
       "      <td>0.527344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    R2-cost contribution  |b| / ||w||_1  # training active,n=64  \\\n",
       "0               2.021811       0.021495                      28   \n",
       "1               0.402139       0.043486                      42   \n",
       "2               1.278993       0.023220                      27   \n",
       "3               4.191544       0.007405                      29   \n",
       "4              21.394932       0.013318                      37   \n",
       "5               9.869071       0.007666                      33   \n",
       "6               5.596780       0.010253                      36   \n",
       "7               9.045827       0.005964                      33   \n",
       "8               3.535087       0.006273                      35   \n",
       "9               9.782236       0.000971                      29   \n",
       "10             12.291021       0.010387                      29   \n",
       "11              3.097131       0.012219                      31   \n",
       "12             12.638483       0.001717                      35   \n",
       "13              6.688879       0.008468                      35   \n",
       "14              3.334482       0.009203                      36   \n",
       "15              3.066603       0.000206                      27   \n",
       "16              5.356540       0.014601                      26   \n",
       "17              0.845111       0.020689                      25   \n",
       "18             36.209156       0.002142                      35   \n",
       "19             29.604319       0.006739                      32   \n",
       "20             42.897360       0.004359                      32   \n",
       "\n",
       "    % training active,n=64  # training active,n=128  % training active,n=128  \\\n",
       "0                 0.437500                       50                 0.390625   \n",
       "1                 0.656250                       90                 0.703125   \n",
       "2                 0.421875                       52                 0.406250   \n",
       "3                 0.453125                       56                 0.437500   \n",
       "4                 0.578125                       76                 0.593750   \n",
       "5                 0.515625                       67                 0.523438   \n",
       "6                 0.562500                       72                 0.562500   \n",
       "7                 0.515625                       71                 0.554688   \n",
       "8                 0.546875                       68                 0.531250   \n",
       "9                 0.453125                       63                 0.492188   \n",
       "10                0.453125                       58                 0.453125   \n",
       "11                0.484375                       57                 0.445312   \n",
       "12                0.546875                       65                 0.507812   \n",
       "13                0.546875                       65                 0.507812   \n",
       "14                0.562500                       70                 0.546875   \n",
       "15                0.421875                       60                 0.468750   \n",
       "16                0.406250                       56                 0.437500   \n",
       "17                0.390625                       49                 0.382812   \n",
       "18                0.546875                       66                 0.515625   \n",
       "19                0.500000                       61                 0.476562   \n",
       "20                0.500000                       61                 0.476562   \n",
       "\n",
       "    # training active,n=256  % training active,n=256  # training active,n=512  \\\n",
       "0                        98                 0.382812                      206   \n",
       "1                       181                 0.707031                      357   \n",
       "2                       107                 0.417969                      206   \n",
       "3                       124                 0.484375                      232   \n",
       "4                       152                 0.593750                      297   \n",
       "5                       130                 0.507812                      271   \n",
       "6                       147                 0.574219                      291   \n",
       "7                       146                 0.570312                      282   \n",
       "8                       141                 0.550781                      271   \n",
       "9                       128                 0.500000                      259   \n",
       "10                      111                 0.433594                      234   \n",
       "11                      111                 0.433594                      226   \n",
       "12                      125                 0.488281                      247   \n",
       "13                      128                 0.500000                      252   \n",
       "14                      137                 0.535156                      286   \n",
       "15                      124                 0.484375                      250   \n",
       "16                      116                 0.453125                      224   \n",
       "17                      106                 0.414062                      201   \n",
       "18                      124                 0.484375                      255   \n",
       "19                      120                 0.468750                      254   \n",
       "20                      118                 0.460938                      252   \n",
       "\n",
       "    % training active,n=512  # training active,n=1024  \\\n",
       "0                  0.402344                       398   \n",
       "1                  0.697266                       725   \n",
       "2                  0.402344                       384   \n",
       "3                  0.453125                       483   \n",
       "4                  0.580078                       578   \n",
       "5                  0.529297                       558   \n",
       "6                  0.568359                       568   \n",
       "7                  0.550781                       573   \n",
       "8                  0.529297                       536   \n",
       "9                  0.505859                       522   \n",
       "10                 0.457031                       474   \n",
       "11                 0.441406                       450   \n",
       "12                 0.482422                       495   \n",
       "13                 0.492188                       470   \n",
       "14                 0.558594                       548   \n",
       "15                 0.488281                       516   \n",
       "16                 0.437500                       447   \n",
       "17                 0.392578                       431   \n",
       "18                 0.498047                       508   \n",
       "19                 0.496094                       483   \n",
       "20                 0.492188                       528   \n",
       "\n",
       "    % training active,n=1024  # training active,n=2048  \\\n",
       "0                   0.388672                       775   \n",
       "1                   0.708008                      1449   \n",
       "2                   0.375000                       808   \n",
       "3                   0.471680                       995   \n",
       "4                   0.564453                      1185   \n",
       "5                   0.544922                      1082   \n",
       "6                   0.554688                      1164   \n",
       "7                   0.559570                      1148   \n",
       "8                   0.523438                      1102   \n",
       "9                   0.509766                      1071   \n",
       "10                  0.462891                       904   \n",
       "11                  0.439453                       877   \n",
       "12                  0.483398                       970   \n",
       "13                  0.458984                       972   \n",
       "14                  0.535156                      1119   \n",
       "15                  0.503906                       996   \n",
       "16                  0.436523                       920   \n",
       "17                  0.420898                       862   \n",
       "18                  0.496094                       994   \n",
       "19                  0.471680                       962   \n",
       "20                  0.515625                      1024   \n",
       "\n",
       "    % training active,n=2048  # validation active  % validation active  \\\n",
       "0                   0.378418                  782             0.381836   \n",
       "1                   0.707520                 1446             0.706055   \n",
       "2                   0.394531                  753             0.367676   \n",
       "3                   0.485840                  976             0.476562   \n",
       "4                   0.578613                 1138             0.555664   \n",
       "5                   0.528320                 1126             0.549805   \n",
       "6                   0.568359                 1118             0.545898   \n",
       "7                   0.560547                 1108             0.541016   \n",
       "8                   0.538086                 1089             0.531738   \n",
       "9                   0.522949                 1030             0.502930   \n",
       "10                  0.441406                  916             0.447266   \n",
       "11                  0.428223                  900             0.439453   \n",
       "12                  0.473633                 1015             0.495605   \n",
       "13                  0.474609                  942             0.459961   \n",
       "14                  0.546387                 1115             0.544434   \n",
       "15                  0.486328                 1039             0.507324   \n",
       "16                  0.449219                  880             0.429688   \n",
       "17                  0.420898                  852             0.416016   \n",
       "18                  0.485352                 1022             0.499023   \n",
       "19                  0.469727                  952             0.464844   \n",
       "20                  0.500000                 1085             0.529785   \n",
       "\n",
       "    # generalization active  % generalization active  # ood active  \\\n",
       "0                       784                 0.382812           925   \n",
       "1                      1453                 0.709473          1220   \n",
       "2                       787                 0.384277           887   \n",
       "3                       968                 0.472656           982   \n",
       "4                      1165                 0.568848          1046   \n",
       "5                      1102                 0.538086          1080   \n",
       "6                      1145                 0.559082          1035   \n",
       "7                      1102                 0.538086          1045   \n",
       "8                      1068                 0.521484          1037   \n",
       "9                      1059                 0.517090          1002   \n",
       "10                      930                 0.454102           973   \n",
       "11                      879                 0.429199           984   \n",
       "12                      986                 0.481445          1051   \n",
       "13                      946                 0.461914           975   \n",
       "14                     1114                 0.543945          1078   \n",
       "15                     1028                 0.501953          1040   \n",
       "16                      917                 0.447754           934   \n",
       "17                      832                 0.406250           937   \n",
       "18                     1029                 0.502441          1048   \n",
       "19                      960                 0.468750          1001   \n",
       "20                     1044                 0.509766          1080   \n",
       "\n",
       "    % ood active  \n",
       "0       0.451660  \n",
       "1       0.595703  \n",
       "2       0.433105  \n",
       "3       0.479492  \n",
       "4       0.510742  \n",
       "5       0.527344  \n",
       "6       0.505371  \n",
       "7       0.510254  \n",
       "8       0.506348  \n",
       "9       0.489258  \n",
       "10      0.475098  \n",
       "11      0.480469  \n",
       "12      0.513184  \n",
       "13      0.476074  \n",
       "14      0.526367  \n",
       "15      0.507812  \n",
       "16      0.456055  \n",
       "17      0.457520  \n",
       "18      0.511719  \n",
       "19      0.488770  \n",
       "20      0.527344  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for r in rs:\n",
    "    print(\"r =\",r)\n",
    "    U = np.load(job_name+f\"_labelnoise0/r{r}U.npy\")\n",
    "    Sigma = np.load(job_name+f\"_labelnoise0/r{r}Sigma.npy\")\n",
    "    V = np.load(job_name+f\"_labelnoise0/r{r}V.npy\")\n",
    "    A = np.load(job_name+f\"_labelnoise0/r{r}A.npy\")\n",
    "    B = np.load(job_name+f\"_labelnoise0/r{r}B.npy\")\n",
    "    W = (U*Sigma)@V.T\n",
    "    rowwise1norms = np.linalg.norm(W,axis=1,ord=1)\n",
    "    ratios = np.abs(B) / np.linalg.norm(W,axis=1,ord=1)\n",
    "    rowwise2norms = np.linalg.norm(W,axis=1,ord=2)\n",
    "    units = pd.DataFrame({\"R2-cost contribution\":np.abs(A)*rowwise2norms,\"|b| / ||w||_1\":ratios})\n",
    "    # units[\"[-1/2,1/2]\"] = ratios <= 1/2\n",
    "    # units[\"[-1,1] but not [-1/2,1/2]\"] = (1/2 < ratios) * (ratios <= 1)\n",
    "    # units[\"not [-1,1]\"] = ratios > 1\n",
    "\n",
    "    datasetsize = 2048\n",
    "    trainX = gen_data(datasetsize=datasetsize,r=r,seed=1,std=0,labelnoiseseed=0)[0]\n",
    "    for n in res.n.unique():\n",
    "        units[f\"# training active,n={n}\"] = ((W@trainX[:n].cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "        units[f\"% training active,n={n}\"] = units[f\"# training active,n={n}\"] / n\n",
    "    units[\"# validation active\"] = ((W@validationX.cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "    units[\"% validation active\"] = units[\"# validation active\"] / datasetsize\n",
    "    units[\"# generalization active\"] = ((W@generalizationX.cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "    units[\"% generalization active\"] = units[\"# generalization active\"] / datasetsize\n",
    "    units[\"# ood active\"] = ((W@oodX.cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "    units[\"% ood active\"] = units[\"# ood active\"] / datasetsize\n",
    "\n",
    "    print(\"\\nTOTALS:\\n~~~~~~~\\n\",units.sum())\n",
    "    print(\"\\nunit-wise table:\\n~~~~~~~\\n\")\n",
    "    display(units)\n",
    "    shift = -0.2\n",
    "    width = 0.2\n",
    "    for n in res.n.unique():\n",
    "        if n >= 1024:\n",
    "            plt.bar(units.index+shift,units[f\"% training active,n={n}\"],label=f\"train,$n={n}$\",width=width,tick_label=units[\"R2-cost contribution\"].round(1))\n",
    "            shift += width\n",
    "    # plt.bar(units.index,units[\"% generalization active\"],label=\"gen\",width=width)\n",
    "    plt.bar(units.index+shift,units[\"% ood active\"],label=\"ood\",width=width)\n",
    "    plt.ylim(0,1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"How many samples is each unit active on? $r =$ {r}\")\n",
    "    plt.xlabel(\"ReLU Unit, labeled by R2-cost contribution\")\n",
    "    plt.ylabel(\"Proportion of samples\")\n",
    "    plt.axhline(0.5,linestyle=\":\",color=\"k\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"samples_active_by_unit_r{r}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec05e2a",
   "metadata": {
    "id": "8ec05e2a"
   },
   "source": [
    "# Active Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pFRGq1re-Uzi",
   "metadata": {
    "id": "pFRGq1re-Uzi"
   },
   "source": [
    "## evaluate gradients and compute singular values and active subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9f0jES0e-cYG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4768,
     "status": "ok",
     "timestamp": 1691614322192,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 300
    },
    "id": "9f0jES0e-cYG",
    "outputId": "5d4441ca-c283-47e7-8957-aaff36383b6a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grads = []\n",
    "sv = []\n",
    "active_subspace = []\n",
    "subspace_dist = []\n",
    "\n",
    "for rownum, row in res.iterrows():\n",
    "    #compute ground truth active subspace\n",
    "    funcseed = 42\n",
    "    d = 20\n",
    "    k = d+1\n",
    "    ln = row['sigma']\n",
    "    if int(ln) == ln:\n",
    "        ln = int(ln)\n",
    "    U = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}U.npy\")\n",
    "    Sigma = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}Sigma.npy\")\n",
    "    V = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}V.npy\")\n",
    "    W = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}W.npy\")\n",
    "    A = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}A.npy\")\n",
    "    B = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}B.npy\")\n",
    "\n",
    "    #evaluate gradients\n",
    "    generalizationX.requires_grad = True\n",
    "    predY = row[\"Model\"](generalizationX)\n",
    "    grad = torch.autograd.grad(predY, generalizationX,\n",
    "                            grad_outputs=torch.ones_like(predY),\n",
    "                            create_graph=True)[0].detach().cpu().numpy()\n",
    "    grads.append(grad)\n",
    "    #compute active subspace and singular values\n",
    "    Uhat,Shat,VhatT = np.linalg.svd(grad)\n",
    "    Vhat = VhatT.T[:,:row[\"r\"]] #form the basis for the active subspace\n",
    "    active_subspace.append(Vhat)\n",
    "    sv.append(Shat)\n",
    "\n",
    "    subspace_dist.append(np.linalg.norm(V@V.T - Vhat@Vhat.T,2))\n",
    "\n",
    "res[\"Gradient Evaluations\"] = grads\n",
    "res[\"Gradient Singular Values\"] = sv\n",
    "res[\"Active Subspace\"] = active_subspace\n",
    "res[\"Active Subspace Distance\"] = subspace_dist\n",
    "res[\"Angle Error (Degrees)\"] = np.degrees(np.arcsin(res[\"Active Subspace Distance\"]))\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297500cd",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QfTgwsYWt0JN",
   "metadata": {
    "id": "QfTgwsYWt0JN"
   },
   "source": [
    "##  determine the lambda parameter that gets the best Validation MSE for each (r,n,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RJvhVj2QsOzz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "RJvhVj2QsOzz",
    "outputId": "f5ab0e61-f93e-4d47-c809-af5ef4d06e06",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validationmse_vs_lambda = res.pivot_table(values=\"Validation MSE\",index = (\"r\",\"sigma\",\"n\",\"L\",\"Activations\"),columns=[\"lambda\"])\n",
    "validationmse_vs_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff1bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestlambda = validationmse_vs_lambda.idxmin(axis=1)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(bestlambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K4ItJEBcjtcZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "K4ItJEBcjtcZ",
    "outputId": "a6ab9ade-ea9d-4b83-b14b-29dd2811b57d"
   },
   "outputs": [],
   "source": [
    "mask = [row[\"lambda\"] == bestlambda[row[\"r\"]][row[\"sigma\"]][row[\"n\"]][row[\"L\"]][row[\"Activations\"]] for rowindex,row in res.iterrows()]\n",
    "res = res[mask]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fabcf44",
   "metadata": {
    "id": "QfTgwsYWt0JN"
   },
   "source": [
    "##  determine the L parameter that gets the best validation MSE for each (r,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc2546",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "RJvhVj2QsOzz",
    "outputId": "f5ab0e61-f93e-4d47-c809-af5ef4d06e06",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validationmse_vs_L = res.pivot_table(values=\"Validation MSE\",index = (\"r\",\"sigma\",\"n\",\"Activations\"),columns=[\"L\"])\n",
    "validationmse_vs_L = validationmse_vs_L.iloc[:,1:]\n",
    "bestL = validationmse_vs_L.idxmin(axis=1)\n",
    "pd.concat((validationmse_vs_L,bestL),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb5ccf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "K4ItJEBcjtcZ",
    "outputId": "a6ab9ade-ea9d-4b83-b14b-29dd2811b57d"
   },
   "outputs": [],
   "source": [
    "mask = [row[\"L\"] == bestL[row[\"r\"]][row[\"sigma\"]][row[\"n\"]][row[\"Activations\"]] for rowindex,row in res.iterrows()]\n",
    "bestLres = res[mask]\n",
    "bestLres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daefd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestLres.sort_values(by=['r','n',\"sigma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f1d0c",
   "metadata": {},
   "source": [
    "## What are the chosen lambda and L for each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestres = pd.concat((res[res[\"L\"] == 2],bestLres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(bestres.loc[:,:\"lambda\"].pivot_table(index=[\"r\",\"sigma\",\"n\",\"L\"],values=[\"lambda\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364946f",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ecf870",
   "metadata": {},
   "source": [
    "## plots of all the singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cacc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the singular values\n",
    "for std in labelnoise:\n",
    "    f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(10,4.8))\n",
    "    for rownum,row in res.iterrows():\n",
    "        if row['sigma'] == std:\n",
    "            whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "            whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "            print(whichrow,whichcol)\n",
    "            print(rf\"{row['r']},{row['n']},{row['L']}\",row[\"Gradient Singular Values\"]/np.sqrt(2048),whichrow,whichcol)\n",
    "            ax[whichrow,whichcol].semilogy(row[\"Gradient Singular Values\"]/np.sqrt(2048),label=rf\"$L={row['L']}$\",linewidth=1,alpha=0.7,marker=\".\")\n",
    "            ax[whichrow,whichcol].set_xticks(list(range(3,20,4)),list(range(4,21,4)))\n",
    "            ax[whichrow,whichcol].set_ylim(10**-9,10**3)\n",
    "            ax[0,whichcol].set_title(rf\"$n={row['n']}$\")\n",
    "            ax[-1,whichcol].set_xlabel(rf\"Index, $k$\")\n",
    "    plt.subplot(2,len(res.n.unique()),1)\n",
    "    leg = plt.legend()\n",
    "    leg = plt.legend(bbox_to_anchor=(-1, 1))\n",
    "    leg.get_frame().set_edgecolor('b')\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "    plt.subplot(2,len(res.n.unique()),1)\n",
    "    plt.ylabel(r\"$r=1$\"+\"\\n\"+r\"$\\sigma_k(\\hat f;\\rho)$\")\n",
    "    plt.yticks([10**p for p in range(-12,3,2)])\n",
    "    plt.subplot(2,len(res.n.unique()),len(res.n.unique())+1)\n",
    "    plt.ylabel(r\"$r=2$\"+\"\\n\"+r\"$\\sigma_k(\\hat f;\\rho)$\")\n",
    "    plt.yticks([10**p for p in range(-12,3,2)])\n",
    "    plt.suptitle(rf\"Singular Values of Trained Networks, $\\sigma =$ {std}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(job_name+f\"_labelnoise{std}/sv.png\",dpi=300)#,bbox_extra_artists=(leg,), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vKIELwC_GeZK",
   "metadata": {
    "id": "vKIELwC_GeZK"
   },
   "source": [
    "## Plots of L vs Validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gvOUGojKGWvw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 1932,
     "status": "ok",
     "timestamp": 1701129174191,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "gvOUGojKGWvw",
    "outputId": "378581a5-d4b5-4eed-8e60-29b5134124ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for std in res[\"sigma\"].unique():\n",
    "    f, ax = plt.subplots(ncols=len(res.r.unique()),nrows=1, sharex=True, sharey=False, figsize=(10,4.8))\n",
    "    for rnum,r in enumerate(res.r.unique()):\n",
    "        for n in res.n.unique():\n",
    "                res_rnstd = res[(res.r == r) * (res.n == n) * (res[\"sigma\"] == std)]\n",
    "                ax[rnum].scatter(res_rnstd.L,res_rnstd[[\"Validation MSE\"]])\n",
    "                ax[rnum].semilogy(res_rnstd.L,res_rnstd[[\"Validation MSE\"]],label=rf\"$n={n}$\")\n",
    "                for _,model in res_rnstd.iterrows():\n",
    "                    text = rf'$\\lambda = {model[\"lambda\"]:.0e}$' + f'\\nfit {model[\"Final Train MSE\"]:.1e}\\nwd{model[\"Final Weight Decay\"]:.1e}'\n",
    "                    ax[rnum].annotate(text,[model.L,model[[\"Validation MSE\"]]],fontsize=1)\n",
    "        ax[rnum].set_xlabel(\"$L$ number of layers\")\n",
    "        ax[rnum].set_title(rf\"$r={r}$\")\n",
    "        if std > 0:\n",
    "            ax[rnum].axhline(y=std**2, color='k', linestyle=':',label=\"$\\sigma^2$\")\n",
    "        ax[0].set_ylabel(\"Validation MSE\")\n",
    "        f.suptitle(rf\"Validation MSE for best $\\lambda$ values, $\\sigma$ = {std}\")\n",
    "    ax[0].legend()\n",
    "    f.tight_layout()\n",
    "    if int(std) == std:\n",
    "        std = int(std)\n",
    "    f.savefig(job_name+f\"_labelnoise{std}/ValidationMSE.png\",dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad12a9",
   "metadata": {},
   "source": [
    "## Performance metrics with/without linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53158aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = {  \n",
    "    0   :\"C0\",\n",
    "    0.25:\"C1\",\n",
    "    0.5 :\"C2\",\n",
    "    1   :\"C3\"\n",
    "}\n",
    "handles = [\n",
    "    Line2D([0], [0], color=color, ls='-', label=rf\"$\\sigma =${sigma}\") for sigma,color in colors.items()\n",
    "]\n",
    "labels = {\n",
    "    \"without linear layers\":\"-\",\n",
    "    \"with linear layers\"   :\"--\",\n",
    "}\n",
    "markers = {\n",
    "    \"without linear layers\":\".\",\n",
    "    \"with linear layers\"   :\"x\",\n",
    "}\n",
    "handles += [\n",
    "    Line2D([0], [0], color='k', ls=ls, label=label, marker = markers[label], markersize=4) for label,ls in labels.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f60e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranktol = 1e-3\n",
    "handles += [Line2D([0], [0], color='k', ls=':', label=\"rank tolerance cutoff\")]\n",
    "f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(15,5))\n",
    "for row,r in enumerate(res.r.unique()):\n",
    "    for col,n in enumerate(res.n.unique()):\n",
    "        for sigma in labelnoise:\n",
    "            for label,ls in labels.items():\n",
    "                if label == \"with linear layers\":\n",
    "                    curr = bestLres[(bestLres.r == r) * (bestLres.n == n) * (bestLres[\"sigma\"] == sigma)]\n",
    "                elif label == \"without linear layers\":\n",
    "                    curr = res[(res.L == 2) * (res.n == n) * (res.r == r) * (res[\"sigma\"]==sigma)]\n",
    "                print(r,n,label,sigma,curr[\"Gradient Singular Values\"].values[0]/np.sqrt(2048))\n",
    "                marker = markers[label]\n",
    "                ax[row,col].semilogy(curr[\"Gradient Singular Values\"].values[0]/np.sqrt(2048),linestyle=ls,linewidth=1,alpha=0.7,marker=marker,markersize=4,color=colors[sigma])\n",
    "        ax[row,col].axhline(y=ranktol, color='k', linestyle=':',alpha=1, label = rf\"rank tolerance cutoff, $\\varepsilon = {ranktol}$\")\n",
    "        ax[row,col].set_xticks(list(range(3,20,4)),list(range(4,21,4)))\n",
    "        ax[-1,col].set_xlabel(rf\"Index, $k$\")\n",
    "        ax[row,0].set_ylabel(rf\"$r={r}$\"+\"\\n\"+r\"$\\sigma_k(\\hat f;\\rho)$\")\n",
    "        ax[0,0].set_yticks([10**p for p in range(-9,3,2)])\n",
    "        ax[row,col].set_ylim(10**-9,10**3)\n",
    "        ax[0,col].set_title(rf\"$n={n}$\")\n",
    "        ax[0,-1].legend(handles=handles, loc='best', bbox_to_anchor=(1.05,1))\n",
    "plt.suptitle(rf\"Singular Values of Trained Networks after $\\lambda, L$ tuning\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(job_name+f\"_labelnoise_sv.png\",dpi=600)#,bbox_extra_artists=(leg,), bbox_inches='tight')\n",
    "plt.show()\n",
    "handles.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#active subspace error plot\n",
    "f, ax = plt.subplots(ncols=len(res.r.unique()),nrows=2, sharex=True, sharey=False, figsize=(10,9))\n",
    "for row,metric in enumerate([rf\"Effective Rank, $\\varepsilon = {ranktol}$\",\"Angle Error (Degrees)\"]):\n",
    "    for col,r in enumerate(res.r.unique()):\n",
    "        for sigma in [0,0.25,0.5,1]:\n",
    "            ax[row,0].set_ylabel(f\"{metric}\")\n",
    "            for label,ls in labels.items():\n",
    "                if label == \"with linear layers\":\n",
    "                    curr = bestLres[(bestLres.r == r) * (bestLres[\"sigma\"] == sigma)]\n",
    "                elif label == \"without linear layers\":\n",
    "                    curr = res[(res.L == 2) * (res.r == r) * (res[\"sigma\"]==sigma)]\n",
    "                if metric == \"Angle Error (Degrees)\":\n",
    "                    points = curr[[metric]].values[:,0]\n",
    "                elif metric == rf\"Effective Rank, $\\varepsilon = {ranktol}$\":\n",
    "                    points = (np.array(curr[\"Gradient Singular Values\"].tolist())/np.sqrt(2048) > ranktol).sum(axis=1)\n",
    "                    ax[row,col].set_yticks(np.arange(0,21,2))\n",
    "                    ax[row,col].set_ylim(0,20.5)\n",
    "                marker = markers[label]\n",
    "                ax[row,col].plot(curr.n,points,\n",
    "                                    linestyle=ls,\n",
    "                                    color=colors[sigma],\n",
    "                                    marker=marker,\n",
    "                                    markersize=4,\n",
    "                                    alpha=0.8)\n",
    "                for (_,model),y in zip(curr.iterrows(),points):\n",
    "                    text = rf'$\\lambda = {model[\"lambda\"]:.0e}$' + f'\\nL = {model[\"L\"]}\\nfit {model[\"Final Train MSE\"]:.1e}\\nwd{model[\"Final Weight Decay\"]:.1e}\\nVal{model[\"Validation MSE\"]:.1e}'\n",
    "                    ax[row,col].annotate(text,[model[\"n\"],y],fontsize=1)\n",
    "        #plot set up\n",
    "        ax[0,col].set_title(rf\"$r={r}$\") \n",
    "        ax[0,1].legend(handles=handles, loc='best', bbox_to_anchor=(1.05,1))\n",
    "        ax[row,col].set_xscale(\"log\",base=2)\n",
    "        ax[1,col].set_xlabel(\"$n$ number of samples\")\n",
    "\n",
    "plt.suptitle(f\"Active Subspaces after $\\lambda, L$ tuning\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(job_name+f\"Active Subspaces after tuning lambda and L.png\",dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a79e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generalization and OOD errors\n",
    "handles += [\n",
    "        Line2D([0], [0], color='k', ls=':', label='$\\sigma^2$, variance of label noise'),\n",
    "]\n",
    "\n",
    "for metric in ['Generalization MSE','Out of Distribution MSE']:\n",
    "    standard_errors = metric[:-3] + 'SEM'\n",
    "    f, ax = plt.subplots(ncols=len(res.r.unique()),nrows=2, sharex=True, sharey=\"row\", figsize=(10,9))\n",
    "    #just the data without label noise in the first row\n",
    "    for col,r in enumerate(res.r.unique()):\n",
    "        for row,sigmas in enumerate([[0],[0.25,0.5,1]]):\n",
    "            for sigma in sigmas:\n",
    "                for label,ls in labels.items():\n",
    "                    if label == \"with linear layers\":\n",
    "                        curr = bestLres[(bestLres.r == r) * (bestLres[\"sigma\"] == sigma)]\n",
    "                    elif label == \"without linear layers\":\n",
    "                        curr = res[(res.L == 2) * (res.r == r) * (res[\"sigma\"]==sigma)]\n",
    "                    points = curr[[metric]].values[:,0]\n",
    "                    errorbars = curr[[standard_errors]].values[:,0]\n",
    "                    marker = markers[label]\n",
    "                    ax[row,col].plot(curr.n,points,\n",
    "                                        linestyle=ls,\n",
    "                                        marker=marker,\n",
    "                                        markersize=4,\n",
    "                                        color=colors[sigma],\n",
    "                                        alpha=0.8)\n",
    "                    #horizontal dashed line for minimal possible MSE (ie sigma^2) in plots with label noise\n",
    "                    ax[1,col].axhline(y=sigma**2, color=colors[sigma], linestyle=':',alpha=0.3)\n",
    "                    for (_,model),y in zip(curr.iterrows(),points):\n",
    "                        text = rf'$\\lambda = {model[\"lambda\"]:.0e}$' + f'\\nL = {model[\"L\"]}\\nfit {model[\"Final Train MSE\"]:.1e}\\nwd{model[\"Final Weight Decay\"]:.1e}\\nVal{model[\"Validation MSE\"]:.1e}'\n",
    "                        ax[row,col].annotate(text,[model[\"n\"],y],fontsize=1)\n",
    "            #plot set up\n",
    "            ax[row,col].set_xscale(\"log\",base=2)\n",
    "            ax[row,col].set_yscale(\"log\",base=10)\n",
    "            ax[row,0].set_ylabel(f\"{metric}\")\n",
    "            ax[0,col].set_title(rf\"$r={r}$\") \n",
    "            ax[1,col].set_xlabel(\"$n$ number of samples\")\n",
    "            ax[0,1].legend(handles=handles, loc='best', bbox_to_anchor=(1.05,1))\n",
    "    plt.suptitle(f\"{metric}\" + r\" after $\\lambda, L$ tuning\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(job_name+f\"{metric}.png\",dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ba908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     pd.options.display.float_format = '{:.1e}'.format\n",
    "#     display(bestres.loc[:,:\"Generalization MSE\"].pivot_table(index=[\"r\",\"sigma\",\"n\",\"L\"],values=[\"Generalization MSE\"])-0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4178b7",
   "metadata": {},
   "source": [
    "## Visualizing Functions in Low Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1\n",
    "#create functions\n",
    "k = d+1\n",
    "U = np.load(job_name+f\"_labelnoise{std}/r{r}U.npy\")\n",
    "Sigma = np.load(job_name+f\"_labelnoise{std}/r{r}Sigma.npy\")\n",
    "V = np.load(job_name+f\"_labelnoise{std}/r{r}V.npy\")\n",
    "A = np.load(job_name+f\"_labelnoise{std}/r{r}A.npy\")\n",
    "B = np.load(job_name+f\"_labelnoise{std}/r{r}B.npy\")\n",
    "def g(z): #active subspace function\n",
    "    hidden_layer = (U*Sigma)@z\n",
    "    hidden_layer = hidden_layer.T + B\n",
    "    hidden_layer = np.maximum(0,hidden_layer).T\n",
    "    return A@hidden_layer\n",
    "def f(x): #teacher network\n",
    "    z = V.T@x    \n",
    "    return g(z)\n",
    "v = np.load(job_name+f\"_labelnoise{std}/r{r}V.npy\")[:,0]\n",
    "vtorch = torch.from_numpy(np.float32(v)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584d1f3",
   "metadata": {},
   "source": [
    "plot of projetion onto true 1d subspace of both models and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a79997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onedimproj(f,v,z,istorch=True):\n",
    "    \"\"\"\n",
    "    Create a projection of the function onto 1D inputs. More specifically, return f(vz')\n",
    "    \"\"\"\n",
    "    #create points vz at which to evaluate f\n",
    "    vz = np.outer(v, z)\n",
    "    if istorch:\n",
    "        vz = np.float32(vz)\n",
    "        vz = torch.from_numpy(vz).to(device).T\n",
    "        #function evals\n",
    "        with torch.no_grad():\n",
    "            fvz = f(vz)\n",
    "            fvz = fvz.cpu().numpy()\n",
    "    else:\n",
    "        #function evals\n",
    "        fvz = f(vz)\n",
    "    return fvz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2fd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "fig.suptitle(f\"Ground Truth vs. Models, $r = {r}$, Projected onto True Subspace\")\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "z = np.linspace(-0.1,0.1,500)\n",
    "#plotting ground truth\n",
    "ax.plot(z,g(z.reshape(1,-1)),label=\"Ground Truth\")\n",
    "#plotting models\n",
    "for rownum,row in bestLres[(bestLres.r ==1)*(bestLres.sigma ==0)*(bestLres.n >= 1024)].iterrows():\n",
    "    ax.plot(z,onedimproj(row.Model,v,z),label=rf\"$\\sigma={row.sigma}, n={row.n}$\",alpha=0.5)\n",
    "plt.legend()\n",
    "plt.savefig(f\"ooderrors_onedimvizr{r}domain1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d1bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "(oodX@vtorch).shape,(trainX@vtorch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in [0.25,2.5]:\n",
    "    z = np.linspace(-domain,domain,500)\n",
    "    fig = plt.figure(figsize = (10,6))\n",
    "    fig.clf()\n",
    "    fig.suptitle(f\"| Ground Truth - Model |, Projected onto True Subspace ($r = {r}$)\")\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    #plotting models and samples\n",
    "    for rownum,row in bestLres[(bestLres.r ==1)*(bestLres.sigma ==0)*(bestLres.n >= 1024)].iterrows():\n",
    "        abserr = np.abs(onedimproj(row.Model,v,z)[:,0]-g(z.reshape(1,-1)))\n",
    "        ax.plot(z,abserr,label=rf\"$\\sigma={row.sigma}, n={row.n}$\",alpha=0.5)\n",
    "    if domain > 1:\n",
    "        with torch.no_grad():\n",
    "            counts,bins = np.histogram((oodX@vtorch).cpu(),bins=50,density=True)\n",
    "            ax.hist(bins[:-1], bins, weights=counts*0.1,label=\"out-distribution samples\",alpha=0.3)\n",
    "            counts,bins = np.histogram((generalizationX@vtorch).cpu(),bins=50,density=True)\n",
    "            ax.hist(bins[:-1], bins, weights=counts*0.1,label=\"in-distribution samples\" ,alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"ooderrors_onedimvizr{r}domain{domain}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da7123",
   "metadata": {},
   "source": [
    "plot of projetion onto true 1d subspace of both models and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projviz(ax,f,d=20,domain=None,projseed=0,istorch=True,verbose=False,kind=\"surface\",**kwargs):\n",
    "    \"\"\"\n",
    "    Plot a projection of the function onto 2D inputs. More specifically, plot visualization of a \n",
    "    function f with d dimensional input and 1 dimension output by plotting f(Az) where A is a \n",
    "    random d x 2 matrix with orthonormal columns and z is a 2-dimensional meshgrid.\n",
    "    \"\"\"\n",
    "    #create a random projection matrix A\n",
    "    A = ortho_group(d,seed=projseed).rvs()[:,:2]\n",
    "    Asupnorm = np.linalg.norm(A,ord=np.inf)\n",
    "    if domain is None:\n",
    "        domain = np.linspace(-1,1,500)\n",
    "        # domain = np.linspace(-2/Asupnorm,2/Asupnorm,500)\n",
    "    if verbose: print(\"random projection d x 2:\\n\",A)\n",
    "    #create 2D meshgrid of domain\n",
    "    Z1,Z2 = np.meshgrid(domain,domain)\n",
    "    numpnts = len(domain)\n",
    "    #create points Az at which to evaluate f\n",
    "    AZ = np.outer(A[:,0], Z1) + np.outer(A[:,1], Z2)\n",
    "    if istorch:\n",
    "        AZ = np.float32(AZ)\n",
    "        AZ = torch.from_numpy(AZ).to(device).T\n",
    "        #function evals\n",
    "        with torch.no_grad():\n",
    "            fAZ = f(AZ)\n",
    "            fAZ = fAZ.cpu().numpy()\n",
    "    else:\n",
    "        #function evals\n",
    "        fAZ = f(AZ)\n",
    "    fAZ  = fAZ.reshape(numpnts,numpnts)\n",
    "    #plotting -- using a surface plot, a wireframe plot, or a contour plot\n",
    "    if kind==\"surface\":\n",
    "        ax.plot_surface(Z1,Z2,fAZ,**kwargs)\n",
    "    elif kind==\"wire\":\n",
    "        ax.plot_wireframe(Z1,Z2,fAZ,**kwargs)\n",
    "    elif kind==\"contour\":\n",
    "        ax.contour(Z1,Z2,fAZ,**kwargs)\n",
    "    elif kind==\"contourf\":\n",
    "        ax.contourf(Z1,Z2,fAZ,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a789a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for projseed in range(1,4):\n",
    "    fig = plt.figure(figsize = (20,10))\n",
    "    fig.suptitle(f\"Ground Truth vs. Models, $r = {r}$, Projection {projseed}\")\n",
    "\n",
    "    # kind = \"surface\"\n",
    "    # #plotting ground truth\n",
    "    # ax = fig.add_subplot(1,3,1,projection='3d')\n",
    "    # projviz(ax,f,istorch=False,kind=kind,projseed=projseed)\n",
    "    # ax.set_title(f\"Ground Truth\")\n",
    "    # #plotting models\n",
    "    # subplot=2\n",
    "    # for rownum,row in bestLres[(bestLres.r ==1)*(bestLres.sigma ==0)*(bestLres.n >= 1024)].iterrows():\n",
    "    #     ax = fig.add_subplot(1,3,subplot,projection='3d')\n",
    "    #     subplot += 1\n",
    "    #     projviz(ax,row.Model,kind=kind,projseed=projseed)\n",
    "    #     ax.set_title(rf\"$\\sigma={row.sigma}, n={row.n}$\")\n",
    "    # plt.show()\n",
    "\n",
    "    kind = \"contour\"\n",
    "    fig = plt.figure(figsize = (20,6))\n",
    "    fig.suptitle(f\"Ground Truth vs. Models, $r = {r}$, Projection {projseed}\")\n",
    "    #plotting ground truth\n",
    "    ax = fig.add_subplot(1,3,1)\n",
    "    projviz(ax,f,istorch=False,kind=kind,projseed=projseed,levels=100)\n",
    "    ax.set_title(f\"Ground Truth\")\n",
    "    #plotting models\n",
    "    subplot=2\n",
    "    for rownum,row in bestLres[(bestLres.r ==1)*(bestLres.sigma ==0)*(bestLres.n >= 1024)].iterrows():\n",
    "        ax = fig.add_subplot(1,3,subplot)\n",
    "        subplot += 1\n",
    "        projviz(ax,row.Model,kind=kind,projseed=projseed,levels=100)\n",
    "        ax.set_title(rf\"$\\sigma={row.sigma}, n={row.n}$\")\n",
    "    plt.savefig(f\"ooderrors_projvizr{r}projection{projseed}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc30dc2",
   "metadata": {},
   "source": [
    "## Training v Epoch after Tuning $(\\lambda, L)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.array([0,0.25,0.5,1])\n",
    "f, ax = plt.subplots(nrows=2, ncols=4, sharex=True, sharey=True, figsize=(20,10))\n",
    "plt.figure(figsize=(10,10))\n",
    "for rownum,row in bestLres.iterrows():\n",
    "    sigma = row['sigma']\n",
    "    r = row['r']\n",
    "    whichcol = np.where(sigma == sigmas)[0][0]\n",
    "    whichrow = 0 if r == 1 else 1\n",
    "    ax[whichrow,whichcol].semilogy(row[\"Train MSE\"],label=rf\"$n = {row['n']}$\",linewidth=1,alpha=0.7)\n",
    "    ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},\\sigma = {sigma}$\")\n",
    "    ax[whichrow,whichrow].set_xlabel(\"Epoch\")\n",
    "ax[0,0].legend()\n",
    "f.suptitle(rf\"Train MSE v Epoch (Tuned $\\lambda,L$)\")\n",
    "f.savefig(job_name+f\"trainmse.png\",dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.array([0,0.25,0.5,1])\n",
    "f, ax = plt.subplots(nrows=2, ncols=4, sharex=True, sharey=True, figsize=(20,10))\n",
    "plt.figure(figsize=(10,10))\n",
    "for rownum,row in bestLres.iterrows():\n",
    "    sigma = row['sigma']\n",
    "    r = row['r']\n",
    "    whichcol = np.where(sigma == sigmas)[0][0]\n",
    "    whichrow = 0 if r == 1 else 1\n",
    "    ax[whichrow,whichcol].semilogy(row[\"Weight Decay\"],label=rf\"$n = {row['n']}$\",linewidth=1,alpha=0.7)\n",
    "    ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},\\sigma = {sigma}$\")\n",
    "    ax[whichrow,whichrow].set_xlabel(\"Epoch\")\n",
    "ax[0,0].legend()\n",
    "f.suptitle(rf\"Weight Decay v Epoch (Tuned $\\lambda,L$)\")\n",
    "f.savefig(job_name+f\"weightdecay.png\",dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

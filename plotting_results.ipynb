{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b21cba",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#imports\" data-toc-modified-id=\"imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>imports</a></span></li><li><span><a href=\"#load-data\" data-toc-modified-id=\"load-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#read-in-the-files\" data-toc-modified-id=\"read-in-the-files-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>read in the files</a></span><ul class=\"toc-item\"><li><span><a href=\"#data-from-linear-and-relu-activations\" data-toc-modified-id=\"data-from-linear-and-relu-activations-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>data from linear and relu activations</a></span></li></ul></li><li><span><a href=\"#create-pandas-table\" data-toc-modified-id=\"create-pandas-table-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>create pandas table</a></span></li></ul></li><li><span><a href=\"#filter-out-bad-training-losses\" data-toc-modified-id=\"filter-out-bad-training-losses-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>filter out bad training losses</a></span></li><li><span><a href=\"#determine-the-lambda-parameter-that-gets-the-best-validation-MSE-for-each-(r,n,L)\" data-toc-modified-id=\"determine-the-lambda-parameter-that-gets-the-best-test-MSE-for-each-(r,n,L)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>determine the lambda parameter that gets the best test MSE for each (r,n,L)</a></span></li><li><span><a href=\"#Generalization-MSE\" data-toc-modified-id=\"Generalization-MSE-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Generalization MSE</a></span><ul class=\"toc-item\"><li><span><a href=\"#generate-data\" data-toc-modified-id=\"generate-data-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>generate data</a></span></li><li><span><a href=\"#compute-MSE\" data-toc-modified-id=\"compute-MSE-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>compute MSE</a></span></li></ul></li><li><span><a href=\"#Out-of-Distribution-MSE\" data-toc-modified-id=\"Out-of-Distribution-MSE-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Out of Distribution MSE</a></span><ul class=\"toc-item\"><li><span><a href=\"#generate-data\" data-toc-modified-id=\"generate-data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>generate data</a></span></li><li><span><a href=\"#compute-MSE\" data-toc-modified-id=\"compute-MSE-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>compute MSE</a></span></li></ul></li><li><span><a href=\"#Active-Subspace\" data-toc-modified-id=\"Active-Subspace-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Active Subspace</a></span><ul class=\"toc-item\"><li><span><a href=\"#evaluate-gradients-and-compute-singular-values-and-active-subspaces\" data-toc-modified-id=\"evaluate-gradients-and-compute-singular-values-and-active-subspaces-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>evaluate gradients and compute singular values and active subspaces</a></span></li><li><span><a href=\"#plot-of-singular-values\" data-toc-modified-id=\"plot-of-singular-values-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>plot of singular values</a></span></li></ul></li><li><span><a href=\"#determine-the-L-parameter-that-gets-the-best-test-MSE-for-each-(r,n)\" data-toc-modified-id=\"determine-the-L-parameter-that-gets-the-best-test-MSE-for-each-(r,n)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>determine the L parameter that gets the best test MSE for each (r,n)</a></span></li><li><span><a href=\"#Plots-of-L-vs-Test-error-and-n-vs-Generalization-metrics-with/without-linear-layers\" data-toc-modified-id=\"Plots-of-L-vs-Validation-error-and-n-vs-Generalization-metrics-with/without-linear-layers-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Plots of L vs Test error and n vs Generalization metrics with/without linear layers</a></span></li><li><span><a href=\"#Final-Table\" data-toc-modified-id=\"Final-Table-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Final Table</a></span></li><li><span><a href=\"#Training-Time-Plots\" data-toc-modified-id=\"Training-Time-Plots-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Training Time Plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-MSE-v-Epoch\" data-toc-modified-id=\"Train-MSE-v-Epoch-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Train MSE v Epoch</a></span></li><li><span><a href=\"#Weight-Decay-v-Epoch\" data-toc-modified-id=\"Weight-Decay-v-Epoch-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Weight Decay v Epoch</a></span></li><li><span><a href=\"#learning-rates\" data-toc-modified-id=\"learning-rates-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>learning rates</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bF8A36cglU4",
   "metadata": {
    "id": "4bF8A36cglU4"
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cca449a",
   "metadata": {
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1701128286405,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "2cca449a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.stats import linregress\n",
    "from scipy import linalg as la\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import sem\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21725609",
   "metadata": {
    "id": "21725609"
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aHdpcWWgi3RZ",
   "metadata": {
    "id": "aHdpcWWgi3RZ"
   },
   "source": [
    "## read in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "s2Q_uTMFlmlR",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "s2Q_uTMFlmlR"
   },
   "outputs": [],
   "source": [
    "rnvals = [(1,64),(1,128),(1,256),(1,512),(1,1024),(1,2048),\n",
    "          (2,64),(2,128),(2,256),(2,512),(2,1024),(2,2048)]\n",
    "Ls = [2,3,4,5,6,7,8,9]\n",
    "rs = [1,2]\n",
    "wds = [1e-3,1e-4,1e-5]\n",
    "labelnoise = [0,0.25,0.5,1]\n",
    "epochs = 60100\n",
    "job_name = \"new_targets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d588a3c2",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "d588a3c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testMSEs = {}\n",
    "trainMSEs = {}\n",
    "weightdecays = {}\n",
    "learningrates = {}\n",
    "files_found_list = []\n",
    "for r,n in rnvals:\n",
    "    for L in Ls:\n",
    "        for wd in wds:\n",
    "            for ln in labelnoise:\n",
    "                paramname = job_name+f\"_labelnoise{ln}\"+f\"/N{n}_L{L}_r{r}_wd{wd}_epochs{epochs}\"\n",
    "                if os.path.exists(paramname+\"testMSE.npy\"):\n",
    "                    testMSEs[r,n,L,wd,ln] = np.load(paramname+\"testMSE.npy\",allow_pickle=True).item()\n",
    "                    trainMSEs[r,n,L,wd,ln] = np.load(paramname+\"trainMSEs.npy\",allow_pickle=True)\n",
    "                    weightdecays[r,n,L,wd,ln] = np.load(paramname+\"weightdecays.npy\",allow_pickle=True)\n",
    "                    learningrates[r,n,L,wd,ln] = np.load(paramname+\"learningrates.npy\",allow_pickle=True)\n",
    "                    files_found_list.append((r,n,L,wd,ln))\n",
    "                else:\n",
    "                    print(f\"{paramname+'testMSE.npy'} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ca93e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 64, 2, 0.001, 0),\n",
       " (1, 64, 2, 0.001, 0.25),\n",
       " (1, 64, 2, 0.001, 0.5),\n",
       " (1, 64, 2, 0.001, 1),\n",
       " (1, 64, 2, 0.0001, 0),\n",
       " (1, 64, 2, 0.0001, 0.25),\n",
       " (1, 64, 2, 0.0001, 0.5),\n",
       " (1, 64, 2, 0.0001, 1),\n",
       " (1, 64, 2, 1e-05, 0),\n",
       " (1, 64, 2, 1e-05, 0.25),\n",
       " (1, 64, 2, 1e-05, 0.5),\n",
       " (1, 64, 2, 1e-05, 1),\n",
       " (1, 64, 3, 0.001, 0),\n",
       " (1, 64, 3, 0.001, 0.25),\n",
       " (1, 64, 3, 0.001, 0.5),\n",
       " (1, 64, 3, 0.001, 1),\n",
       " (1, 64, 3, 0.0001, 0),\n",
       " (1, 64, 3, 0.0001, 0.25),\n",
       " (1, 64, 3, 0.0001, 0.5),\n",
       " (1, 64, 3, 0.0001, 1),\n",
       " (1, 64, 3, 1e-05, 0),\n",
       " (1, 64, 3, 1e-05, 0.25),\n",
       " (1, 64, 3, 1e-05, 0.5),\n",
       " (1, 64, 3, 1e-05, 1),\n",
       " (1, 64, 4, 0.001, 0),\n",
       " (1, 64, 4, 0.001, 0.25),\n",
       " (1, 64, 4, 0.001, 0.5),\n",
       " (1, 64, 4, 0.001, 1),\n",
       " (1, 64, 4, 0.0001, 0),\n",
       " (1, 64, 4, 0.0001, 0.25),\n",
       " (1, 64, 4, 0.0001, 0.5),\n",
       " (1, 64, 4, 0.0001, 1),\n",
       " (1, 64, 4, 1e-05, 0),\n",
       " (1, 64, 4, 1e-05, 0.25),\n",
       " (1, 64, 4, 1e-05, 0.5),\n",
       " (1, 64, 4, 1e-05, 1),\n",
       " (1, 64, 5, 0.001, 0),\n",
       " (1, 64, 5, 0.001, 0.25),\n",
       " (1, 64, 5, 0.001, 0.5),\n",
       " (1, 64, 5, 0.001, 1),\n",
       " (1, 64, 5, 0.0001, 0),\n",
       " (1, 64, 5, 0.0001, 0.25),\n",
       " (1, 64, 5, 0.0001, 0.5),\n",
       " (1, 64, 5, 0.0001, 1),\n",
       " (1, 64, 5, 1e-05, 0),\n",
       " (1, 64, 5, 1e-05, 0.25),\n",
       " (1, 64, 5, 1e-05, 0.5),\n",
       " (1, 64, 5, 1e-05, 1),\n",
       " (1, 64, 6, 0.001, 0),\n",
       " (1, 64, 6, 0.001, 0.25),\n",
       " (1, 64, 6, 0.001, 0.5),\n",
       " (1, 64, 6, 0.001, 1),\n",
       " (1, 64, 6, 0.0001, 0),\n",
       " (1, 64, 6, 0.0001, 0.25),\n",
       " (1, 64, 6, 0.0001, 0.5),\n",
       " (1, 64, 6, 0.0001, 1),\n",
       " (1, 64, 6, 1e-05, 0),\n",
       " (1, 64, 6, 1e-05, 0.25),\n",
       " (1, 64, 6, 1e-05, 0.5),\n",
       " (1, 64, 6, 1e-05, 1),\n",
       " (1, 64, 7, 0.001, 0),\n",
       " (1, 64, 7, 0.001, 0.25),\n",
       " (1, 64, 7, 0.001, 0.5),\n",
       " (1, 64, 7, 0.001, 1),\n",
       " (1, 64, 7, 0.0001, 0),\n",
       " (1, 64, 7, 0.0001, 0.25),\n",
       " (1, 64, 7, 0.0001, 0.5),\n",
       " (1, 64, 7, 0.0001, 1),\n",
       " (1, 64, 7, 1e-05, 0),\n",
       " (1, 64, 7, 1e-05, 0.25),\n",
       " (1, 64, 7, 1e-05, 0.5),\n",
       " (1, 64, 7, 1e-05, 1),\n",
       " (1, 64, 8, 0.001, 0),\n",
       " (1, 64, 8, 0.001, 0.25),\n",
       " (1, 64, 8, 0.001, 0.5),\n",
       " (1, 64, 8, 0.001, 1),\n",
       " (1, 64, 8, 0.0001, 0),\n",
       " (1, 64, 8, 0.0001, 0.25),\n",
       " (1, 64, 8, 0.0001, 0.5),\n",
       " (1, 64, 8, 0.0001, 1),\n",
       " (1, 64, 8, 1e-05, 0),\n",
       " (1, 64, 8, 1e-05, 0.25),\n",
       " (1, 64, 8, 1e-05, 0.5),\n",
       " (1, 64, 8, 1e-05, 1),\n",
       " (1, 64, 9, 0.001, 0),\n",
       " (1, 64, 9, 0.001, 0.25),\n",
       " (1, 64, 9, 0.001, 0.5),\n",
       " (1, 64, 9, 0.001, 1),\n",
       " (1, 64, 9, 0.0001, 0),\n",
       " (1, 64, 9, 0.0001, 0.25),\n",
       " (1, 64, 9, 0.0001, 0.5),\n",
       " (1, 64, 9, 0.0001, 1),\n",
       " (1, 64, 9, 1e-05, 0),\n",
       " (1, 64, 9, 1e-05, 0.25),\n",
       " (1, 64, 9, 1e-05, 0.5),\n",
       " (1, 64, 9, 1e-05, 1),\n",
       " (1, 128, 2, 0.001, 0),\n",
       " (1, 128, 2, 0.001, 0.25),\n",
       " (1, 128, 2, 0.001, 0.5),\n",
       " (1, 128, 2, 0.001, 1),\n",
       " (1, 128, 2, 0.0001, 0),\n",
       " (1, 128, 2, 0.0001, 0.25),\n",
       " (1, 128, 2, 0.0001, 0.5),\n",
       " (1, 128, 2, 0.0001, 1),\n",
       " (1, 128, 2, 1e-05, 0),\n",
       " (1, 128, 2, 1e-05, 0.25),\n",
       " (1, 128, 2, 1e-05, 0.5),\n",
       " (1, 128, 2, 1e-05, 1),\n",
       " (1, 128, 3, 0.001, 0),\n",
       " (1, 128, 3, 0.001, 0.25),\n",
       " (1, 128, 3, 0.001, 0.5),\n",
       " (1, 128, 3, 0.001, 1),\n",
       " (1, 128, 3, 0.0001, 0),\n",
       " (1, 128, 3, 0.0001, 0.25),\n",
       " (1, 128, 3, 0.0001, 0.5),\n",
       " (1, 128, 3, 0.0001, 1),\n",
       " (1, 128, 3, 1e-05, 0),\n",
       " (1, 128, 3, 1e-05, 0.25),\n",
       " (1, 128, 3, 1e-05, 0.5),\n",
       " (1, 128, 3, 1e-05, 1),\n",
       " (1, 128, 4, 0.001, 0),\n",
       " (1, 128, 4, 0.001, 0.25),\n",
       " (1, 128, 4, 0.001, 0.5),\n",
       " (1, 128, 4, 0.001, 1),\n",
       " (1, 128, 4, 0.0001, 0),\n",
       " (1, 128, 4, 0.0001, 0.25),\n",
       " (1, 128, 4, 0.0001, 0.5),\n",
       " (1, 128, 4, 0.0001, 1),\n",
       " (1, 128, 4, 1e-05, 0),\n",
       " (1, 128, 4, 1e-05, 0.25),\n",
       " (1, 128, 4, 1e-05, 0.5),\n",
       " (1, 128, 4, 1e-05, 1),\n",
       " (1, 128, 5, 0.001, 0),\n",
       " (1, 128, 5, 0.001, 0.25),\n",
       " (1, 128, 5, 0.001, 0.5),\n",
       " (1, 128, 5, 0.001, 1),\n",
       " (1, 128, 5, 0.0001, 0),\n",
       " (1, 128, 5, 0.0001, 0.25),\n",
       " (1, 128, 5, 0.0001, 0.5),\n",
       " (1, 128, 5, 0.0001, 1),\n",
       " (1, 128, 5, 1e-05, 0),\n",
       " (1, 128, 5, 1e-05, 0.25),\n",
       " (1, 128, 5, 1e-05, 0.5),\n",
       " (1, 128, 5, 1e-05, 1),\n",
       " (1, 128, 6, 0.001, 0),\n",
       " (1, 128, 6, 0.001, 0.25),\n",
       " (1, 128, 6, 0.001, 0.5),\n",
       " (1, 128, 6, 0.001, 1),\n",
       " (1, 128, 6, 0.0001, 0),\n",
       " (1, 128, 6, 0.0001, 0.25),\n",
       " (1, 128, 6, 0.0001, 0.5),\n",
       " (1, 128, 6, 0.0001, 1),\n",
       " (1, 128, 6, 1e-05, 0),\n",
       " (1, 128, 6, 1e-05, 0.25),\n",
       " (1, 128, 6, 1e-05, 0.5),\n",
       " (1, 128, 6, 1e-05, 1),\n",
       " (1, 128, 7, 0.001, 0),\n",
       " (1, 128, 7, 0.001, 0.25),\n",
       " (1, 128, 7, 0.001, 0.5),\n",
       " (1, 128, 7, 0.001, 1),\n",
       " (1, 128, 7, 0.0001, 0),\n",
       " (1, 128, 7, 0.0001, 0.25),\n",
       " (1, 128, 7, 0.0001, 0.5),\n",
       " (1, 128, 7, 0.0001, 1),\n",
       " (1, 128, 7, 1e-05, 0),\n",
       " (1, 128, 7, 1e-05, 0.25),\n",
       " (1, 128, 7, 1e-05, 0.5),\n",
       " (1, 128, 7, 1e-05, 1),\n",
       " (1, 128, 8, 0.001, 0),\n",
       " (1, 128, 8, 0.001, 0.25),\n",
       " (1, 128, 8, 0.001, 0.5),\n",
       " (1, 128, 8, 0.001, 1),\n",
       " (1, 128, 8, 0.0001, 0),\n",
       " (1, 128, 8, 0.0001, 0.25),\n",
       " (1, 128, 8, 0.0001, 0.5),\n",
       " (1, 128, 8, 0.0001, 1),\n",
       " (1, 128, 8, 1e-05, 0),\n",
       " (1, 128, 8, 1e-05, 0.25),\n",
       " (1, 128, 8, 1e-05, 0.5),\n",
       " (1, 128, 8, 1e-05, 1),\n",
       " (1, 128, 9, 0.001, 0),\n",
       " (1, 128, 9, 0.001, 0.25),\n",
       " (1, 128, 9, 0.001, 0.5),\n",
       " (1, 128, 9, 0.001, 1),\n",
       " (1, 128, 9, 0.0001, 0),\n",
       " (1, 128, 9, 0.0001, 0.25),\n",
       " (1, 128, 9, 0.0001, 0.5),\n",
       " (1, 128, 9, 0.0001, 1),\n",
       " (1, 128, 9, 1e-05, 0),\n",
       " (1, 128, 9, 1e-05, 0.25),\n",
       " (1, 128, 9, 1e-05, 0.5),\n",
       " (1, 128, 9, 1e-05, 1),\n",
       " (1, 256, 2, 0.001, 0),\n",
       " (1, 256, 2, 0.001, 0.25),\n",
       " (1, 256, 2, 0.001, 0.5),\n",
       " (1, 256, 2, 0.001, 1),\n",
       " (1, 256, 2, 0.0001, 0),\n",
       " (1, 256, 2, 0.0001, 0.25),\n",
       " (1, 256, 2, 0.0001, 0.5),\n",
       " (1, 256, 2, 0.0001, 1),\n",
       " (1, 256, 2, 1e-05, 0),\n",
       " (1, 256, 2, 1e-05, 0.25),\n",
       " (1, 256, 2, 1e-05, 0.5),\n",
       " (1, 256, 2, 1e-05, 1),\n",
       " (1, 256, 3, 0.001, 0),\n",
       " (1, 256, 3, 0.001, 0.25),\n",
       " (1, 256, 3, 0.001, 0.5),\n",
       " (1, 256, 3, 0.001, 1),\n",
       " (1, 256, 3, 0.0001, 0),\n",
       " (1, 256, 3, 0.0001, 0.25),\n",
       " (1, 256, 3, 0.0001, 0.5),\n",
       " (1, 256, 3, 0.0001, 1),\n",
       " (1, 256, 3, 1e-05, 0),\n",
       " (1, 256, 3, 1e-05, 0.25),\n",
       " (1, 256, 3, 1e-05, 0.5),\n",
       " (1, 256, 3, 1e-05, 1),\n",
       " (1, 256, 4, 0.001, 0),\n",
       " (1, 256, 4, 0.001, 0.25),\n",
       " (1, 256, 4, 0.001, 0.5),\n",
       " (1, 256, 4, 0.001, 1),\n",
       " (1, 256, 4, 0.0001, 0),\n",
       " (1, 256, 4, 0.0001, 0.25),\n",
       " (1, 256, 4, 0.0001, 0.5),\n",
       " (1, 256, 4, 0.0001, 1),\n",
       " (1, 256, 4, 1e-05, 0),\n",
       " (1, 256, 4, 1e-05, 0.25),\n",
       " (1, 256, 4, 1e-05, 0.5),\n",
       " (1, 256, 4, 1e-05, 1),\n",
       " (1, 256, 5, 0.001, 0),\n",
       " (1, 256, 5, 0.001, 0.25),\n",
       " (1, 256, 5, 0.001, 0.5),\n",
       " (1, 256, 5, 0.001, 1),\n",
       " (1, 256, 5, 0.0001, 0),\n",
       " (1, 256, 5, 0.0001, 0.25),\n",
       " (1, 256, 5, 0.0001, 0.5),\n",
       " (1, 256, 5, 0.0001, 1),\n",
       " (1, 256, 5, 1e-05, 0),\n",
       " (1, 256, 5, 1e-05, 0.25),\n",
       " (1, 256, 5, 1e-05, 0.5),\n",
       " (1, 256, 5, 1e-05, 1),\n",
       " (1, 256, 6, 0.001, 0),\n",
       " (1, 256, 6, 0.001, 0.25),\n",
       " (1, 256, 6, 0.001, 0.5),\n",
       " (1, 256, 6, 0.001, 1),\n",
       " (1, 256, 6, 0.0001, 0),\n",
       " (1, 256, 6, 0.0001, 0.25),\n",
       " (1, 256, 6, 0.0001, 0.5),\n",
       " (1, 256, 6, 0.0001, 1),\n",
       " (1, 256, 6, 1e-05, 0),\n",
       " (1, 256, 6, 1e-05, 0.25),\n",
       " (1, 256, 6, 1e-05, 0.5),\n",
       " (1, 256, 6, 1e-05, 1),\n",
       " (1, 256, 7, 0.001, 0),\n",
       " (1, 256, 7, 0.001, 0.25),\n",
       " (1, 256, 7, 0.001, 0.5),\n",
       " (1, 256, 7, 0.001, 1),\n",
       " (1, 256, 7, 0.0001, 0),\n",
       " (1, 256, 7, 0.0001, 0.25),\n",
       " (1, 256, 7, 0.0001, 0.5),\n",
       " (1, 256, 7, 0.0001, 1),\n",
       " (1, 256, 7, 1e-05, 0),\n",
       " (1, 256, 7, 1e-05, 0.25),\n",
       " (1, 256, 7, 1e-05, 0.5),\n",
       " (1, 256, 7, 1e-05, 1),\n",
       " (1, 256, 8, 0.001, 0),\n",
       " (1, 256, 8, 0.001, 0.25),\n",
       " (1, 256, 8, 0.001, 0.5),\n",
       " (1, 256, 8, 0.001, 1),\n",
       " (1, 256, 8, 0.0001, 0),\n",
       " (1, 256, 8, 0.0001, 0.25),\n",
       " (1, 256, 8, 0.0001, 0.5),\n",
       " (1, 256, 8, 0.0001, 1),\n",
       " (1, 256, 8, 1e-05, 0),\n",
       " (1, 256, 8, 1e-05, 0.25),\n",
       " (1, 256, 8, 1e-05, 0.5),\n",
       " (1, 256, 8, 1e-05, 1),\n",
       " (1, 256, 9, 0.001, 0),\n",
       " (1, 256, 9, 0.001, 0.25),\n",
       " (1, 256, 9, 0.001, 0.5),\n",
       " (1, 256, 9, 0.001, 1),\n",
       " (1, 256, 9, 0.0001, 0),\n",
       " (1, 256, 9, 0.0001, 0.25),\n",
       " (1, 256, 9, 0.0001, 0.5),\n",
       " (1, 256, 9, 0.0001, 1),\n",
       " (1, 256, 9, 1e-05, 0),\n",
       " (1, 256, 9, 1e-05, 0.25),\n",
       " (1, 256, 9, 1e-05, 0.5),\n",
       " (1, 256, 9, 1e-05, 1),\n",
       " (1, 512, 2, 0.001, 0),\n",
       " (1, 512, 2, 0.001, 0.25),\n",
       " (1, 512, 2, 0.001, 0.5),\n",
       " (1, 512, 2, 0.001, 1),\n",
       " (1, 512, 2, 0.0001, 0),\n",
       " (1, 512, 2, 0.0001, 0.25),\n",
       " (1, 512, 2, 0.0001, 0.5),\n",
       " (1, 512, 2, 0.0001, 1),\n",
       " (1, 512, 2, 1e-05, 0),\n",
       " (1, 512, 2, 1e-05, 0.25),\n",
       " (1, 512, 2, 1e-05, 0.5),\n",
       " (1, 512, 2, 1e-05, 1),\n",
       " (1, 512, 3, 0.001, 0),\n",
       " (1, 512, 3, 0.001, 0.25),\n",
       " (1, 512, 3, 0.001, 0.5),\n",
       " (1, 512, 3, 0.001, 1),\n",
       " (1, 512, 3, 0.0001, 0),\n",
       " (1, 512, 3, 0.0001, 0.25),\n",
       " (1, 512, 3, 0.0001, 0.5),\n",
       " (1, 512, 3, 0.0001, 1),\n",
       " (1, 512, 3, 1e-05, 0),\n",
       " (1, 512, 3, 1e-05, 0.25),\n",
       " (1, 512, 3, 1e-05, 0.5),\n",
       " (1, 512, 3, 1e-05, 1),\n",
       " (1, 512, 4, 0.001, 0),\n",
       " (1, 512, 4, 0.001, 0.25),\n",
       " (1, 512, 4, 0.001, 0.5),\n",
       " (1, 512, 4, 0.001, 1),\n",
       " (1, 512, 4, 0.0001, 0),\n",
       " (1, 512, 4, 0.0001, 0.25),\n",
       " (1, 512, 4, 0.0001, 0.5),\n",
       " (1, 512, 4, 0.0001, 1),\n",
       " (1, 512, 4, 1e-05, 0),\n",
       " (1, 512, 4, 1e-05, 0.25),\n",
       " (1, 512, 4, 1e-05, 0.5),\n",
       " (1, 512, 4, 1e-05, 1),\n",
       " (1, 512, 5, 0.001, 0),\n",
       " (1, 512, 5, 0.001, 0.25),\n",
       " (1, 512, 5, 0.001, 0.5),\n",
       " (1, 512, 5, 0.001, 1),\n",
       " (1, 512, 5, 0.0001, 0),\n",
       " (1, 512, 5, 0.0001, 0.25),\n",
       " (1, 512, 5, 0.0001, 0.5),\n",
       " (1, 512, 5, 0.0001, 1),\n",
       " (1, 512, 5, 1e-05, 0),\n",
       " (1, 512, 5, 1e-05, 0.25),\n",
       " (1, 512, 5, 1e-05, 0.5),\n",
       " (1, 512, 5, 1e-05, 1),\n",
       " (1, 512, 6, 0.001, 0),\n",
       " (1, 512, 6, 0.001, 0.25),\n",
       " (1, 512, 6, 0.001, 0.5),\n",
       " (1, 512, 6, 0.001, 1),\n",
       " (1, 512, 6, 0.0001, 0),\n",
       " (1, 512, 6, 0.0001, 0.25),\n",
       " (1, 512, 6, 0.0001, 0.5),\n",
       " (1, 512, 6, 0.0001, 1),\n",
       " (1, 512, 6, 1e-05, 0),\n",
       " (1, 512, 6, 1e-05, 0.25),\n",
       " (1, 512, 6, 1e-05, 0.5),\n",
       " (1, 512, 6, 1e-05, 1),\n",
       " (1, 512, 7, 0.001, 0),\n",
       " (1, 512, 7, 0.001, 0.25),\n",
       " (1, 512, 7, 0.001, 0.5),\n",
       " (1, 512, 7, 0.001, 1),\n",
       " (1, 512, 7, 0.0001, 0),\n",
       " (1, 512, 7, 0.0001, 0.25),\n",
       " (1, 512, 7, 0.0001, 0.5),\n",
       " (1, 512, 7, 0.0001, 1),\n",
       " (1, 512, 7, 1e-05, 0),\n",
       " (1, 512, 7, 1e-05, 0.25),\n",
       " (1, 512, 7, 1e-05, 0.5),\n",
       " (1, 512, 7, 1e-05, 1),\n",
       " (1, 512, 8, 0.001, 0),\n",
       " (1, 512, 8, 0.001, 0.25),\n",
       " (1, 512, 8, 0.001, 0.5),\n",
       " (1, 512, 8, 0.001, 1),\n",
       " (1, 512, 8, 0.0001, 0),\n",
       " (1, 512, 8, 0.0001, 0.25),\n",
       " (1, 512, 8, 0.0001, 0.5),\n",
       " (1, 512, 8, 0.0001, 1),\n",
       " (1, 512, 8, 1e-05, 0),\n",
       " (1, 512, 8, 1e-05, 0.25),\n",
       " (1, 512, 8, 1e-05, 0.5),\n",
       " (1, 512, 8, 1e-05, 1),\n",
       " (1, 512, 9, 0.001, 0),\n",
       " (1, 512, 9, 0.001, 0.25),\n",
       " (1, 512, 9, 0.001, 0.5),\n",
       " (1, 512, 9, 0.001, 1),\n",
       " (1, 512, 9, 0.0001, 0),\n",
       " (1, 512, 9, 0.0001, 0.25),\n",
       " (1, 512, 9, 0.0001, 0.5),\n",
       " (1, 512, 9, 0.0001, 1),\n",
       " (1, 512, 9, 1e-05, 0),\n",
       " (1, 512, 9, 1e-05, 0.25),\n",
       " (1, 512, 9, 1e-05, 0.5),\n",
       " (1, 512, 9, 1e-05, 1),\n",
       " (1, 1024, 2, 0.001, 0),\n",
       " (1, 1024, 2, 0.001, 0.25),\n",
       " (1, 1024, 2, 0.001, 0.5),\n",
       " (1, 1024, 2, 0.001, 1),\n",
       " (1, 1024, 2, 0.0001, 0),\n",
       " (1, 1024, 2, 0.0001, 0.25),\n",
       " (1, 1024, 2, 0.0001, 0.5),\n",
       " (1, 1024, 2, 0.0001, 1),\n",
       " (1, 1024, 2, 1e-05, 0),\n",
       " (1, 1024, 2, 1e-05, 0.25),\n",
       " (1, 1024, 2, 1e-05, 0.5),\n",
       " (1, 1024, 2, 1e-05, 1),\n",
       " (1, 1024, 3, 0.001, 0),\n",
       " (1, 1024, 3, 0.001, 0.25),\n",
       " (1, 1024, 3, 0.001, 0.5),\n",
       " (1, 1024, 3, 0.001, 1),\n",
       " (1, 1024, 3, 0.0001, 0),\n",
       " (1, 1024, 3, 0.0001, 0.25),\n",
       " (1, 1024, 3, 0.0001, 0.5),\n",
       " (1, 1024, 3, 0.0001, 1),\n",
       " (1, 1024, 3, 1e-05, 0),\n",
       " (1, 1024, 3, 1e-05, 0.25),\n",
       " (1, 1024, 3, 1e-05, 0.5),\n",
       " (1, 1024, 3, 1e-05, 1),\n",
       " (1, 1024, 4, 0.001, 0),\n",
       " (1, 1024, 4, 0.001, 0.25),\n",
       " (1, 1024, 4, 0.001, 0.5),\n",
       " (1, 1024, 4, 0.001, 1),\n",
       " (1, 1024, 4, 0.0001, 0),\n",
       " (1, 1024, 4, 0.0001, 0.25),\n",
       " (1, 1024, 4, 0.0001, 0.5),\n",
       " (1, 1024, 4, 0.0001, 1),\n",
       " (1, 1024, 4, 1e-05, 0),\n",
       " (1, 1024, 4, 1e-05, 0.25),\n",
       " (1, 1024, 4, 1e-05, 0.5),\n",
       " (1, 1024, 4, 1e-05, 1),\n",
       " (1, 1024, 5, 0.001, 0),\n",
       " (1, 1024, 5, 0.001, 0.25),\n",
       " (1, 1024, 5, 0.001, 0.5),\n",
       " (1, 1024, 5, 0.001, 1),\n",
       " (1, 1024, 5, 0.0001, 0),\n",
       " (1, 1024, 5, 0.0001, 0.25),\n",
       " (1, 1024, 5, 0.0001, 0.5),\n",
       " (1, 1024, 5, 0.0001, 1),\n",
       " (1, 1024, 5, 1e-05, 0),\n",
       " (1, 1024, 5, 1e-05, 0.25),\n",
       " (1, 1024, 5, 1e-05, 0.5),\n",
       " (1, 1024, 5, 1e-05, 1),\n",
       " (1, 1024, 6, 0.001, 0),\n",
       " (1, 1024, 6, 0.001, 0.25),\n",
       " (1, 1024, 6, 0.001, 0.5),\n",
       " (1, 1024, 6, 0.001, 1),\n",
       " (1, 1024, 6, 0.0001, 0),\n",
       " (1, 1024, 6, 0.0001, 0.25),\n",
       " (1, 1024, 6, 0.0001, 0.5),\n",
       " (1, 1024, 6, 0.0001, 1),\n",
       " (1, 1024, 6, 1e-05, 0),\n",
       " (1, 1024, 6, 1e-05, 0.25),\n",
       " (1, 1024, 6, 1e-05, 0.5),\n",
       " (1, 1024, 6, 1e-05, 1),\n",
       " (1, 1024, 7, 0.001, 0),\n",
       " (1, 1024, 7, 0.001, 0.25),\n",
       " (1, 1024, 7, 0.001, 0.5),\n",
       " (1, 1024, 7, 0.001, 1),\n",
       " (1, 1024, 7, 0.0001, 0),\n",
       " (1, 1024, 7, 0.0001, 0.25),\n",
       " (1, 1024, 7, 0.0001, 0.5),\n",
       " (1, 1024, 7, 0.0001, 1),\n",
       " (1, 1024, 7, 1e-05, 0),\n",
       " (1, 1024, 7, 1e-05, 0.25),\n",
       " (1, 1024, 7, 1e-05, 0.5),\n",
       " (1, 1024, 7, 1e-05, 1),\n",
       " (1, 1024, 8, 0.001, 0),\n",
       " (1, 1024, 8, 0.001, 0.25),\n",
       " (1, 1024, 8, 0.001, 0.5),\n",
       " (1, 1024, 8, 0.001, 1),\n",
       " (1, 1024, 8, 0.0001, 0),\n",
       " (1, 1024, 8, 0.0001, 0.25),\n",
       " (1, 1024, 8, 0.0001, 0.5),\n",
       " (1, 1024, 8, 0.0001, 1),\n",
       " (1, 1024, 8, 1e-05, 0),\n",
       " (1, 1024, 8, 1e-05, 0.25),\n",
       " (1, 1024, 8, 1e-05, 0.5),\n",
       " (1, 1024, 8, 1e-05, 1),\n",
       " (1, 1024, 9, 0.001, 0),\n",
       " (1, 1024, 9, 0.001, 0.25),\n",
       " (1, 1024, 9, 0.001, 0.5),\n",
       " (1, 1024, 9, 0.001, 1),\n",
       " (1, 1024, 9, 0.0001, 0),\n",
       " (1, 1024, 9, 0.0001, 0.25),\n",
       " (1, 1024, 9, 0.0001, 0.5),\n",
       " (1, 1024, 9, 0.0001, 1),\n",
       " (1, 1024, 9, 1e-05, 0),\n",
       " (1, 1024, 9, 1e-05, 0.25),\n",
       " (1, 1024, 9, 1e-05, 0.5),\n",
       " (1, 1024, 9, 1e-05, 1),\n",
       " (1, 2048, 2, 0.001, 0),\n",
       " (1, 2048, 2, 0.001, 0.25),\n",
       " (1, 2048, 2, 0.001, 0.5),\n",
       " (1, 2048, 2, 0.001, 1),\n",
       " (1, 2048, 2, 0.0001, 0),\n",
       " (1, 2048, 2, 0.0001, 0.25),\n",
       " (1, 2048, 2, 0.0001, 0.5),\n",
       " (1, 2048, 2, 0.0001, 1),\n",
       " (1, 2048, 2, 1e-05, 0),\n",
       " (1, 2048, 2, 1e-05, 0.25),\n",
       " (1, 2048, 2, 1e-05, 0.5),\n",
       " (1, 2048, 2, 1e-05, 1),\n",
       " (1, 2048, 3, 0.001, 0),\n",
       " (1, 2048, 3, 0.001, 0.25),\n",
       " (1, 2048, 3, 0.001, 0.5),\n",
       " (1, 2048, 3, 0.001, 1),\n",
       " (1, 2048, 3, 0.0001, 0),\n",
       " (1, 2048, 3, 0.0001, 0.25),\n",
       " (1, 2048, 3, 0.0001, 0.5),\n",
       " (1, 2048, 3, 0.0001, 1),\n",
       " (1, 2048, 3, 1e-05, 0),\n",
       " (1, 2048, 3, 1e-05, 0.25),\n",
       " (1, 2048, 3, 1e-05, 0.5),\n",
       " (1, 2048, 3, 1e-05, 1),\n",
       " (1, 2048, 4, 0.001, 0),\n",
       " (1, 2048, 4, 0.001, 0.25),\n",
       " (1, 2048, 4, 0.001, 0.5),\n",
       " (1, 2048, 4, 0.001, 1),\n",
       " (1, 2048, 4, 0.0001, 0),\n",
       " (1, 2048, 4, 0.0001, 0.25),\n",
       " (1, 2048, 4, 0.0001, 0.5),\n",
       " (1, 2048, 4, 0.0001, 1),\n",
       " (1, 2048, 4, 1e-05, 0),\n",
       " (1, 2048, 4, 1e-05, 0.25),\n",
       " (1, 2048, 4, 1e-05, 0.5),\n",
       " (1, 2048, 4, 1e-05, 1),\n",
       " (1, 2048, 5, 0.001, 0),\n",
       " (1, 2048, 5, 0.001, 0.25),\n",
       " (1, 2048, 5, 0.001, 0.5),\n",
       " (1, 2048, 5, 0.001, 1),\n",
       " (1, 2048, 5, 0.0001, 0),\n",
       " (1, 2048, 5, 0.0001, 0.25),\n",
       " (1, 2048, 5, 0.0001, 0.5),\n",
       " (1, 2048, 5, 0.0001, 1),\n",
       " (1, 2048, 5, 1e-05, 0),\n",
       " (1, 2048, 5, 1e-05, 0.25),\n",
       " (1, 2048, 5, 1e-05, 0.5),\n",
       " (1, 2048, 5, 1e-05, 1),\n",
       " (1, 2048, 6, 0.001, 0),\n",
       " (1, 2048, 6, 0.001, 0.25),\n",
       " (1, 2048, 6, 0.001, 0.5),\n",
       " (1, 2048, 6, 0.001, 1),\n",
       " (1, 2048, 6, 0.0001, 0),\n",
       " (1, 2048, 6, 0.0001, 0.25),\n",
       " (1, 2048, 6, 0.0001, 0.5),\n",
       " (1, 2048, 6, 0.0001, 1),\n",
       " (1, 2048, 6, 1e-05, 0),\n",
       " (1, 2048, 6, 1e-05, 0.25),\n",
       " (1, 2048, 6, 1e-05, 0.5),\n",
       " (1, 2048, 6, 1e-05, 1),\n",
       " (1, 2048, 7, 0.001, 0),\n",
       " (1, 2048, 7, 0.001, 0.25),\n",
       " (1, 2048, 7, 0.001, 0.5),\n",
       " (1, 2048, 7, 0.001, 1),\n",
       " (1, 2048, 7, 0.0001, 0),\n",
       " (1, 2048, 7, 0.0001, 0.25),\n",
       " (1, 2048, 7, 0.0001, 0.5),\n",
       " (1, 2048, 7, 0.0001, 1),\n",
       " (1, 2048, 7, 1e-05, 0),\n",
       " (1, 2048, 7, 1e-05, 0.25),\n",
       " (1, 2048, 7, 1e-05, 0.5),\n",
       " (1, 2048, 7, 1e-05, 1),\n",
       " (1, 2048, 8, 0.001, 0),\n",
       " (1, 2048, 8, 0.001, 0.25),\n",
       " (1, 2048, 8, 0.001, 0.5),\n",
       " (1, 2048, 8, 0.001, 1),\n",
       " (1, 2048, 8, 0.0001, 0),\n",
       " (1, 2048, 8, 0.0001, 0.25),\n",
       " (1, 2048, 8, 0.0001, 0.5),\n",
       " (1, 2048, 8, 0.0001, 1),\n",
       " (1, 2048, 8, 1e-05, 0),\n",
       " (1, 2048, 8, 1e-05, 0.25),\n",
       " (1, 2048, 8, 1e-05, 0.5),\n",
       " (1, 2048, 8, 1e-05, 1),\n",
       " (1, 2048, 9, 0.001, 0),\n",
       " (1, 2048, 9, 0.001, 0.25),\n",
       " (1, 2048, 9, 0.001, 0.5),\n",
       " (1, 2048, 9, 0.001, 1),\n",
       " (1, 2048, 9, 0.0001, 0),\n",
       " (1, 2048, 9, 0.0001, 0.25),\n",
       " (1, 2048, 9, 0.0001, 0.5),\n",
       " (1, 2048, 9, 0.0001, 1),\n",
       " (1, 2048, 9, 1e-05, 0),\n",
       " (1, 2048, 9, 1e-05, 0.25),\n",
       " (1, 2048, 9, 1e-05, 0.5),\n",
       " (1, 2048, 9, 1e-05, 1),\n",
       " (2, 64, 2, 0.001, 0),\n",
       " (2, 64, 2, 0.001, 0.25),\n",
       " (2, 64, 2, 0.001, 0.5),\n",
       " (2, 64, 2, 0.001, 1),\n",
       " (2, 64, 2, 0.0001, 0),\n",
       " (2, 64, 2, 0.0001, 0.25),\n",
       " (2, 64, 2, 0.0001, 0.5),\n",
       " (2, 64, 2, 0.0001, 1),\n",
       " (2, 64, 2, 1e-05, 0),\n",
       " (2, 64, 2, 1e-05, 0.25),\n",
       " (2, 64, 2, 1e-05, 0.5),\n",
       " (2, 64, 2, 1e-05, 1),\n",
       " (2, 64, 3, 0.001, 0),\n",
       " (2, 64, 3, 0.001, 0.25),\n",
       " (2, 64, 3, 0.001, 0.5),\n",
       " (2, 64, 3, 0.001, 1),\n",
       " (2, 64, 3, 0.0001, 0),\n",
       " (2, 64, 3, 0.0001, 0.25),\n",
       " (2, 64, 3, 0.0001, 0.5),\n",
       " (2, 64, 3, 0.0001, 1),\n",
       " (2, 64, 3, 1e-05, 0),\n",
       " (2, 64, 3, 1e-05, 0.25),\n",
       " (2, 64, 3, 1e-05, 0.5),\n",
       " (2, 64, 3, 1e-05, 1),\n",
       " (2, 64, 4, 0.001, 0),\n",
       " (2, 64, 4, 0.001, 0.25),\n",
       " (2, 64, 4, 0.001, 0.5),\n",
       " (2, 64, 4, 0.001, 1),\n",
       " (2, 64, 4, 0.0001, 0),\n",
       " (2, 64, 4, 0.0001, 0.25),\n",
       " (2, 64, 4, 0.0001, 0.5),\n",
       " (2, 64, 4, 0.0001, 1),\n",
       " (2, 64, 4, 1e-05, 0),\n",
       " (2, 64, 4, 1e-05, 0.25),\n",
       " (2, 64, 4, 1e-05, 0.5),\n",
       " (2, 64, 4, 1e-05, 1),\n",
       " (2, 64, 5, 0.001, 0),\n",
       " (2, 64, 5, 0.001, 0.25),\n",
       " (2, 64, 5, 0.001, 0.5),\n",
       " (2, 64, 5, 0.001, 1),\n",
       " (2, 64, 5, 0.0001, 0),\n",
       " (2, 64, 5, 0.0001, 0.25),\n",
       " (2, 64, 5, 0.0001, 0.5),\n",
       " (2, 64, 5, 0.0001, 1),\n",
       " (2, 64, 5, 1e-05, 0),\n",
       " (2, 64, 5, 1e-05, 0.25),\n",
       " (2, 64, 5, 1e-05, 0.5),\n",
       " (2, 64, 5, 1e-05, 1),\n",
       " (2, 64, 6, 0.001, 0),\n",
       " (2, 64, 6, 0.001, 0.25),\n",
       " (2, 64, 6, 0.001, 0.5),\n",
       " (2, 64, 6, 0.001, 1),\n",
       " (2, 64, 6, 0.0001, 0),\n",
       " (2, 64, 6, 0.0001, 0.25),\n",
       " (2, 64, 6, 0.0001, 0.5),\n",
       " (2, 64, 6, 0.0001, 1),\n",
       " (2, 64, 6, 1e-05, 0),\n",
       " (2, 64, 6, 1e-05, 0.25),\n",
       " (2, 64, 6, 1e-05, 0.5),\n",
       " (2, 64, 6, 1e-05, 1),\n",
       " (2, 64, 7, 0.001, 0),\n",
       " (2, 64, 7, 0.001, 0.25),\n",
       " (2, 64, 7, 0.001, 0.5),\n",
       " (2, 64, 7, 0.001, 1),\n",
       " (2, 64, 7, 0.0001, 0),\n",
       " (2, 64, 7, 0.0001, 0.25),\n",
       " (2, 64, 7, 0.0001, 0.5),\n",
       " (2, 64, 7, 0.0001, 1),\n",
       " (2, 64, 7, 1e-05, 0),\n",
       " (2, 64, 7, 1e-05, 0.25),\n",
       " (2, 64, 7, 1e-05, 0.5),\n",
       " (2, 64, 7, 1e-05, 1),\n",
       " (2, 64, 8, 0.001, 0),\n",
       " (2, 64, 8, 0.001, 0.25),\n",
       " (2, 64, 8, 0.001, 0.5),\n",
       " (2, 64, 8, 0.001, 1),\n",
       " (2, 64, 8, 0.0001, 0),\n",
       " (2, 64, 8, 0.0001, 0.25),\n",
       " (2, 64, 8, 0.0001, 0.5),\n",
       " (2, 64, 8, 0.0001, 1),\n",
       " (2, 64, 8, 1e-05, 0),\n",
       " (2, 64, 8, 1e-05, 0.25),\n",
       " (2, 64, 8, 1e-05, 0.5),\n",
       " (2, 64, 8, 1e-05, 1),\n",
       " (2, 64, 9, 0.001, 0),\n",
       " (2, 64, 9, 0.001, 0.25),\n",
       " (2, 64, 9, 0.001, 0.5),\n",
       " (2, 64, 9, 0.001, 1),\n",
       " (2, 64, 9, 0.0001, 0),\n",
       " (2, 64, 9, 0.0001, 0.25),\n",
       " (2, 64, 9, 0.0001, 0.5),\n",
       " (2, 64, 9, 0.0001, 1),\n",
       " (2, 64, 9, 1e-05, 0),\n",
       " (2, 64, 9, 1e-05, 0.25),\n",
       " (2, 64, 9, 1e-05, 0.5),\n",
       " (2, 64, 9, 1e-05, 1),\n",
       " (2, 128, 2, 0.001, 0),\n",
       " (2, 128, 2, 0.001, 0.25),\n",
       " (2, 128, 2, 0.001, 0.5),\n",
       " (2, 128, 2, 0.001, 1),\n",
       " (2, 128, 2, 0.0001, 0),\n",
       " (2, 128, 2, 0.0001, 0.25),\n",
       " (2, 128, 2, 0.0001, 0.5),\n",
       " (2, 128, 2, 0.0001, 1),\n",
       " (2, 128, 2, 1e-05, 0),\n",
       " (2, 128, 2, 1e-05, 0.25),\n",
       " (2, 128, 2, 1e-05, 0.5),\n",
       " (2, 128, 2, 1e-05, 1),\n",
       " (2, 128, 3, 0.001, 0),\n",
       " (2, 128, 3, 0.001, 0.25),\n",
       " (2, 128, 3, 0.001, 0.5),\n",
       " (2, 128, 3, 0.001, 1),\n",
       " (2, 128, 3, 0.0001, 0),\n",
       " (2, 128, 3, 0.0001, 0.25),\n",
       " (2, 128, 3, 0.0001, 0.5),\n",
       " (2, 128, 3, 0.0001, 1),\n",
       " (2, 128, 3, 1e-05, 0),\n",
       " (2, 128, 3, 1e-05, 0.25),\n",
       " (2, 128, 3, 1e-05, 0.5),\n",
       " (2, 128, 3, 1e-05, 1),\n",
       " (2, 128, 4, 0.001, 0),\n",
       " (2, 128, 4, 0.001, 0.25),\n",
       " (2, 128, 4, 0.001, 0.5),\n",
       " (2, 128, 4, 0.001, 1),\n",
       " (2, 128, 4, 0.0001, 0),\n",
       " (2, 128, 4, 0.0001, 0.25),\n",
       " (2, 128, 4, 0.0001, 0.5),\n",
       " (2, 128, 4, 0.0001, 1),\n",
       " (2, 128, 4, 1e-05, 0),\n",
       " (2, 128, 4, 1e-05, 0.25),\n",
       " (2, 128, 4, 1e-05, 0.5),\n",
       " (2, 128, 4, 1e-05, 1),\n",
       " (2, 128, 5, 0.001, 0),\n",
       " (2, 128, 5, 0.001, 0.25),\n",
       " (2, 128, 5, 0.001, 0.5),\n",
       " (2, 128, 5, 0.001, 1),\n",
       " (2, 128, 5, 0.0001, 0),\n",
       " (2, 128, 5, 0.0001, 0.25),\n",
       " (2, 128, 5, 0.0001, 0.5),\n",
       " (2, 128, 5, 0.0001, 1),\n",
       " (2, 128, 5, 1e-05, 0),\n",
       " (2, 128, 5, 1e-05, 0.25),\n",
       " (2, 128, 5, 1e-05, 0.5),\n",
       " (2, 128, 5, 1e-05, 1),\n",
       " (2, 128, 6, 0.001, 0),\n",
       " (2, 128, 6, 0.001, 0.25),\n",
       " (2, 128, 6, 0.001, 0.5),\n",
       " (2, 128, 6, 0.001, 1),\n",
       " (2, 128, 6, 0.0001, 0),\n",
       " (2, 128, 6, 0.0001, 0.25),\n",
       " (2, 128, 6, 0.0001, 0.5),\n",
       " (2, 128, 6, 0.0001, 1),\n",
       " (2, 128, 6, 1e-05, 0),\n",
       " (2, 128, 6, 1e-05, 0.25),\n",
       " (2, 128, 6, 1e-05, 0.5),\n",
       " (2, 128, 6, 1e-05, 1),\n",
       " (2, 128, 7, 0.001, 0),\n",
       " (2, 128, 7, 0.001, 0.25),\n",
       " (2, 128, 7, 0.001, 0.5),\n",
       " (2, 128, 7, 0.001, 1),\n",
       " (2, 128, 7, 0.0001, 0),\n",
       " (2, 128, 7, 0.0001, 0.25),\n",
       " (2, 128, 7, 0.0001, 0.5),\n",
       " (2, 128, 7, 0.0001, 1),\n",
       " (2, 128, 7, 1e-05, 0),\n",
       " (2, 128, 7, 1e-05, 0.25),\n",
       " (2, 128, 7, 1e-05, 0.5),\n",
       " (2, 128, 7, 1e-05, 1),\n",
       " (2, 128, 8, 0.001, 0),\n",
       " (2, 128, 8, 0.001, 0.25),\n",
       " (2, 128, 8, 0.001, 0.5),\n",
       " (2, 128, 8, 0.001, 1),\n",
       " (2, 128, 8, 0.0001, 0),\n",
       " (2, 128, 8, 0.0001, 0.25),\n",
       " (2, 128, 8, 0.0001, 0.5),\n",
       " (2, 128, 8, 0.0001, 1),\n",
       " (2, 128, 8, 1e-05, 0),\n",
       " (2, 128, 8, 1e-05, 0.25),\n",
       " (2, 128, 8, 1e-05, 0.5),\n",
       " (2, 128, 8, 1e-05, 1),\n",
       " (2, 128, 9, 0.001, 0),\n",
       " (2, 128, 9, 0.001, 0.25),\n",
       " (2, 128, 9, 0.001, 0.5),\n",
       " (2, 128, 9, 0.001, 1),\n",
       " (2, 128, 9, 0.0001, 0),\n",
       " (2, 128, 9, 0.0001, 0.25),\n",
       " (2, 128, 9, 0.0001, 0.5),\n",
       " (2, 128, 9, 0.0001, 1),\n",
       " (2, 128, 9, 1e-05, 0),\n",
       " (2, 128, 9, 1e-05, 0.25),\n",
       " (2, 128, 9, 1e-05, 0.5),\n",
       " (2, 128, 9, 1e-05, 1),\n",
       " (2, 256, 2, 0.001, 0),\n",
       " (2, 256, 2, 0.001, 0.25),\n",
       " (2, 256, 2, 0.001, 0.5),\n",
       " (2, 256, 2, 0.001, 1),\n",
       " (2, 256, 2, 0.0001, 0),\n",
       " (2, 256, 2, 0.0001, 0.25),\n",
       " (2, 256, 2, 0.0001, 0.5),\n",
       " (2, 256, 2, 0.0001, 1),\n",
       " (2, 256, 2, 1e-05, 0),\n",
       " (2, 256, 2, 1e-05, 0.25),\n",
       " (2, 256, 2, 1e-05, 0.5),\n",
       " (2, 256, 2, 1e-05, 1),\n",
       " (2, 256, 3, 0.001, 0),\n",
       " (2, 256, 3, 0.001, 0.25),\n",
       " (2, 256, 3, 0.001, 0.5),\n",
       " (2, 256, 3, 0.001, 1),\n",
       " (2, 256, 3, 0.0001, 0),\n",
       " (2, 256, 3, 0.0001, 0.25),\n",
       " (2, 256, 3, 0.0001, 0.5),\n",
       " (2, 256, 3, 0.0001, 1),\n",
       " (2, 256, 3, 1e-05, 0),\n",
       " (2, 256, 3, 1e-05, 0.25),\n",
       " (2, 256, 3, 1e-05, 0.5),\n",
       " (2, 256, 3, 1e-05, 1),\n",
       " (2, 256, 4, 0.001, 0),\n",
       " (2, 256, 4, 0.001, 0.25),\n",
       " (2, 256, 4, 0.001, 0.5),\n",
       " (2, 256, 4, 0.001, 1),\n",
       " (2, 256, 4, 0.0001, 0),\n",
       " (2, 256, 4, 0.0001, 0.25),\n",
       " (2, 256, 4, 0.0001, 0.5),\n",
       " (2, 256, 4, 0.0001, 1),\n",
       " (2, 256, 4, 1e-05, 0),\n",
       " (2, 256, 4, 1e-05, 0.25),\n",
       " (2, 256, 4, 1e-05, 0.5),\n",
       " (2, 256, 4, 1e-05, 1),\n",
       " (2, 256, 5, 0.001, 0),\n",
       " (2, 256, 5, 0.001, 0.25),\n",
       " (2, 256, 5, 0.001, 0.5),\n",
       " (2, 256, 5, 0.001, 1),\n",
       " (2, 256, 5, 0.0001, 0),\n",
       " (2, 256, 5, 0.0001, 0.25),\n",
       " (2, 256, 5, 0.0001, 0.5),\n",
       " (2, 256, 5, 0.0001, 1),\n",
       " (2, 256, 5, 1e-05, 0),\n",
       " (2, 256, 5, 1e-05, 0.25),\n",
       " (2, 256, 5, 1e-05, 0.5),\n",
       " (2, 256, 5, 1e-05, 1),\n",
       " (2, 256, 6, 0.001, 0),\n",
       " (2, 256, 6, 0.001, 0.25),\n",
       " (2, 256, 6, 0.001, 0.5),\n",
       " (2, 256, 6, 0.001, 1),\n",
       " (2, 256, 6, 0.0001, 0),\n",
       " (2, 256, 6, 0.0001, 0.25),\n",
       " (2, 256, 6, 0.0001, 0.5),\n",
       " (2, 256, 6, 0.0001, 1),\n",
       " (2, 256, 6, 1e-05, 0),\n",
       " (2, 256, 6, 1e-05, 0.25),\n",
       " (2, 256, 6, 1e-05, 0.5),\n",
       " (2, 256, 6, 1e-05, 1),\n",
       " (2, 256, 7, 0.001, 0),\n",
       " (2, 256, 7, 0.001, 0.25),\n",
       " (2, 256, 7, 0.001, 0.5),\n",
       " (2, 256, 7, 0.001, 1),\n",
       " (2, 256, 7, 0.0001, 0),\n",
       " (2, 256, 7, 0.0001, 0.25),\n",
       " (2, 256, 7, 0.0001, 0.5),\n",
       " (2, 256, 7, 0.0001, 1),\n",
       " (2, 256, 7, 1e-05, 0),\n",
       " (2, 256, 7, 1e-05, 0.25),\n",
       " (2, 256, 7, 1e-05, 0.5),\n",
       " (2, 256, 7, 1e-05, 1),\n",
       " (2, 256, 8, 0.001, 0),\n",
       " (2, 256, 8, 0.001, 0.25),\n",
       " (2, 256, 8, 0.001, 0.5),\n",
       " (2, 256, 8, 0.001, 1),\n",
       " (2, 256, 8, 0.0001, 0),\n",
       " (2, 256, 8, 0.0001, 0.25),\n",
       " (2, 256, 8, 0.0001, 0.5),\n",
       " (2, 256, 8, 0.0001, 1),\n",
       " (2, 256, 8, 1e-05, 0),\n",
       " (2, 256, 8, 1e-05, 0.25),\n",
       " (2, 256, 8, 1e-05, 0.5),\n",
       " (2, 256, 8, 1e-05, 1),\n",
       " (2, 256, 9, 0.001, 0),\n",
       " (2, 256, 9, 0.001, 0.25),\n",
       " (2, 256, 9, 0.001, 0.5),\n",
       " (2, 256, 9, 0.001, 1),\n",
       " (2, 256, 9, 0.0001, 0),\n",
       " (2, 256, 9, 0.0001, 0.25),\n",
       " (2, 256, 9, 0.0001, 0.5),\n",
       " (2, 256, 9, 0.0001, 1),\n",
       " (2, 256, 9, 1e-05, 0),\n",
       " (2, 256, 9, 1e-05, 0.25),\n",
       " (2, 256, 9, 1e-05, 0.5),\n",
       " (2, 256, 9, 1e-05, 1),\n",
       " (2, 512, 2, 0.001, 0),\n",
       " (2, 512, 2, 0.001, 0.25),\n",
       " (2, 512, 2, 0.001, 0.5),\n",
       " (2, 512, 2, 0.001, 1),\n",
       " (2, 512, 2, 0.0001, 0),\n",
       " (2, 512, 2, 0.0001, 0.25),\n",
       " (2, 512, 2, 0.0001, 0.5),\n",
       " (2, 512, 2, 0.0001, 1),\n",
       " (2, 512, 2, 1e-05, 0),\n",
       " (2, 512, 2, 1e-05, 0.25),\n",
       " (2, 512, 2, 1e-05, 0.5),\n",
       " (2, 512, 2, 1e-05, 1),\n",
       " (2, 512, 3, 0.001, 0),\n",
       " (2, 512, 3, 0.001, 0.25),\n",
       " (2, 512, 3, 0.001, 0.5),\n",
       " (2, 512, 3, 0.001, 1),\n",
       " (2, 512, 3, 0.0001, 0),\n",
       " (2, 512, 3, 0.0001, 0.25),\n",
       " (2, 512, 3, 0.0001, 0.5),\n",
       " (2, 512, 3, 0.0001, 1),\n",
       " (2, 512, 3, 1e-05, 0),\n",
       " (2, 512, 3, 1e-05, 0.25),\n",
       " (2, 512, 3, 1e-05, 0.5),\n",
       " (2, 512, 3, 1e-05, 1),\n",
       " (2, 512, 4, 0.001, 0),\n",
       " (2, 512, 4, 0.001, 0.25),\n",
       " (2, 512, 4, 0.001, 0.5),\n",
       " (2, 512, 4, 0.001, 1),\n",
       " (2, 512, 4, 0.0001, 0),\n",
       " (2, 512, 4, 0.0001, 0.25),\n",
       " (2, 512, 4, 0.0001, 0.5),\n",
       " (2, 512, 4, 0.0001, 1),\n",
       " (2, 512, 4, 1e-05, 0),\n",
       " (2, 512, 4, 1e-05, 0.25),\n",
       " (2, 512, 4, 1e-05, 0.5),\n",
       " (2, 512, 4, 1e-05, 1),\n",
       " (2, 512, 5, 0.001, 0),\n",
       " (2, 512, 5, 0.001, 0.25),\n",
       " (2, 512, 5, 0.001, 0.5),\n",
       " (2, 512, 5, 0.001, 1),\n",
       " (2, 512, 5, 0.0001, 0),\n",
       " (2, 512, 5, 0.0001, 0.25),\n",
       " (2, 512, 5, 0.0001, 0.5),\n",
       " (2, 512, 5, 0.0001, 1),\n",
       " (2, 512, 5, 1e-05, 0),\n",
       " (2, 512, 5, 1e-05, 0.25),\n",
       " (2, 512, 5, 1e-05, 0.5),\n",
       " (2, 512, 5, 1e-05, 1),\n",
       " (2, 512, 6, 0.001, 0),\n",
       " (2, 512, 6, 0.001, 0.25),\n",
       " (2, 512, 6, 0.001, 0.5),\n",
       " (2, 512, 6, 0.001, 1),\n",
       " (2, 512, 6, 0.0001, 0),\n",
       " (2, 512, 6, 0.0001, 0.25),\n",
       " (2, 512, 6, 0.0001, 0.5),\n",
       " (2, 512, 6, 0.0001, 1),\n",
       " (2, 512, 6, 1e-05, 0),\n",
       " (2, 512, 6, 1e-05, 0.25),\n",
       " (2, 512, 6, 1e-05, 0.5),\n",
       " (2, 512, 6, 1e-05, 1),\n",
       " (2, 512, 7, 0.001, 0),\n",
       " (2, 512, 7, 0.001, 0.25),\n",
       " (2, 512, 7, 0.001, 0.5),\n",
       " (2, 512, 7, 0.001, 1),\n",
       " (2, 512, 7, 0.0001, 0),\n",
       " (2, 512, 7, 0.0001, 0.25),\n",
       " (2, 512, 7, 0.0001, 0.5),\n",
       " (2, 512, 7, 0.0001, 1),\n",
       " (2, 512, 7, 1e-05, 0),\n",
       " (2, 512, 7, 1e-05, 0.25),\n",
       " (2, 512, 7, 1e-05, 0.5),\n",
       " (2, 512, 7, 1e-05, 1),\n",
       " (2, 512, 8, 0.001, 0),\n",
       " (2, 512, 8, 0.001, 0.25),\n",
       " (2, 512, 8, 0.001, 0.5),\n",
       " (2, 512, 8, 0.001, 1),\n",
       " (2, 512, 8, 0.0001, 0),\n",
       " (2, 512, 8, 0.0001, 0.25),\n",
       " (2, 512, 8, 0.0001, 0.5),\n",
       " (2, 512, 8, 0.0001, 1),\n",
       " (2, 512, 8, 1e-05, 0),\n",
       " (2, 512, 8, 1e-05, 0.25),\n",
       " (2, 512, 8, 1e-05, 0.5),\n",
       " (2, 512, 8, 1e-05, 1),\n",
       " (2, 512, 9, 0.001, 0),\n",
       " (2, 512, 9, 0.001, 0.25),\n",
       " (2, 512, 9, 0.001, 0.5),\n",
       " (2, 512, 9, 0.001, 1),\n",
       " (2, 512, 9, 0.0001, 0),\n",
       " (2, 512, 9, 0.0001, 0.25),\n",
       " (2, 512, 9, 0.0001, 0.5),\n",
       " (2, 512, 9, 0.0001, 1),\n",
       " (2, 512, 9, 1e-05, 0),\n",
       " (2, 512, 9, 1e-05, 0.25),\n",
       " (2, 512, 9, 1e-05, 0.5),\n",
       " (2, 512, 9, 1e-05, 1),\n",
       " (2, 1024, 2, 0.001, 0),\n",
       " (2, 1024, 2, 0.001, 0.25),\n",
       " (2, 1024, 2, 0.001, 0.5),\n",
       " (2, 1024, 2, 0.001, 1),\n",
       " (2, 1024, 2, 0.0001, 0),\n",
       " (2, 1024, 2, 0.0001, 0.25),\n",
       " (2, 1024, 2, 0.0001, 0.5),\n",
       " (2, 1024, 2, 0.0001, 1),\n",
       " (2, 1024, 2, 1e-05, 0),\n",
       " (2, 1024, 2, 1e-05, 0.25),\n",
       " (2, 1024, 2, 1e-05, 0.5),\n",
       " (2, 1024, 2, 1e-05, 1),\n",
       " (2, 1024, 3, 0.001, 0),\n",
       " (2, 1024, 3, 0.001, 0.25),\n",
       " (2, 1024, 3, 0.001, 0.5),\n",
       " (2, 1024, 3, 0.001, 1),\n",
       " (2, 1024, 3, 0.0001, 0),\n",
       " (2, 1024, 3, 0.0001, 0.25),\n",
       " (2, 1024, 3, 0.0001, 0.5),\n",
       " (2, 1024, 3, 0.0001, 1),\n",
       " (2, 1024, 3, 1e-05, 0),\n",
       " (2, 1024, 3, 1e-05, 0.25),\n",
       " (2, 1024, 3, 1e-05, 0.5),\n",
       " (2, 1024, 3, 1e-05, 1),\n",
       " (2, 1024, 4, 0.001, 0),\n",
       " (2, 1024, 4, 0.001, 0.25),\n",
       " (2, 1024, 4, 0.001, 0.5),\n",
       " (2, 1024, 4, 0.001, 1),\n",
       " (2, 1024, 4, 0.0001, 0),\n",
       " (2, 1024, 4, 0.0001, 0.25),\n",
       " (2, 1024, 4, 0.0001, 0.5),\n",
       " (2, 1024, 4, 0.0001, 1),\n",
       " (2, 1024, 4, 1e-05, 0),\n",
       " (2, 1024, 4, 1e-05, 0.25),\n",
       " (2, 1024, 4, 1e-05, 0.5),\n",
       " (2, 1024, 4, 1e-05, 1),\n",
       " (2, 1024, 5, 0.001, 0),\n",
       " (2, 1024, 5, 0.001, 0.25),\n",
       " (2, 1024, 5, 0.001, 0.5),\n",
       " (2, 1024, 5, 0.001, 1),\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_found_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "g92mE6Ljkoq9",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "g92mE6Ljkoq9"
   },
   "outputs": [],
   "source": [
    "def Llayers(L,d=20,width=1000):\n",
    "    #construct L-1 linear layers; bias term only on last linear layer\n",
    "    if L < 2:\n",
    "        raise ValueError(\"L must be at least 2\")\n",
    "    if L == 2:\n",
    "        linear_layers = [nn.Linear(d,width,bias=True)]\n",
    "    if L > 2:\n",
    "        linear_layers = [nn.Linear(d,width,bias=False)]\n",
    "        for l in range(L-3):\n",
    "            linear_layers.append(nn.Linear(width,width,bias=False))\n",
    "        linear_layers.append(nn.Linear(width,width,bias=True))\n",
    "\n",
    "    relu = nn.ReLU()\n",
    "\n",
    "    last_layer = nn.Linear(width,1)\n",
    "\n",
    "    layers = linear_layers + [relu,last_layer]\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "jl4Oh8Vp4QIb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "jl4Oh8Vp4QIb",
    "outputId": "8514097d-0db9-4bcd-baf4-42b4faad166a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4G22AjiIkYNE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1701128292778,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "4G22AjiIkYNE",
    "outputId": "4147ad49-f1a8-410f-c9bb-6822320eed52",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files_found_list = []\n",
    "models = {}\n",
    "for r,n in rnvals:\n",
    "    for L in Ls:\n",
    "        for wd in wds:\n",
    "            for ln in labelnoise:\n",
    "                paramname = job_name+f\"_labelnoise{ln}/N{n}_L{L}_r{r}_wd{wd}_epochs{epochs}\"\n",
    "                if os.path.exists(paramname+\"model.pt\"):\n",
    "                    models[r,n,L,wd,ln] = Llayers(L,width=1000)\n",
    "                    models[r,n,L,wd,ln].to(device)\n",
    "                    if torch.cuda.is_available():\n",
    "                        models[r,n,L,wd,ln].load_state_dict(torch.load(paramname+\"model.pt\"))\n",
    "                    else:\n",
    "                        models[r,n,L,wd,ln].load_state_dict(torch.load(paramname+\"model.pt\"),map_location=torch.device('cpu'))\n",
    "                    models[r,n,L,wd,ln].eval()\n",
    "                    files_found_list.append((r,n,L,wd,ln))\n",
    "                else:\n",
    "                    print(paramname+\"model.pt\",\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101bd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_found_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wt9Aud_elI-7",
   "metadata": {
    "id": "Wt9Aud_elI-7"
   },
   "source": [
    "## create pandas table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8hubUl6NlIh3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "8hubUl6NlIh3",
    "outputId": "008a8a90-61cb-40fb-e1fe-d5a2df77ddb8"
   },
   "outputs": [],
   "source": [
    "res = {\n",
    "  \"r\"                                    : [r                          for r,n,L,wd,ln in files_found_list],# + [r                         for wd in wds for r,n in relu_rnvals],\n",
    "  \"sigma\"                                : [ln                         for r,n,L,wd,ln in files_found_list],# + [ln                        for wd in wds for r,n in relu_rnvals],\n",
    "  \"n\"                                    : [n                          for r,n,L,wd,ln in files_found_list],# + [n                         for wd in wds for r,n in relu_rnvals],\n",
    "  \"L\"                                    : [L                          for r,n,L,wd,ln in files_found_list],# + [4                         for wd in wds for r,n in relu_rnvals],\n",
    "  \"lambda\"                               : [wd                         for r,n,L,wd,ln in files_found_list],# + [wd                        for wd in wds for r,n in relu_rnvals],\n",
    "  \"Learning Rate\"                        : [learningrates[r,n,L,wd,ln] for r,n,L,wd,ln in files_found_list],# + [RELUlearningrates[r,n][4][wd] for wd in wds for r,n in relu_rnvals],\n",
    "  \"Train MSE\"                            : [trainMSEs[r,n,L,wd,ln]     for r,n,L,wd,ln in files_found_list],# + [RELUtrainMSEs[r,n][4][wd]     for wd in wds for r,n in relu_rnvals],\n",
    "  \"Weight Decay\"                         : [weightdecays[r,n,L,wd,ln]  for r,n,L,wd,ln in files_found_list],# + [RELUweightdecays[r,n][4][wd]  for wd in wds for r,n in relu_rnvals],\n",
    "  \"Model\"                                : [models[r,n,L,wd,ln]        for r,n,L,wd,ln in files_found_list],# + [RELUmodels[r,n,4,wd]          for wd in wds for r,n in relu_rnvals],\n",
    "  \"Test MSE\"                             : [testMSEs[r,n,L,wd,ln]      for r,n,L,wd,ln in files_found_list],# + [RELUtestMSEs[r,n][4][wd].item()      for wd in wds for r,n in relu_rnvals],\n",
    "  \"Activations\"                          : [\"linear and relu\"          for r,n,L,wd,ln in files_found_list],# + [\"relu only\"         for wd in wds for r,n in relu_rnvals]\n",
    "}\n",
    "res = pd.DataFrame(res)\n",
    "res[\"Final Train MSE\"] = [r[-1] for r in res[\"Train MSE\"]]\n",
    "res[\"Final Weight Decay\"] = [r[-1] for r in res[\"Weight Decay\"]]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc089a26",
   "metadata": {},
   "source": [
    "# Initial Training Time Checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659b93a",
   "metadata": {},
   "source": [
    "## Train MSE v Epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ln in labelnoise:\n",
    "#     for lamb in wds:\n",
    "#         f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(20,10))#len(res.r.unique()),len(res.n.unique())\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for rownum,row in res.iterrows():\n",
    "#             if row[\"L\"] <= 4 and row[\"sigma\"] == ln and row[\"lambda\"] == lamb:\n",
    "#                 whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "#                 whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "#                 ax[whichrow,whichcol].semilogy(row[\"Train MSE\"],label=rf\"$L = {row['L']}$\",linewidth=1,alpha=0.5)\n",
    "#                 ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},n = {row['n']}$\")\n",
    "#                 ax[whichrow,whichcol].set_xlabel(\"Epoch\")\n",
    "#         ax[0,0].legend()\n",
    "#         f.suptitle(rf\"Train MSE v Epoch (label noise $N(0,{ln**2})$, weight decay $\\lambda$ = {lamb})\")\n",
    "#         f.savefig(job_name+f\"_labelnoise{ln}/trainmse234_lamb{lamb}.png\",dpi=300)\n",
    "#         plt.show()\n",
    "#         f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(20,10))#len(res.r.unique()),len(res.n.unique())\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for rownum,row in res.iterrows():\n",
    "#             if 4 < row[\"L\"] <= 7 and row[\"sigma\"] == ln and row[\"lambda\"] == lamb:\n",
    "#                 whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "#                 whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "#                 ax[whichrow,whichcol].semilogy(row[\"Train MSE\"],label=rf\"$L = {row['L']}$\",linewidth=1,alpha=0.5)\n",
    "#                 ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},n = {row['n']}$\")\n",
    "#                 ax[whichrow,whichcol].set_xlabel(\"Epoch\")\n",
    "#         ax[0,0].legend()\n",
    "#         f.suptitle(rf\"Train MSE v Epoch (label noise $N(0,{ln**2})$, weight decay $\\lambda$ = {lamb})\")\n",
    "#         f.savefig(job_name+f\"_labelnoise{ln}/trainmse567_lamb{lamb}.png\",dpi=300)\n",
    "#         plt.show()\n",
    "#         f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(20,10))\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for rownum,row in res.iterrows():\n",
    "#             if row[\"L\"] > 7 and row[\"sigma\"] == ln and row[\"lambda\"] == lamb:\n",
    "#                 whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "#                 whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "#                 ax[whichrow,whichcol].semilogy(row[\"Train MSE\"],label=rf\"$L = {row['L']}$\",linewidth=1,alpha=0.5)\n",
    "#                 ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},n = {row['n']}$\")\n",
    "#                 ax[whichrow,whichcol].set_xlabel(\"Epoch\")\n",
    "#         ax[0,0].legend()\n",
    "#         f.suptitle(rf\"Train MSE v Epoch (label noise $N(0,{ln**2})$, weight decay $\\lambda$ = {lamb})\")\n",
    "#         f.savefig(job_name+f\"_labelnoise{ln}/trainmse8910_lamb{lamb}.png\",dpi=300)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1613a4",
   "metadata": {},
   "source": [
    "## Weight Decay v Epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c619fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ln in labelnoise:\n",
    "#     for lamb in wds:\n",
    "#         f, ax = plt.subplots(len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(20,10))#len(res.r.unique()),len(res.n.unique())\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for rownum,row in res.iterrows():\n",
    "#             if row[\"sigma\"] == ln and row[\"lambda\"] == lamb:\n",
    "#                 whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "#                 whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "#                 ax[whichrow,whichcol].semilogy(row[\"Weight Decay\"],label=rf\"$L = {row['L']}$\",linewidth=1,alpha=0.7)\n",
    "#                 ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},n = {row['n']}$\")\n",
    "#                 ax[whichrow,whichcol].set_xlabel(\"Epoch\")\n",
    "#         ax[0,0].legend()\n",
    "#         f.suptitle(rf\"Weight Decay v Epoch (label noise $N(0,{ln**2})$, weight decay $\\lambda$ = {lamb})\")\n",
    "#         f.savefig(job_name+f\"_labelnoise{ln}/weightdecay_lamb{lamb}.png\",dpi=300)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605ff27",
   "metadata": {},
   "source": [
    "## learning rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064707ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ln in labelnoise:\n",
    "#     for lamb in wds:\n",
    "#         f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(20,10))#len(res.r.unique()),len(res.n.unique())\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for rownum,row in res.iterrows():\n",
    "#             if row[\"sigma\"] == ln and row[\"lambda\"] == lamb:\n",
    "#                     whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "#                     whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "#                     ax[whichrow,whichcol].semilogy(row[\"Learning Rate\"],label=rf\"$L = {row['L']}$\",linewidth=1,alpha=0.7)\n",
    "#                     ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},n = {row['n']}$\")\n",
    "#                     ax[whichrow,whichcol].set_xlabel(\"Epoch\")\n",
    "#         ax[0,0].legend()\n",
    "#         f.suptitle(rf\"Learning Rate v Epoch (label noise $N(0,{ln**2})$, weight decay $\\lambda$ = {lamb})\")\n",
    "#         f.savefig(job_name+f\"_labelnoise{ln}/LearningRate_lamb{lamb}.png\",dpi=300)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2d3b4",
   "metadata": {},
   "source": [
    "# filter out bad training losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMSE_threshold = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res[\"Final Train MSE\"] >= trainMSE_threshold + res[\"sigma\"]] #TODO is this reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aX7gRNo_xet",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "4aX7gRNo_xet"
   },
   "outputs": [],
   "source": [
    "res = res[res[\"Final Train MSE\"] < trainMSE_threshold + res[\"sigma\"]]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96588e81",
   "metadata": {},
   "source": [
    "# generate data function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(datasetsize,r,seed,std,labelnoiseseed,trainsize=2**18,testsize=2**10,d=20,funcseed=42,verbose=False,ood=False):\n",
    "\n",
    "    ##Generate data with a true central subspaces of varying dimensions\n",
    "    #generate X values for training and test sets\n",
    "    np.random.seed(seed) #set seed for data generation\n",
    "    trainX = np.random.rand(d,trainsize).astype(np.float32)[:,:datasetsize] - 0.5 #distributed as U[-1/2, 1/2]\n",
    "    testX = np.random.rand(d,testsize).astype(np.float32) - 0.5 #distributed as U[-1/2, 1/2]\n",
    "    #out of distribution datagen\n",
    "    if ood:\n",
    "        trainX *= 2 #now distributed as U[-1, 1]\n",
    "        testX *= 2 #now distributed as U[-1, 1]\n",
    "    ##for each $r$ value create and store data-gen functions and $y$ evaluations\n",
    "    #geneate params for functions\n",
    "    k = d+1\n",
    "    U = np.load(job_name+f\"_labelnoise{std}/r{r}U.npy\")\n",
    "    Sigma = np.load(job_name+f\"_labelnoise{std}/r{r}Sigma.npy\")\n",
    "    V = np.load(job_name+f\"_labelnoise{std}/r{r}V.npy\")\n",
    "    A = np.load(job_name+f\"_labelnoise{std}/r{r}A.npy\")\n",
    "    B = np.load(job_name+f\"_labelnoise{std}/r{r}B.npy\")\n",
    "    #create functions\n",
    "    np.random.seed(labelnoiseseed) #set seed for data generation\n",
    "    def g(z): #active subspace function\n",
    "        hidden_layer = (U*Sigma)@z\n",
    "        hidden_layer = hidden_layer.T + B\n",
    "        hidden_layer = np.maximum(0,hidden_layer).T\n",
    "        return A@hidden_layer\n",
    "    def f(x): #teacher network\n",
    "        z = V.T@x    \n",
    "        eps = std*np.random.randn(x.shape[1])    \n",
    "        return g(z) + eps\n",
    "    #generate data\n",
    "    trainY = f(trainX).astype(np.float32)\n",
    "    testY = f(testX).astype(np.float32)\n",
    "    #move data to device\n",
    "    if verbose:\n",
    "        print(\"device: {}\".format(device))\n",
    "    trainX = torch.from_numpy(trainX).T.to(device)\n",
    "    trainY = torch.from_numpy(trainY).to(device)\n",
    "    testX = torch.from_numpy(testX).T.to(device)\n",
    "    testY = torch.from_numpy(testY).to(device)\n",
    "    if verbose:\n",
    "        print(\"trainX shape = {} trainY shape = {}\".format(\n",
    "            trainX.shape,\n",
    "            trainY.shape\n",
    "        ))\n",
    "    return trainX,trainY,testX,testY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ed4f3",
   "metadata": {},
   "source": [
    "# Validation MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf426e6",
   "metadata": {},
   "source": [
    "\n",
    "## generate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36305fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "validationY = {}\n",
    "\n",
    "validationsize = 2048\n",
    "for r in rs:\n",
    "    for k,std in enumerate(labelnoise):\n",
    "        labelnoiseseed = 686 + k\n",
    "        datagenseed = 1107\n",
    "        print(\"validation size =\",validationsize,\"r =\",r,\"label noise std =\",std,\"label noise seed =\",labelnoiseseed)\n",
    "        validationX,validationY[r,std] = gen_data(datasetsize=validationsize,r=r,seed=datagenseed,std=std,labelnoiseseed=labelnoiseseed)[:2]\n",
    "validationX.min(),validationX.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e5647",
   "metadata": {},
   "source": [
    "## compute squared errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c299da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    validation = []\n",
    "    normalized_validation = []\n",
    "    squared_errors = []\n",
    "    validation_sem = []\n",
    "    validation_std = []\n",
    "    for rownum, row in res.iterrows():\n",
    "        std = row[\"sigma\"]\n",
    "        predY = row[\"Model\"](validationX)\n",
    "        squared_err = (predY[:,0] - validationY[row[\"r\"],std])**2\n",
    "        squared_err = squared_err.cpu().numpy()\n",
    "        mse = nn.functional.mse_loss(predY[:,0],validationY[row[\"r\"],std]).item()\n",
    "        assert np.isclose(mse,np.mean(squared_err))\n",
    "        validation.append(mse)\n",
    "        sem_sqared_err = sem(squared_err)\n",
    "        validation_sem.append(sem_sqared_err)\n",
    "        std_sqared_err = np.std(squared_err)\n",
    "        validation_std.append(std_sqared_err)\n",
    "        if std > 0:\n",
    "            normalized_validation.append(mse/(std**2))\n",
    "        else:\n",
    "            normalized_validation.append(np.nan)\n",
    "        squared_errors.append(squared_err)\n",
    "    res[\"Validation MSE\"] = validation\n",
    "    res[\"Validation MSE$/\\sigma^2$\"] = normalized_validation\n",
    "    res[\"Validation Squared Errors\"] = squared_errors\n",
    "    res[\"Validation SEM\"] = validation_sem\n",
    "    res[\"Validation STD of Squared Errors\"] = validation_std\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f6014",
   "metadata": {},
   "source": [
    "# Generalization MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf53019",
   "metadata": {},
   "source": [
    "## generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pNH8hcsX35vb",
   "metadata": {
    "id": "pNH8hcsX35vb"
   },
   "outputs": [],
   "source": [
    "generalizationY = {}\n",
    "generalizationsize = 2048\n",
    "for r in rs:\n",
    "    for k,std in enumerate(labelnoise):\n",
    "        labelnoiseseed = 743 + k\n",
    "        datagenseed = 555\n",
    "        print(\"generalization size =\",generalizationsize,\"r =\",r,\"label noise std =\",std,\"label noise seed =\",labelnoiseseed)\n",
    "        generalizationX,generalizationY[r,std] = gen_data(datasetsize=generalizationsize,r=r,seed=datagenseed,std=std,labelnoiseseed=labelnoiseseed)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SGjV4q985lrM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1691614314236,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 300
    },
    "id": "SGjV4q985lrM",
    "outputId": "20b86ce8-f70f-4eec-d2cc-4c6313a4bfa0"
   },
   "outputs": [],
   "source": [
    "generalizationX.min(),generalizationX.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iHCTTCWw4l4s",
   "metadata": {
    "id": "iHCTTCWw4l4s"
   },
   "source": [
    "## compute squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulWXCTcd2_Xo",
   "metadata": {
    "id": "ulWXCTcd2_Xo"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generalization = []\n",
    "    normalized_generalization = []\n",
    "    squared_errors = []\n",
    "    generalization_sem = []\n",
    "    generalization_std = []\n",
    "    for rownum, row in res.iterrows():\n",
    "        std = row[\"sigma\"]\n",
    "        predY = row[\"Model\"](generalizationX)\n",
    "        squared_err = (predY[:,0] - generalizationY[row[\"r\"],std])**2\n",
    "        squared_err = squared_err.cpu().numpy()\n",
    "        mse = nn.functional.mse_loss(predY[:,0],generalizationY[row[\"r\"],std]).item()\n",
    "        assert np.isclose(mse,np.mean(squared_err))\n",
    "        generalization.append(mse)\n",
    "        sem_sqared_err = sem(squared_err)\n",
    "        generalization_sem.append(sem_sqared_err)\n",
    "        std_sqared_err = np.std(squared_err)\n",
    "        generalization_std.append(std_sqared_err)\n",
    "        if std > 0:\n",
    "            normalized_generalization.append(mse/(std**2))\n",
    "        else:\n",
    "            normalized_generalization.append(np.nan)\n",
    "        squared_errors.append(squared_err)\n",
    "    res[\"Generalization MSE\"] = generalization\n",
    "    res[\"Generalization MSE$/\\sigma^2$\"] = normalized_generalization\n",
    "    res[\"Generalization Squared Errors\"] = squared_errors\n",
    "    res[\"Generalization SEM\"] = generalization_sem\n",
    "    res[\"Generalization STD of Squared Errors\"] = generalization_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LbkhZQcT8Z4S",
   "metadata": {
    "id": "LbkhZQcT8Z4S"
   },
   "source": [
    "# Out of Distribution MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GEplm-eG8g74",
   "metadata": {
    "id": "GEplm-eG8g74"
   },
   "source": [
    "## generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc4f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "oodY = {}\n",
    "oodsize = 2048\n",
    "for r in rs:\n",
    "    for k,std in enumerate(labelnoise):\n",
    "        labelnoiseseed = 235 + k\n",
    "        datagenseed = 333\n",
    "        print(\"ood size =\",oodsize,\"r =\",r,\"label noise std =\",std,\"label noise seed =\",labelnoiseseed)\n",
    "        oodX,oodY[r,std] = gen_data(datasetsize=oodsize,r=r,seed=datagenseed,std=std,labelnoiseseed=labelnoiseseed,ood=True)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SiXhFaOE8n4j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1691614317198,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 300
    },
    "id": "SiXhFaOE8n4j",
    "outputId": "cf4681fa-710e-4e13-b705-78939987d20c"
   },
   "outputs": [],
   "source": [
    "oodX.min(),oodX.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yt15AMGL84jk",
   "metadata": {
    "id": "yt15AMGL84jk"
   },
   "source": [
    "## compute squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ood = []\n",
    "    normalized_ood = []\n",
    "    squared_errors = []\n",
    "    ood_sem = []\n",
    "    ood_std = []\n",
    "    for rownum, row in res.iterrows():\n",
    "        std = row[\"sigma\"]\n",
    "        predY = row[\"Model\"](oodX)\n",
    "        squared_err = (predY[:,0] - oodY[row[\"r\"],std])**2\n",
    "        squared_err = squared_err.cpu().numpy()\n",
    "        mse = nn.functional.mse_loss(predY[:,0],oodY[row[\"r\"],std]).item()\n",
    "        assert np.isclose(mse,np.mean(squared_err))\n",
    "        ood.append(mse)\n",
    "        sem_sqared_err = sem(squared_err)\n",
    "        ood_sem.append(sem_sqared_err)\n",
    "        std_sqared_err = np.std(squared_err)\n",
    "        ood_std.append(std_sqared_err)\n",
    "        if std > 0:\n",
    "            normalized_ood.append(mse/(std**2))\n",
    "        else:\n",
    "            normalized_ood.append(np.nan)\n",
    "        squared_errors.append(squared_err)\n",
    "    res[\"Out of Distribution MSE\"] = ood\n",
    "    res[\"Out of Distribution MSE$/\\sigma^2$\"] = normalized_ood\n",
    "    res[\"Out of Distribution Squared Errors\"] = squared_errors\n",
    "    res[\"Out of Distribution SEM\"] = ood_sem\n",
    "    res[\"Out of Distribution STD of Squared Errors\"] = ood_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c9fb6",
   "metadata": {},
   "source": [
    "# Check that most or all ReLU hyperplanes intersect the support of the distributions of the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in rs:\n",
    "    print(\"r =\",r)\n",
    "    U = np.load(job_name+f\"_labelnoise0/r{r}U.npy\")\n",
    "    Sigma = np.load(job_name+f\"_labelnoise0/r{r}Sigma.npy\")\n",
    "    V = np.load(job_name+f\"_labelnoise0/r{r}V.npy\")\n",
    "    A = np.load(job_name+f\"_labelnoise0/r{r}A.npy\")\n",
    "    B = np.load(job_name+f\"_labelnoise0/r{r}B.npy\")\n",
    "    W = (U*Sigma)@V.T\n",
    "    rowwise1norms = np.linalg.norm(W,axis=1,ord=1)\n",
    "    ratios = np.abs(B) / np.linalg.norm(W,axis=1,ord=1)\n",
    "    rowwise2norms = np.linalg.norm(W,axis=1,ord=2)\n",
    "\n",
    "    for datasetsize in res.n.unique():\n",
    "        units = pd.DataFrame({\"R2-cost contribution\":np.abs(A)*rowwise2norms,\"|b| / ||w||_1\":ratios})\n",
    "        # units[\"[-1/2,1/2]\"] = ratios <= 1/2\n",
    "        # units[\"[-1,1] but not [-1/2,1/2]\"] = (1/2 < ratios) * (ratios <= 1)\n",
    "        # units[\"not [-1,1]\"] = ratios > 1\n",
    "        trainX = gen_data(datasetsize=datasetsize,r=r,seed=1,std=0,labelnoiseseed=0)[0]\n",
    "        units[\"# training active\"] = ((W@trainX.cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "        units[\"% training active\"] = units[\"# training active\"] / datasetsize\n",
    "        units[\"# validation active\"] = ((W@validationX.cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "        units[\"% validation active\"] = units[\"# validation active\"] / datasetsize\n",
    "        units[\"# generalization active\"] = ((W@generalizationX.cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "        units[\"% generalization active\"] = units[\"# generalization active\"] / datasetsize\n",
    "        units[\"# ood active\"] = ((W@oodX.cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "        units[\"% ood active\"] = units[\"# ood active\"] / datasetsize\n",
    "\n",
    "        print(\"\\nTOTALS:\\n~~~~~~~\\n\",units.sum())\n",
    "        print(\"\\nunit-wise table:\\n~~~~~~~\\n\")\n",
    "        display(units)\n",
    "        plt.bar(units.index-0.2,units[\"# training active\"]/datasetsize,label=\"train\",width=0.4,tick_label=units[\"R2-cost contribution\"].round(1))\n",
    "        # plt.bar(units.index,units[\"# generalization active\"]/datasetsize,label=\"gen\",width=0.4)\n",
    "        plt.bar(units.index+0.2,units[\"# ood active\"]/datasetsize,label=\"ood\",width=0.4)\n",
    "        plt.ylim(0,1)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f\"How many samples is each unit active on? $r = {r}, n = {datasetsize}$\")\n",
    "        plt.xlabel(\"ReLU Unit, labeled by R2-cost contribution\")\n",
    "        plt.ylabel(\"Proportion of samples\")\n",
    "        plt.axhline(0.5,linestyle=\":\",color=\"k\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"samples_active_by_unit_r{r}_n{datasetsize}\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec05e2a",
   "metadata": {
    "id": "8ec05e2a"
   },
   "source": [
    "# Active Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pFRGq1re-Uzi",
   "metadata": {
    "id": "pFRGq1re-Uzi"
   },
   "source": [
    "## evaluate gradients and compute singular values and active subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0jES0e-cYG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4768,
     "status": "ok",
     "timestamp": 1691614322192,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 300
    },
    "id": "9f0jES0e-cYG",
    "outputId": "5d4441ca-c283-47e7-8957-aaff36383b6a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grads = []\n",
    "sv = []\n",
    "active_subspace = []\n",
    "subspace_dist = []\n",
    "\n",
    "for rownum, row in res.iterrows():\n",
    "    #compute ground truth active subspace\n",
    "    funcseed = 42\n",
    "    d = 20\n",
    "    k = d+1\n",
    "    ln = row['sigma']\n",
    "    if int(ln) == ln:\n",
    "        ln = int(ln)\n",
    "    U = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}U.npy\")\n",
    "    Sigma = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}Sigma.npy\")\n",
    "    V = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}V.npy\")\n",
    "    W = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}W.npy\")\n",
    "    A = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}A.npy\")\n",
    "    B = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}B.npy\")\n",
    "\n",
    "    #evaluate gradients\n",
    "    generalizationX.requires_grad = True\n",
    "    predY = row[\"Model\"](generalizationX)\n",
    "    grad = torch.autograd.grad(predY, generalizationX,\n",
    "                            grad_outputs=torch.ones_like(predY),\n",
    "                            create_graph=True)[0].detach().cpu().numpy()\n",
    "    grads.append(grad)\n",
    "    #compute active subspace and singular values\n",
    "    Uhat,Shat,VhatT = np.linalg.svd(grad)\n",
    "    Vhat = VhatT.T[:,:row[\"r\"]] #form the basis for the active subspace\n",
    "    active_subspace.append(Vhat)\n",
    "    sv.append(Shat)\n",
    "\n",
    "    subspace_dist.append(np.linalg.norm(V@V.T - Vhat@Vhat.T,2))\n",
    "\n",
    "res[\"Gradient Evaluations\"] = grads\n",
    "res[\"Gradient Singular Values\"] = sv\n",
    "res[\"Active Subspace\"] = active_subspace\n",
    "res[\"Active Subspace Distance\"] = subspace_dist\n",
    "res[\"Angle Error (Degrees)\"] = np.degrees(np.arcsin(res[\"Active Subspace Distance\"]))\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297500cd",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QfTgwsYWt0JN",
   "metadata": {
    "id": "QfTgwsYWt0JN"
   },
   "source": [
    "##  determine the lambda parameter that gets the best Validation MSE for each (r,n,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RJvhVj2QsOzz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "RJvhVj2QsOzz",
    "outputId": "f5ab0e61-f93e-4d47-c809-af5ef4d06e06",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validationmse_vs_lambda = res.pivot_table(values=\"Validation MSE\",index = (\"r\",\"sigma\",\"n\",\"L\",\"Activations\"),columns=[\"lambda\"])\n",
    "validationmse_vs_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff1bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestlambda = validationmse_vs_lambda.idxmin(axis=1)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(bestlambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K4ItJEBcjtcZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "K4ItJEBcjtcZ",
    "outputId": "a6ab9ade-ea9d-4b83-b14b-29dd2811b57d"
   },
   "outputs": [],
   "source": [
    "mask = [row[\"lambda\"] == bestlambda[row[\"r\"]][row[\"sigma\"]][row[\"n\"]][row[\"L\"]][row[\"Activations\"]] for rowindex,row in res.iterrows()]\n",
    "res = res[mask]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fabcf44",
   "metadata": {
    "id": "QfTgwsYWt0JN"
   },
   "source": [
    "##  determine the L parameter that gets the best validation MSE for each (r,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc2546",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "RJvhVj2QsOzz",
    "outputId": "f5ab0e61-f93e-4d47-c809-af5ef4d06e06",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validationmse_vs_L = res.pivot_table(values=\"Validation MSE\",index = (\"r\",\"sigma\",\"n\",\"Activations\"),columns=[\"L\"])\n",
    "validationmse_vs_L = validationmse_vs_L.iloc[:,1:]\n",
    "bestL = validationmse_vs_L.idxmin(axis=1)\n",
    "pd.concat((validationmse_vs_L,bestL),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb5ccf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "K4ItJEBcjtcZ",
    "outputId": "a6ab9ade-ea9d-4b83-b14b-29dd2811b57d"
   },
   "outputs": [],
   "source": [
    "mask = [row[\"L\"] == bestL[row[\"r\"]][row[\"sigma\"]][row[\"n\"]][row[\"Activations\"]] for rowindex,row in res.iterrows()]\n",
    "bestLres = res[mask]\n",
    "bestLres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daefd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestLres.sort_values(by=['r','n',\"sigma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f1d0c",
   "metadata": {},
   "source": [
    "## What are the chosen lambda and L for each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestres = pd.concat((res[res[\"L\"] == 2],bestLres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(bestres.loc[:,:\"lambda\"].pivot_table(index=[\"r\",\"sigma\",\"n\",\"L\"],values=[\"lambda\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364946f",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ecf870",
   "metadata": {},
   "source": [
    "## plots of all the singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cacc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the singular values\n",
    "for std in labelnoise:\n",
    "    f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(10,4.8))\n",
    "    for rownum,row in res.iterrows():\n",
    "        if row['sigma'] == std:\n",
    "            whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "            whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "            print(whichrow,whichcol)\n",
    "            print(rf\"{row['r']},{row['n']},{row['L']}\",row[\"Gradient Singular Values\"]/np.sqrt(2048),whichrow,whichcol)\n",
    "            ax[whichrow,whichcol].semilogy(row[\"Gradient Singular Values\"]/np.sqrt(2048),label=rf\"$L={row['L']}$\",linewidth=1,alpha=0.7,marker=\".\")\n",
    "            ax[whichrow,whichcol].set_xticks(list(range(3,20,4)),list(range(4,21,4)))\n",
    "            ax[whichrow,whichcol].set_ylim(10**-9,10**3)\n",
    "            ax[0,whichcol].set_title(rf\"$n={row['n']}$\")\n",
    "            ax[-1,whichcol].set_xlabel(rf\"Index, $k$\")\n",
    "    plt.subplot(2,len(res.n.unique()),1)\n",
    "    leg = plt.legend()\n",
    "    leg = plt.legend(bbox_to_anchor=(-1, 1))\n",
    "    leg.get_frame().set_edgecolor('b')\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "    plt.subplot(2,len(res.n.unique()),1)\n",
    "    plt.ylabel(r\"$r=1$\"+\"\\n\"+r\"$\\sigma_k(\\hat f;\\rho)$\")\n",
    "    plt.yticks([10**p for p in range(-12,3,2)])\n",
    "    plt.subplot(2,len(res.n.unique()),len(res.n.unique())+1)\n",
    "    plt.ylabel(r\"$r=2$\"+\"\\n\"+r\"$\\sigma_k(\\hat f;\\rho)$\")\n",
    "    plt.yticks([10**p for p in range(-12,3,2)])\n",
    "    plt.suptitle(rf\"Singular Values of Trained Networks, $\\sigma =$ {std}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(job_name+f\"_labelnoise{std}/sv.png\",dpi=300)#,bbox_extra_artists=(leg,), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vKIELwC_GeZK",
   "metadata": {
    "id": "vKIELwC_GeZK"
   },
   "source": [
    "## Plots of L vs Validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gvOUGojKGWvw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 1932,
     "status": "ok",
     "timestamp": 1701129174191,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "gvOUGojKGWvw",
    "outputId": "378581a5-d4b5-4eed-8e60-29b5134124ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for std in res[\"sigma\"].unique():\n",
    "    f, ax = plt.subplots(ncols=len(res.r.unique()),nrows=1, sharex=True, sharey=False, figsize=(10,4.8))\n",
    "    for rnum,r in enumerate(res.r.unique()):\n",
    "        for n in res.n.unique():\n",
    "                res_rnstd = res[(res.r == r) * (res.n == n) * (res[\"sigma\"] == std)]\n",
    "                ax[rnum].scatter(res_rnstd.L,res_rnstd[[\"Validation MSE\"]])\n",
    "                ax[rnum].semilogy(res_rnstd.L,res_rnstd[[\"Validation MSE\"]],label=rf\"$n={n}$\")\n",
    "                for _,model in res_rnstd.iterrows():\n",
    "                    text = rf'$\\lambda = {model[\"lambda\"]:.0e}$' + f'\\nfit {model[\"Final Train MSE\"]:.1e}\\nwd{model[\"Final Weight Decay\"]:.1e}'\n",
    "                    ax[rnum].annotate(text,[model.L,model[[\"Validation MSE\"]]],fontsize=1)\n",
    "        ax[rnum].set_xlabel(\"$L$ number of layers\")\n",
    "        ax[rnum].set_title(rf\"$r={r}$\")\n",
    "        if std > 0:\n",
    "            ax[rnum].axhline(y=std**2, color='k', linestyle=':',label=\"$\\sigma^2$\")\n",
    "        ax[0].set_ylabel(\"Validation MSE\")\n",
    "        f.suptitle(rf\"Validation MSE for best $\\lambda$ values, $\\sigma$ = {std}\")\n",
    "    ax[0].legend()\n",
    "    f.tight_layout()\n",
    "    if int(std) == std:\n",
    "        std = int(std)\n",
    "    f.savefig(job_name+f\"_labelnoise{std}/ValidationMSE.png\",dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad12a9",
   "metadata": {},
   "source": [
    "## Performance metrics with/without linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53158aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = {  \n",
    "    0   :\"C0\",\n",
    "    0.25:\"C1\",\n",
    "    0.5 :\"C2\",\n",
    "    1   :\"C3\"\n",
    "}\n",
    "handles = [\n",
    "    Line2D([0], [0], color=color, ls='-', label=rf\"$\\sigma =${sigma}\") for sigma,color in colors.items()\n",
    "]\n",
    "labels = {\n",
    "    \"without linear layers\":\"-\",\n",
    "    \"with linear layers\"   :\"--\",\n",
    "}\n",
    "markers = {\n",
    "    \"without linear layers\":\".\",\n",
    "    \"with linear layers\"   :\"x\",\n",
    "}\n",
    "handles += [\n",
    "    Line2D([0], [0], color='k', ls=ls, label=label, marker = markers[label], markersize=4) for label,ls in labels.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f60e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranktol = 1e-3\n",
    "handles += [Line2D([0], [0], color='k', ls=':', label=\"rank tolerance cutoff\")]\n",
    "f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(15,5))\n",
    "for row,r in enumerate(res.r.unique()):\n",
    "    for col,n in enumerate(res.n.unique()):\n",
    "        for sigma in labelnoise:\n",
    "            for label,ls in labels.items():\n",
    "                if label == \"with linear layers\":\n",
    "                    curr = bestLres[(bestLres.r == r) * (bestLres.n == n) * (bestLres[\"sigma\"] == sigma)]\n",
    "                elif label == \"without linear layers\":\n",
    "                    curr = res[(res.L == 2) * (res.n == n) * (res.r == r) * (res[\"sigma\"]==sigma)]\n",
    "                print(r,n,label,sigma,curr[\"Gradient Singular Values\"].values[0]/np.sqrt(2048))\n",
    "                marker = markers[label]\n",
    "                ax[row,col].semilogy(curr[\"Gradient Singular Values\"].values[0]/np.sqrt(2048),linestyle=ls,linewidth=1,alpha=0.7,marker=marker,markersize=4,color=colors[sigma])\n",
    "        ax[row,col].axhline(y=ranktol, color='k', linestyle=':',alpha=1, label = rf\"rank tolerance cutoff, $\\varepsilon = {ranktol}$\")\n",
    "        ax[row,col].set_xticks(list(range(3,20,4)),list(range(4,21,4)))\n",
    "        ax[-1,col].set_xlabel(rf\"Index, $k$\")\n",
    "        ax[row,0].set_ylabel(rf\"$r={r}$\"+\"\\n\"+r\"$\\sigma_k(\\hat f;\\rho)$\")\n",
    "        ax[0,0].set_yticks([10**p for p in range(-9,3,2)])\n",
    "        ax[row,col].set_ylim(10**-9,10**3)\n",
    "        ax[0,col].set_title(rf\"$n={n}$\")\n",
    "        ax[0,-1].legend(handles=handles, loc='best', bbox_to_anchor=(1.05,1))\n",
    "plt.suptitle(rf\"Singular Values of Trained Networks after $\\lambda, L$ tuning\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(job_name+f\"_labelnoise_sv.png\",dpi=600)#,bbox_extra_artists=(leg,), bbox_inches='tight')\n",
    "plt.show()\n",
    "handles.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#active subspace error plot\n",
    "f, ax = plt.subplots(ncols=len(res.r.unique()),nrows=2, sharex=True, sharey=False, figsize=(10,9))\n",
    "for row,metric in enumerate([rf\"Effective Rank, $\\varepsilon = {ranktol}$\",\"Angle Error (Degrees)\"]):\n",
    "    for col,r in enumerate(res.r.unique()):\n",
    "        for sigma in [0,0.25,0.5,1]:\n",
    "            ax[row,0].set_ylabel(f\"{metric}\")\n",
    "            for label,ls in labels.items():\n",
    "                if label == \"with linear layers\":\n",
    "                    curr = bestLres[(bestLres.r == r) * (bestLres[\"sigma\"] == sigma)]\n",
    "                elif label == \"without linear layers\":\n",
    "                    curr = res[(res.L == 2) * (res.r == r) * (res[\"sigma\"]==sigma)]\n",
    "                if metric == \"Angle Error (Degrees)\":\n",
    "                    points = curr[[metric]].values[:,0]\n",
    "                elif metric == rf\"Effective Rank, $\\varepsilon = {ranktol}$\":\n",
    "                    points = (np.array(curr[\"Gradient Singular Values\"].tolist())/np.sqrt(2048) > ranktol).sum(axis=1)\n",
    "                    ax[row,col].set_yticks(np.arange(0,21,2))\n",
    "                    ax[row,col].set_ylim(0,20.5)\n",
    "                marker = markers[label]\n",
    "                ax[row,col].plot(curr.n,points,\n",
    "                                    linestyle=ls,\n",
    "                                    color=colors[sigma],\n",
    "                                    marker=marker,\n",
    "                                    markersize=4,\n",
    "                                    alpha=0.8)\n",
    "                for (_,model),y in zip(curr.iterrows(),points):\n",
    "                    text = rf'$\\lambda = {model[\"lambda\"]:.0e}$' + f'\\nL = {model[\"L\"]}\\nfit {model[\"Final Train MSE\"]:.1e}\\nwd{model[\"Final Weight Decay\"]:.1e}\\nVal{model[\"Validation MSE\"]:.1e}'\n",
    "                    ax[row,col].annotate(text,[model[\"n\"],y],fontsize=1)\n",
    "        #plot set up\n",
    "        ax[0,col].set_title(rf\"$r={r}$\") \n",
    "        ax[0,1].legend(handles=handles, loc='best', bbox_to_anchor=(1.05,1))\n",
    "        ax[row,col].set_xscale(\"log\",base=2)\n",
    "        ax[1,col].set_xlabel(\"$n$ number of samples\")\n",
    "\n",
    "plt.suptitle(f\"Active Subspaces after $\\lambda, L$ tuning\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(job_name+f\"Active Subspaces after tuning lambda and L.png\",dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a79e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generalization and OOD errors\n",
    "handles += [\n",
    "        Line2D([0], [0], color='k', ls=':', label='$\\sigma^2$, variance of label noise'),\n",
    "]\n",
    "\n",
    "for metric in ['Generalization MSE','Out of Distribution MSE']:\n",
    "    standard_errors = metric[:-3] + 'SEM'\n",
    "    f, ax = plt.subplots(ncols=len(res.r.unique()),nrows=2, sharex=True, sharey=\"row\", figsize=(10,9))\n",
    "    #just the data without label noise in the first row\n",
    "    for col,r in enumerate(res.r.unique()):\n",
    "        for row,sigmas in enumerate([[0],[0.25,0.5,1]]):\n",
    "            for sigma in sigmas:\n",
    "                for label,ls in labels.items():\n",
    "                    if label == \"with linear layers\":\n",
    "                        curr = bestLres[(bestLres.r == r) * (bestLres[\"sigma\"] == sigma)]\n",
    "                    elif label == \"without linear layers\":\n",
    "                        curr = res[(res.L == 2) * (res.r == r) * (res[\"sigma\"]==sigma)]\n",
    "                    points = curr[[metric]].values[:,0]\n",
    "                    errorbars = curr[[standard_errors]].values[:,0]\n",
    "                    marker = markers[label]\n",
    "                    ax[row,col].plot(curr.n,points,\n",
    "                                        linestyle=ls,\n",
    "                                        marker=marker,\n",
    "                                        markersize=4,\n",
    "                                        color=colors[sigma],\n",
    "                                        alpha=0.8)\n",
    "                    #horizontal dashed line for minimal possible MSE (ie sigma^2) in plots with label noise\n",
    "                    ax[1,col].axhline(y=sigma**2, color=colors[sigma], linestyle=':',alpha=0.3)\n",
    "                    for (_,model),y in zip(curr.iterrows(),points):\n",
    "                        text = rf'$\\lambda = {model[\"lambda\"]:.0e}$' + f'\\nL = {model[\"L\"]}\\nfit {model[\"Final Train MSE\"]:.1e}\\nwd{model[\"Final Weight Decay\"]:.1e}\\nVal{model[\"Validation MSE\"]:.1e}'\n",
    "                        ax[row,col].annotate(text,[model[\"n\"],y],fontsize=1)\n",
    "            #plot set up\n",
    "            ax[row,col].set_xscale(\"log\",base=2)\n",
    "            ax[row,col].set_yscale(\"log\",base=10)\n",
    "            ax[row,0].set_ylabel(f\"{metric}\")\n",
    "            ax[0,col].set_title(rf\"$r={r}$\") \n",
    "            ax[1,col].set_xlabel(\"$n$ number of samples\")\n",
    "            ax[0,1].legend(handles=handles, loc='best', bbox_to_anchor=(1.05,1))\n",
    "    plt.suptitle(f\"{metric}\" + r\" after $\\lambda, L$ tuning\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(job_name+f\"{metric}.png\",dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ba908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     pd.options.display.float_format = '{:.1e}'.format\n",
    "#     display(bestres.loc[:,:\"Generalization MSE\"].pivot_table(index=[\"r\",\"sigma\",\"n\",\"L\"],values=[\"Generalization MSE\"])-0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4178b7",
   "metadata": {},
   "source": [
    "## Visualizing Functions in Low Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe485ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_func(r,std=0):\n",
    "    #geneate params for functions\n",
    "    k = d+1\n",
    "    U = np.load(job_name+f\"_labelnoise{std}/r{r}U.npy\")\n",
    "    Sigma = np.load(job_name+f\"_labelnoise{std}/r{r}Sigma.npy\")\n",
    "    V = np.load(job_name+f\"_labelnoise{std}/r{r}V.npy\")\n",
    "    A = np.load(job_name+f\"_labelnoise{std}/r{r}A.npy\")\n",
    "    B = np.load(job_name+f\"_labelnoise{std}/r{r}B.npy\")\n",
    "    #create functions\n",
    "    def g(z): #active subspace function\n",
    "        hidden_layer = (U*Sigma)@z\n",
    "        hidden_layer = hidden_layer.T + B\n",
    "        hidden_layer = np.maximum(0,hidden_layer).T\n",
    "        return A@hidden_layer\n",
    "    def f(x): #teacher network\n",
    "        z = V.T@x    \n",
    "        return g(z)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1\n",
    "f = get_ground_truth_func(r=r) #ground truth function\n",
    "v = np.load(job_name+f\"_labelnoise{std}/r{r}V.npy\")[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584d1f3",
   "metadata": {},
   "source": [
    "plot of projetion onto true 1d subspace of both models and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a79997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onedimproj(f,v,z,istorch=True):\n",
    "    \"\"\"\n",
    "    Create a projection of the function onto 1D inputs. More specifically, return f(vz')\n",
    "    \"\"\"\n",
    "    #create points vz at which to evaluate f\n",
    "    vz = np.outer(v, z)\n",
    "    if istorch:\n",
    "        vz = np.float32(vz)\n",
    "        vz = torch.from_numpy(vz).to(device).T\n",
    "        #function evals\n",
    "        with torch.no_grad():\n",
    "            fvz = f(vz)\n",
    "            fvz = fvz.cpu().numpy()\n",
    "    else:\n",
    "        #function evals\n",
    "        fvz = f(vz)\n",
    "    return fvz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2fd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "fig.suptitle(f\"Ground Truth vs. Models, $r = {r}$, Projected onto True Subspace\")\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "z = np.linspace(-0.1,0.1,500)\n",
    "#plotting ground truth\n",
    "ax.plot(z,g(z.reshape(1,-1)),label=\"Ground Truth\")\n",
    "#plotting models\n",
    "for rownum,row in bestLres[(bestLres.r ==1)*(bestLres.sigma ==0)*(bestLres.n >= 1024)].iterrows():\n",
    "    ax.plot(z,onedimproj(row.Model,v,z),label=rf\"$\\sigma={row.sigma}, n={row.n}$\",alpha=0.5)\n",
    "plt.legend()\n",
    "plt.savefig(f\"ooderrors_onedimvizr{r}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "fig.suptitle(f\"Ground Truth - Models, $r = {r}$, Projected onto True Subspace\")\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "z = np.linspace(-0.4,0.4,500)\n",
    "#plotting models\n",
    "for rownum,row in bestLres[(bestLres.r ==1)*(bestLres.sigma ==0)*(bestLres.n >= 1024)].iterrows():\n",
    "    ax.plot(z,onedimproj(row.Model,v,z)[:,0]-g(z.reshape(1,-1)),label=rf\"$\\sigma={row.sigma}, n={row.n}$\",alpha=0.5)\n",
    "plt.legend()\n",
    "plt.savefig(f\"ooderrors_onedimvizr{r}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81548123",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "fig.suptitle(f\"Ground Truth - Models, $r = {r}$, Projected onto True Subspace\")\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "z = np.linspace(-3,3,500)\n",
    "#plotting models\n",
    "for rownum,row in bestLres[(bestLres.r ==1)*(bestLres.sigma ==0)*(bestLres.n >= 1024)].iterrows():\n",
    "    ax.plot(z,onedimproj(row.Model,v,z)[:,0]-g(z.reshape(1,-1)),label=rf\"$\\sigma={row.sigma}, n={row.n}$\",alpha=0.5)\n",
    "plt.legend()\n",
    "plt.savefig(f\"ooderrors_onedimvizr{r}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da7123",
   "metadata": {},
   "source": [
    "plot of projetion onto true 1d subspace of both models and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projviz(ax,f,d=20,domain=None,projseed=0,istorch=True,verbose=False,kind=\"surface\",**kwargs):\n",
    "    \"\"\"\n",
    "    Plot a projection of the function onto 2D inputs. More specifically, plot visualization of a \n",
    "    function f with d dimensional input and 1 dimension output by plotting f(Az) where A is a \n",
    "    random d x 2 matrix with orthonormal columns and z is a 2-dimensional meshgrid.\n",
    "    \"\"\"\n",
    "    #create a random projection matrix A\n",
    "    A = ortho_group(d,seed=projseed).rvs()[:,:2]\n",
    "    Asupnorm = np.linalg.norm(A,ord=np.inf)\n",
    "    if domain is None:\n",
    "        domain = np.linspace(-1,1,500)\n",
    "        # domain = np.linspace(-2/Asupnorm,2/Asupnorm,500)\n",
    "    if verbose: print(\"random projection d x 2:\\n\",A)\n",
    "    #create 2D meshgrid of domain\n",
    "    Z1,Z2 = np.meshgrid(domain,domain)\n",
    "    numpnts = len(domain)\n",
    "    #create points Az at which to evaluate f\n",
    "    AZ = np.outer(A[:,0], Z1) + np.outer(A[:,1], Z2)\n",
    "    if istorch:\n",
    "        AZ = np.float32(AZ)\n",
    "        AZ = torch.from_numpy(AZ).to(device).T\n",
    "        #function evals\n",
    "        with torch.no_grad():\n",
    "            fAZ = f(AZ)\n",
    "            fAZ = fAZ.cpu().numpy()\n",
    "    else:\n",
    "        #function evals\n",
    "        fAZ = f(AZ)\n",
    "    fAZ  = fAZ.reshape(numpnts,numpnts)\n",
    "    #plotting -- using a surface plot, a wireframe plot, or a contour plot\n",
    "    if kind==\"surface\":\n",
    "        ax.plot_surface(Z1,Z2,fAZ,**kwargs)\n",
    "    elif kind==\"wire\":\n",
    "        ax.plot_wireframe(Z1,Z2,fAZ,**kwargs)\n",
    "    elif kind==\"contour\":\n",
    "        ax.contour(Z1,Z2,fAZ,**kwargs)\n",
    "    elif kind==\"contourf\":\n",
    "        ax.contourf(Z1,Z2,fAZ,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a789a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for projseed in range(1,4):\n",
    "    fig = plt.figure(figsize = (20,10))\n",
    "    fig.suptitle(f\"Ground Truth vs. Models, $r = {r}$, Projection {projseed}\")\n",
    "\n",
    "    # kind = \"surface\"\n",
    "    # #plotting ground truth\n",
    "    # ax = fig.add_subplot(1,3,1,projection='3d')\n",
    "    # projviz(ax,f,istorch=False,kind=kind,projseed=projseed)\n",
    "    # ax.set_title(f\"Ground Truth\")\n",
    "    # #plotting models\n",
    "    # subplot=2\n",
    "    # for rownum,row in bestLres[(bestLres.r ==1)*(bestLres.sigma ==0)*(bestLres.n >= 1024)].iterrows():\n",
    "    #     ax = fig.add_subplot(1,3,subplot,projection='3d')\n",
    "    #     subplot += 1\n",
    "    #     projviz(ax,row.Model,kind=kind,projseed=projseed)\n",
    "    #     ax.set_title(rf\"$\\sigma={row.sigma}, n={row.n}$\")\n",
    "    # plt.show()\n",
    "\n",
    "    kind = \"contour\"\n",
    "    fig = plt.figure(figsize = (20,6))\n",
    "    fig.suptitle(f\"Ground Truth vs. Models, $r = {r}$, Projection {projseed}\")\n",
    "    #plotting ground truth\n",
    "    ax = fig.add_subplot(1,3,1)\n",
    "    projviz(ax,f,istorch=False,kind=kind,projseed=projseed,levels=100)\n",
    "    ax.set_title(f\"Ground Truth\")\n",
    "    #plotting models\n",
    "    subplot=2\n",
    "    for rownum,row in bestLres[(bestLres.r ==1)*(bestLres.sigma ==0)*(bestLres.n >= 1024)].iterrows():\n",
    "        ax = fig.add_subplot(1,3,subplot)\n",
    "        subplot += 1\n",
    "        projviz(ax,row.Model,kind=kind,projseed=projseed,levels=100)\n",
    "        ax.set_title(rf\"$\\sigma={row.sigma}, n={row.n}$\")\n",
    "    plt.savefig(f\"ooderrors_projvizr{r}projection{projseed}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc30dc2",
   "metadata": {},
   "source": [
    "## Training v Epoch after Tuning $(\\lambda, L)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.array([0,0.25,0.5,1])\n",
    "f, ax = plt.subplots(nrows=2, ncols=4, sharex=True, sharey=True, figsize=(20,10))\n",
    "plt.figure(figsize=(10,10))\n",
    "for rownum,row in bestLres.iterrows():\n",
    "    sigma = row['sigma']\n",
    "    r = row['r']\n",
    "    whichcol = np.where(sigma == sigmas)[0][0]\n",
    "    whichrow = 0 if r == 1 else 1\n",
    "    ax[whichrow,whichcol].semilogy(row[\"Train MSE\"],label=rf\"$n = {row['n']}$\",linewidth=1,alpha=0.7)\n",
    "    ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},\\sigma = {sigma}$\")\n",
    "    ax[whichrow,whichrow].set_xlabel(\"Epoch\")\n",
    "ax[0,0].legend()\n",
    "f.suptitle(rf\"Train MSE v Epoch (Tuned $\\lambda,L$)\")\n",
    "f.savefig(job_name+f\"trainmse.png\",dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.array([0,0.25,0.5,1])\n",
    "f, ax = plt.subplots(nrows=2, ncols=4, sharex=True, sharey=True, figsize=(20,10))\n",
    "plt.figure(figsize=(10,10))\n",
    "for rownum,row in bestLres.iterrows():\n",
    "    sigma = row['sigma']\n",
    "    r = row['r']\n",
    "    whichcol = np.where(sigma == sigmas)[0][0]\n",
    "    whichrow = 0 if r == 1 else 1\n",
    "    ax[whichrow,whichcol].semilogy(row[\"Weight Decay\"],label=rf\"$n = {row['n']}$\",linewidth=1,alpha=0.7)\n",
    "    ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},\\sigma = {sigma}$\")\n",
    "    ax[whichrow,whichrow].set_xlabel(\"Epoch\")\n",
    "ax[0,0].legend()\n",
    "f.suptitle(rf\"Weight Decay v Epoch (Tuned $\\lambda,L$)\")\n",
    "f.savefig(job_name+f\"weightdecay.png\",dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

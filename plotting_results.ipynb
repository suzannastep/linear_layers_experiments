{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b21cba",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#imports\" data-toc-modified-id=\"imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>imports</a></span></li><li><span><a href=\"#load-data\" data-toc-modified-id=\"load-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#read-in-the-files\" data-toc-modified-id=\"read-in-the-files-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>read in the files</a></span><ul class=\"toc-item\"><li><span><a href=\"#data-from-linear-and-relu-activations\" data-toc-modified-id=\"data-from-linear-and-relu-activations-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>data from linear and relu activations</a></span></li></ul></li><li><span><a href=\"#create-pandas-table\" data-toc-modified-id=\"create-pandas-table-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>create pandas table</a></span></li></ul></li><li><span><a href=\"#filter-out-bad-training-losses\" data-toc-modified-id=\"filter-out-bad-training-losses-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>filter out bad training losses</a></span></li><li><span><a href=\"#determine-the-lambda-parameter-that-gets-the-best-validation-MSE-for-each-(r,n,L)\" data-toc-modified-id=\"determine-the-lambda-parameter-that-gets-the-best-test-MSE-for-each-(r,n,L)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>determine the lambda parameter that gets the best test MSE for each (r,n,L)</a></span></li><li><span><a href=\"#Generalization-MSE\" data-toc-modified-id=\"Generalization-MSE-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Generalization MSE</a></span><ul class=\"toc-item\"><li><span><a href=\"#generate-data\" data-toc-modified-id=\"generate-data-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>generate data</a></span></li><li><span><a href=\"#compute-MSE\" data-toc-modified-id=\"compute-MSE-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>compute MSE</a></span></li></ul></li><li><span><a href=\"#Out-of-Distribution-MSE\" data-toc-modified-id=\"Out-of-Distribution-MSE-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Out of Distribution MSE</a></span><ul class=\"toc-item\"><li><span><a href=\"#generate-data\" data-toc-modified-id=\"generate-data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>generate data</a></span></li><li><span><a href=\"#compute-MSE\" data-toc-modified-id=\"compute-MSE-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>compute MSE</a></span></li></ul></li><li><span><a href=\"#Active-Subspace\" data-toc-modified-id=\"Active-Subspace-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Active Subspace</a></span><ul class=\"toc-item\"><li><span><a href=\"#evaluate-gradients-and-compute-singular-values-and-active-subspaces\" data-toc-modified-id=\"evaluate-gradients-and-compute-singular-values-and-active-subspaces-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>evaluate gradients and compute singular values and active subspaces</a></span></li><li><span><a href=\"#plot-of-singular-values\" data-toc-modified-id=\"plot-of-singular-values-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>plot of singular values</a></span></li></ul></li><li><span><a href=\"#determine-the-L-parameter-that-gets-the-best-test-MSE-for-each-(r,n)\" data-toc-modified-id=\"determine-the-L-parameter-that-gets-the-best-test-MSE-for-each-(r,n)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>determine the L parameter that gets the best test MSE for each (r,n)</a></span></li><li><span><a href=\"#Plots-of-L-vs-Test-error-and-n-vs-Generalization-metrics-with/without-linear-layers\" data-toc-modified-id=\"Plots-of-L-vs-Validation-error-and-n-vs-Generalization-metrics-with/without-linear-layers-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Plots of L vs Test error and n vs Generalization metrics with/without linear layers</a></span></li><li><span><a href=\"#Final-Table\" data-toc-modified-id=\"Final-Table-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Final Table</a></span></li><li><span><a href=\"#Training-Time-Plots\" data-toc-modified-id=\"Training-Time-Plots-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Training Time Plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-MSE-v-Epoch\" data-toc-modified-id=\"Train-MSE-v-Epoch-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Train MSE v Epoch</a></span></li><li><span><a href=\"#Weight-Decay-v-Epoch\" data-toc-modified-id=\"Weight-Decay-v-Epoch-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Weight Decay v Epoch</a></span></li><li><span><a href=\"#learning-rates\" data-toc-modified-id=\"learning-rates-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>learning rates</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bF8A36cglU4",
   "metadata": {
    "id": "4bF8A36cglU4"
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca449a",
   "metadata": {
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1701128286405,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "2cca449a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.stats import linregress\n",
    "from scipy import linalg as la\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import sem\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21725609",
   "metadata": {
    "id": "21725609"
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aHdpcWWgi3RZ",
   "metadata": {
    "id": "aHdpcWWgi3RZ"
   },
   "source": [
    "## read in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2Q_uTMFlmlR",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "s2Q_uTMFlmlR"
   },
   "outputs": [],
   "source": [
    "rnvals = [(1,64),(1,128),(1,256),(1,512)]#,(1,1024),(1,2048),\n",
    "          #(2,64),(2,128),(2,256),(2,512),(2,1024),(2,2048),\n",
    "          #(5,64),(5,128),(5,256),(5,512),(5,1024),(5,2048),]\n",
    "Ls = [2,3,4,5,6,7,8,9]\n",
    "rs = [1]#,2,5]\n",
    "wds = [1e-3,1e-4,1e-5]\n",
    "labelnoise = [0,0.25]#,0.5,1]\n",
    "epochs = 60100\n",
    "job_name = \"middlelinear_SIM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d588a3c2",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "d588a3c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testMSEs = {}\n",
    "trainMSEs = {}\n",
    "weightdecays = {}\n",
    "learningrates = {}\n",
    "files_found_list = []\n",
    "for r,n in rnvals:\n",
    "    for L in Ls:\n",
    "        for wd in wds:\n",
    "            for ln in labelnoise:\n",
    "                paramname = job_name+f\"_labelnoise{ln}\"+f\"/N{n}_L{L}_r{r}_wd{wd}_epochs{epochs}\"\n",
    "                if os.path.exists(paramname+\"testMSE.npy\"):\n",
    "                    testMSEs[r,n,L,wd,ln] = np.load(paramname+\"testMSE.npy\",allow_pickle=True).item()\n",
    "                    trainMSEs[r,n,L,wd,ln] = np.load(paramname+\"trainMSEs.npy\",allow_pickle=True)\n",
    "                    weightdecays[r,n,L,wd,ln] = np.load(paramname+\"weightdecays.npy\",allow_pickle=True)\n",
    "                    learningrates[r,n,L,wd,ln] = np.load(paramname+\"learningrates.npy\",allow_pickle=True)\n",
    "                    files_found_list.append((r,n,L,wd,ln))\n",
    "                else:\n",
    "                    print(f\"{paramname+'testMSE.npy'} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca93e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files_found_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g92mE6Ljkoq9",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "g92mE6Ljkoq9"
   },
   "outputs": [],
   "source": [
    "def Llayers(L,d=20,width=1000,relus=False,middlelinear=True):\n",
    "    \"\"\"\n",
    "    model class. Construct L-1 linear layers; bias terms only on last linear layer and final relu layer.\n",
    "    \"\"\"\n",
    "    if L < 2:\n",
    "        raise ValueError(\"L must be at least 2\")\n",
    "    if L == 2:\n",
    "        linear_layers = [nn.Linear(d,width,bias=True)]\n",
    "    if L > 2:\n",
    "        linear_layers = [nn.Linear(d,width,bias=False)]\n",
    "        if relus or middlelinear: linear_layers.append(nn.ReLU())\n",
    "        for l in range(L-3):\n",
    "            linear_layers.append(nn.Linear(width,width,bias=False))\n",
    "            if relus: linear_layers.append(nn.ReLU())\n",
    "        linear_layers.append(nn.Linear(width,width,bias=True))\n",
    "\n",
    "    relu = nn.ReLU()\n",
    "\n",
    "    last_layer = nn.Linear(width,1)\n",
    "\n",
    "    layers = linear_layers + [relu,last_layer]\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jl4Oh8Vp4QIb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1701128289117,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "jl4Oh8Vp4QIb",
    "outputId": "8514097d-0db9-4bcd-baf4-42b4faad166a"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4G22AjiIkYNE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1701128292778,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "4G22AjiIkYNE",
    "outputId": "4147ad49-f1a8-410f-c9bb-6822320eed52",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files_found_list = []\n",
    "models = {}\n",
    "for r,n in rnvals:\n",
    "    for L in Ls:\n",
    "        for wd in wds:\n",
    "            for ln in labelnoise:\n",
    "                paramname = job_name+f\"_labelnoise{ln}/N{n}_L{L}_r{r}_wd{wd}_epochs{epochs}\"\n",
    "                if os.path.exists(paramname+\"model.pt\"):\n",
    "                    models[r,n,L,wd,ln] = Llayers(L,width=1000)\n",
    "                    models[r,n,L,wd,ln].to(device)\n",
    "                    if torch.cuda.is_available():\n",
    "                        models[r,n,L,wd,ln].load_state_dict(torch.load(paramname+\"model.pt\"))\n",
    "                    else:\n",
    "                        models[r,n,L,wd,ln].load_state_dict(torch.load(paramname+\"model.pt\"),map_location=torch.device('cpu'))\n",
    "                    models[r,n,L,wd,ln].eval()\n",
    "                    files_found_list.append((r,n,L,wd,ln))\n",
    "                else:\n",
    "                    print(paramname+\"model.pt\",\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101bd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_found_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wt9Aud_elI-7",
   "metadata": {
    "id": "Wt9Aud_elI-7"
   },
   "source": [
    "## create pandas table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8hubUl6NlIh3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "8hubUl6NlIh3",
    "outputId": "008a8a90-61cb-40fb-e1fe-d5a2df77ddb8"
   },
   "outputs": [],
   "source": [
    "res = {\n",
    "  \"r\"                                    : [r                          for r,n,L,wd,ln in files_found_list],# + [r                         for wd in wds for r,n in relu_rnvals],\n",
    "  \"sigma\"                                : [ln                         for r,n,L,wd,ln in files_found_list],# + [ln                        for wd in wds for r,n in relu_rnvals],\n",
    "  \"n\"                                    : [n                          for r,n,L,wd,ln in files_found_list],# + [n                         for wd in wds for r,n in relu_rnvals],\n",
    "  \"L\"                                    : [L                          for r,n,L,wd,ln in files_found_list],# + [4                         for wd in wds for r,n in relu_rnvals],\n",
    "  \"lambda\"                               : [wd                         for r,n,L,wd,ln in files_found_list],# + [wd                        for wd in wds for r,n in relu_rnvals],\n",
    "  \"Learning Rate\"                        : [learningrates[r,n,L,wd,ln] for r,n,L,wd,ln in files_found_list],# + [RELUlearningrates[r,n][4][wd] for wd in wds for r,n in relu_rnvals],\n",
    "  \"Train MSE\"                            : [trainMSEs[r,n,L,wd,ln]     for r,n,L,wd,ln in files_found_list],# + [RELUtrainMSEs[r,n][4][wd]     for wd in wds for r,n in relu_rnvals],\n",
    "  \"Weight Decay\"                         : [weightdecays[r,n,L,wd,ln]  for r,n,L,wd,ln in files_found_list],# + [RELUweightdecays[r,n][4][wd]  for wd in wds for r,n in relu_rnvals],\n",
    "  \"Model\"                                : [models[r,n,L,wd,ln]        for r,n,L,wd,ln in files_found_list],# + [RELUmodels[r,n,4,wd]          for wd in wds for r,n in relu_rnvals],\n",
    "  \"Test MSE\"                             : [testMSEs[r,n,L,wd,ln]      for r,n,L,wd,ln in files_found_list],# + [RELUtestMSEs[r,n][4][wd].item()      for wd in wds for r,n in relu_rnvals],\n",
    "  \"Activations\"                          : [\"relu\"          for r,n,L,wd,ln in files_found_list],# + [\"relu only\"         for wd in wds for r,n in relu_rnvals]\n",
    "}\n",
    "res = pd.DataFrame(res)\n",
    "res[\"Final Train MSE\"] = [r[-1] for r in res[\"Train MSE\"]]\n",
    "res[\"Final Weight Decay\"] = [r[-1] for r in res[\"Weight Decay\"]]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2d3b4",
   "metadata": {},
   "source": [
    "# filter out bad training losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMSE_threshold = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res[\"Final Train MSE\"] >= trainMSE_threshold + res[\"sigma\"]] #TODO is this reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aX7gRNo_xet",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "4aX7gRNo_xet"
   },
   "outputs": [],
   "source": [
    "res = res[res[\"Final Train MSE\"] < trainMSE_threshold + res[\"sigma\"]]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96588e81",
   "metadata": {},
   "source": [
    "# generate data function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(datasetsize,r,seed,std,labelnoiseseed,trainsize=2**18,testsize=2**10,d=20,funcseed=42,verbose=False,ood=False):\n",
    "\n",
    "    ##Generate data with a true central subspaces of varying dimensions\n",
    "    #generate X values for training and test sets\n",
    "    np.random.seed(seed) #set seed for data generation\n",
    "    trainX = np.random.rand(d,trainsize).astype(np.float32)[:,:datasetsize] - 0.5 #distributed as U[-1/2, 1/2]\n",
    "    testX = np.random.rand(d,testsize).astype(np.float32) - 0.5 #distributed as U[-1/2, 1/2]\n",
    "    #out of distribution datagen\n",
    "    if ood:\n",
    "        trainX *= 2 #now distributed as U[-1, 1]\n",
    "        testX *= 2 #now distributed as U[-1, 1]\n",
    "    ##for each $r$ value create and store data-gen functions and $y$ evaluations\n",
    "    #geneate params for functions\n",
    "    k = d+1\n",
    "    U = np.load(job_name+f\"_labelnoise{std}/r{r}U.npy\")\n",
    "    Sigma = np.load(job_name+f\"_labelnoise{std}/r{r}Sigma.npy\")\n",
    "    V = np.load(job_name+f\"_labelnoise{std}/r{r}V.npy\")\n",
    "    A = np.load(job_name+f\"_labelnoise{std}/r{r}A.npy\")\n",
    "    B = np.load(job_name+f\"_labelnoise{std}/r{r}B.npy\")\n",
    "    #create functions\n",
    "    np.random.seed(labelnoiseseed) #set seed for data generation\n",
    "    def g(z): #active subspace function\n",
    "        hidden_layer = (U*Sigma)@z\n",
    "        hidden_layer = hidden_layer.T + B\n",
    "        hidden_layer = np.maximum(0,hidden_layer).T\n",
    "        return A@hidden_layer\n",
    "    def f(x): #teacher network\n",
    "        z = V.T@x    \n",
    "        eps = std*np.random.randn(x.shape[1])    \n",
    "        return g(z) + eps\n",
    "    #generate data\n",
    "    trainY = f(trainX).astype(np.float32)\n",
    "    testY = f(testX).astype(np.float32)\n",
    "    #move data to device\n",
    "    if verbose:\n",
    "        print(\"device: {}\".format(device))\n",
    "    trainX = torch.from_numpy(trainX).T.to(device)\n",
    "    trainY = torch.from_numpy(trainY).to(device)\n",
    "    testX = torch.from_numpy(testX).T.to(device)\n",
    "    testY = torch.from_numpy(testY).to(device)\n",
    "    if verbose:\n",
    "        print(\"trainX shape = {} trainY shape = {}\".format(\n",
    "            trainX.shape,\n",
    "            trainY.shape\n",
    "        ))\n",
    "    return trainX,trainY,testX,testY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ed4f3",
   "metadata": {},
   "source": [
    "# Validation MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf426e6",
   "metadata": {},
   "source": [
    "\n",
    "## generate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36305fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "validationY = {}\n",
    "\n",
    "validationsize = 2048\n",
    "for r in rs:\n",
    "    for k,std in enumerate(labelnoise):\n",
    "        labelnoiseseed = 686 + k\n",
    "        datagenseed = 1107\n",
    "        print(\"validation size =\",validationsize,\"r =\",r,\"label noise std =\",std,\"label noise seed =\",labelnoiseseed)\n",
    "        validationX,validationY[r,std] = gen_data(datasetsize=validationsize,r=r,seed=datagenseed,std=std,labelnoiseseed=labelnoiseseed)[:2]\n",
    "validationX.min(),validationX.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e5647",
   "metadata": {},
   "source": [
    "## compute squared errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c299da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    validation = []\n",
    "    normalized_validation = []\n",
    "    squared_errors = []\n",
    "    validation_sem = []\n",
    "    validation_std = []\n",
    "    for rownum, row in res.iterrows():\n",
    "        print(row)\n",
    "        std = row[\"sigma\"]\n",
    "        predY = row[\"Model\"](validationX)\n",
    "        squared_err = (predY[:,0] - validationY[row[\"r\"],std])**2\n",
    "        squared_err = squared_err.cpu().numpy()\n",
    "        mse = nn.functional.mse_loss(predY[:,0],validationY[row[\"r\"],std]).item()\n",
    "        assert np.isclose(mse,np.mean(squared_err))\n",
    "        validation.append(mse)\n",
    "        sem_sqared_err = sem(squared_err)\n",
    "        validation_sem.append(sem_sqared_err)\n",
    "        std_sqared_err = np.std(squared_err)\n",
    "        validation_std.append(std_sqared_err)\n",
    "        if std > 0:\n",
    "            normalized_validation.append(mse/(std**2))\n",
    "        else:\n",
    "            normalized_validation.append(np.nan)\n",
    "        squared_errors.append(squared_err)\n",
    "    res[\"Validation MSE\"] = validation\n",
    "    res[\"Validation MSE$/\\sigma^2$\"] = normalized_validation\n",
    "    res[\"Validation Squared Errors\"] = squared_errors\n",
    "    res[\"Validation SEM\"] = validation_sem\n",
    "    res[\"Validation STD of Squared Errors\"] = validation_std\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f6014",
   "metadata": {},
   "source": [
    "# In-Distribution Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf53019",
   "metadata": {},
   "source": [
    "## generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pNH8hcsX35vb",
   "metadata": {
    "id": "pNH8hcsX35vb"
   },
   "outputs": [],
   "source": [
    "generalizationY = {}\n",
    "generalizationsize = 2048\n",
    "for r in rs:\n",
    "    for k,std in enumerate(labelnoise):\n",
    "        labelnoiseseed = 743 + k\n",
    "        datagenseed = 555\n",
    "        print(\"generalization size =\",generalizationsize,\"r =\",r,\"label noise std =\",std,\"label noise seed =\",labelnoiseseed)\n",
    "        generalizationX,generalizationY[r,std] = gen_data(datasetsize=generalizationsize,r=r,seed=datagenseed,std=std,labelnoiseseed=labelnoiseseed)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SGjV4q985lrM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1691614314236,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 300
    },
    "id": "SGjV4q985lrM",
    "outputId": "20b86ce8-f70f-4eec-d2cc-4c6313a4bfa0"
   },
   "outputs": [],
   "source": [
    "generalizationX.min(),generalizationX.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iHCTTCWw4l4s",
   "metadata": {
    "id": "iHCTTCWw4l4s"
   },
   "source": [
    "## compute squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulWXCTcd2_Xo",
   "metadata": {
    "id": "ulWXCTcd2_Xo"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generalization = []\n",
    "    normalized_generalization = []\n",
    "    squared_errors = []\n",
    "    generalization_sem = []\n",
    "    generalization_std = []\n",
    "    for rownum, row in res.iterrows():\n",
    "        std = row[\"sigma\"]\n",
    "        predY = row[\"Model\"](generalizationX)\n",
    "        squared_err = (predY[:,0] - generalizationY[row[\"r\"],std])**2\n",
    "        squared_err = squared_err.cpu().numpy()\n",
    "        mse = nn.functional.mse_loss(predY[:,0],generalizationY[row[\"r\"],std]).item()\n",
    "        assert np.isclose(mse,np.mean(squared_err))\n",
    "        generalization.append(mse)\n",
    "        sem_sqared_err = sem(squared_err)\n",
    "        generalization_sem.append(sem_sqared_err)\n",
    "        std_sqared_err = np.std(squared_err)\n",
    "        generalization_std.append(std_sqared_err)\n",
    "        if std > 0:\n",
    "            normalized_generalization.append(mse/(std**2))\n",
    "        else:\n",
    "            normalized_generalization.append(np.nan)\n",
    "        squared_errors.append(squared_err)\n",
    "    res[\"In-Distribution Generalization\"] = generalization\n",
    "    res[\"In-Distribution Generalization$/\\sigma^2$\"] = normalized_generalization\n",
    "    res[\"In-Distribution Squared Errors\"] = squared_errors\n",
    "    res[\"In-Distribution SEM\"] = generalization_sem\n",
    "    res[\"In-Distribution STD of Squared Errors\"] = generalization_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LbkhZQcT8Z4S",
   "metadata": {
    "id": "LbkhZQcT8Z4S"
   },
   "source": [
    "# Out-of-Distribution Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GEplm-eG8g74",
   "metadata": {
    "id": "GEplm-eG8g74"
   },
   "source": [
    "## generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc4f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "oodY = {}\n",
    "oodsize = 2048\n",
    "for r in rs:\n",
    "    for k,std in enumerate(labelnoise):\n",
    "        labelnoiseseed = 235 + k\n",
    "        datagenseed = 333\n",
    "        print(\"ood size =\",oodsize,\"r =\",r,\"label noise std =\",std,\"label noise seed =\",labelnoiseseed)\n",
    "        oodX,oodY[r,std] = gen_data(datasetsize=oodsize,r=r,seed=datagenseed,std=std,labelnoiseseed=labelnoiseseed,ood=True)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SiXhFaOE8n4j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1691614317198,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 300
    },
    "id": "SiXhFaOE8n4j",
    "outputId": "cf4681fa-710e-4e13-b705-78939987d20c"
   },
   "outputs": [],
   "source": [
    "oodX.min(),oodX.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yt15AMGL84jk",
   "metadata": {
    "id": "yt15AMGL84jk"
   },
   "source": [
    "## compute squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ood = []\n",
    "    normalized_ood = []\n",
    "    squared_errors = []\n",
    "    ood_sem = []\n",
    "    ood_std = []\n",
    "    for rownum, row in res.iterrows():\n",
    "        std = row[\"sigma\"]\n",
    "        predY = row[\"Model\"](oodX)\n",
    "        squared_err = (predY[:,0] - oodY[row[\"r\"],std])**2\n",
    "        squared_err = squared_err.cpu().numpy()\n",
    "        mse = nn.functional.mse_loss(predY[:,0],oodY[row[\"r\"],std]).item()\n",
    "        assert np.isclose(mse,np.mean(squared_err))\n",
    "        ood.append(mse)\n",
    "        sem_sqared_err = sem(squared_err)\n",
    "        ood_sem.append(sem_sqared_err)\n",
    "        std_sqared_err = np.std(squared_err)\n",
    "        ood_std.append(std_sqared_err)\n",
    "        if std > 0:\n",
    "            normalized_ood.append(mse/(std**2))\n",
    "        else:\n",
    "            normalized_ood.append(np.nan)\n",
    "        squared_errors.append(squared_err)\n",
    "    res[\"Out-of-Distribution Generalization\"] = ood\n",
    "    res[\"Out-of-Distribution Generalization$/\\sigma^2$\"] = normalized_ood\n",
    "    res[\"Out-of-Distribution Squared Errors\"] = squared_errors\n",
    "    res[\"Out-of-Distribution SEM\"] = ood_sem\n",
    "    res[\"Out-of-Distribution STD of Squared Errors\"] = ood_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c9fb6",
   "metadata": {},
   "source": [
    "# Check that most or all ReLU hyperplanes intersect the support of the distributions of the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in rs:\n",
    "    print(\"r =\",r)\n",
    "    U = np.load(job_name+f\"_labelnoise0/r{r}U.npy\")\n",
    "    Sigma = np.load(job_name+f\"_labelnoise0/r{r}Sigma.npy\")\n",
    "    V = np.load(job_name+f\"_labelnoise0/r{r}V.npy\")\n",
    "    A = np.load(job_name+f\"_labelnoise0/r{r}A.npy\")\n",
    "    B = np.load(job_name+f\"_labelnoise0/r{r}B.npy\")\n",
    "    W = (U*Sigma)@V.T\n",
    "    rowwise1norms = np.linalg.norm(W,axis=1,ord=1)\n",
    "    ratios = np.abs(B) / np.linalg.norm(W,axis=1,ord=1)\n",
    "    rowwise2norms = np.linalg.norm(W,axis=1,ord=2)\n",
    "    units = pd.DataFrame({\"R2-cost contribution\":np.abs(A)*rowwise2norms,\"|b| / ||w||_1\":ratios})\n",
    "    # units[\"[-1/2,1/2]\"] = ratios <= 1/2\n",
    "    # units[\"[-1,1] but not [-1/2,1/2]\"] = (1/2 < ratios) * (ratios <= 1)\n",
    "    # units[\"not [-1,1]\"] = ratios > 1\n",
    "\n",
    "    datasetsize = 2048\n",
    "    trainX = gen_data(datasetsize=datasetsize,r=r,seed=1,std=0,labelnoiseseed=0)[0]\n",
    "    for n in res.n.unique():\n",
    "        units[f\"# training active,n={n}\"] = ((W@trainX[:n].cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "        units[f\"% training active,n={n}\"] = units[f\"# training active,n={n}\"] / n\n",
    "    units[\"# validation active\"] = ((W@validationX.cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "    units[\"% validation active\"] = units[\"# validation active\"] / datasetsize\n",
    "    units[\"# generalization active\"] = ((W@generalizationX.cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "    units[\"% generalization active\"] = units[\"# generalization active\"] / datasetsize\n",
    "    units[\"# ood active\"] = ((W@oodX.cpu().numpy().T).T + B > 0).sum(axis=0)\n",
    "    units[\"% ood active\"] = units[\"# ood active\"] / datasetsize\n",
    "\n",
    "    print(\"\\nTOTALS:\\n~~~~~~~\\n\",units.sum())\n",
    "    print(\"\\nunit-wise table:\\n~~~~~~~\\n\")\n",
    "    display(units)\n",
    "    shift = -0.2\n",
    "    width = 0.2\n",
    "    for n in res.n.unique():\n",
    "        if n >= np.max(res.n.unique())/2:\n",
    "            plt.bar(units.index+shift,units[f\"% training active,n={n}\"],label=f\"train,$n={n}$\",width=width,tick_label=units[\"R2-cost contribution\"].round(1))\n",
    "            shift += width\n",
    "    # plt.bar(units.index,units[\"% generalization active\"],label=\"gen\",width=width)\n",
    "    plt.bar(units.index+shift,units[\"% ood active\"],label=\"ood\",width=width)\n",
    "    plt.ylim(0,1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"How many samples is each unit active on? $r =$ {r}\")\n",
    "    plt.xlabel(\"ReLU Unit, labeled by R2-cost contribution\")\n",
    "    plt.ylabel(\"Proportion of samples\")\n",
    "    plt.axhline(0.5,linestyle=\":\",color=\"k\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"samples_active_by_unit_r{r}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec05e2a",
   "metadata": {
    "id": "8ec05e2a"
   },
   "source": [
    "# Active Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pFRGq1re-Uzi",
   "metadata": {
    "id": "pFRGq1re-Uzi"
   },
   "source": [
    "## evaluate gradients and compute singular values and active subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0jES0e-cYG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4768,
     "status": "ok",
     "timestamp": 1691614322192,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 300
    },
    "id": "9f0jES0e-cYG",
    "outputId": "5d4441ca-c283-47e7-8957-aaff36383b6a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grads = []\n",
    "sv = []\n",
    "active_subspace = []\n",
    "subspace_dist = []\n",
    "\n",
    "for rownum, row in res.iterrows():\n",
    "    #compute ground truth active subspace\n",
    "    funcseed = 42\n",
    "    d = 20\n",
    "    k = d+1\n",
    "    ln = row['sigma']\n",
    "    if int(ln) == ln:\n",
    "        ln = int(ln)\n",
    "    U = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}U.npy\")\n",
    "    Sigma = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}Sigma.npy\")\n",
    "    V = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}V.npy\")\n",
    "    W = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}W.npy\")\n",
    "    A = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}A.npy\")\n",
    "    B = np.load(job_name+f\"_labelnoise{ln}/r{row['r']}B.npy\")\n",
    "\n",
    "    #evaluate gradients\n",
    "    generalizationX.requires_grad = True\n",
    "    predY = row[\"Model\"](generalizationX)\n",
    "    grad = torch.autograd.grad(predY, generalizationX,\n",
    "                            grad_outputs=torch.ones_like(predY),\n",
    "                            create_graph=True)[0].detach().cpu().numpy()\n",
    "    grads.append(grad)\n",
    "    #compute active subspace and singular values\n",
    "    Uhat,Shat,VhatT = np.linalg.svd(grad)\n",
    "    Vhat = VhatT.T[:,:row[\"r\"]] #form the basis for the active subspace\n",
    "    active_subspace.append(Vhat)\n",
    "    sv.append(Shat)\n",
    "\n",
    "    subspace_dist.append(np.linalg.norm(V@V.T - Vhat@Vhat.T,2))\n",
    "\n",
    "res[\"Gradient Evaluations\"] = grads\n",
    "res[\"Gradient Singular Values\"] = sv\n",
    "res[\"Active Subspace\"] = active_subspace\n",
    "res[\"Active Subspace Distance\"] = subspace_dist\n",
    "res[\"Principal Angle (Degrees)\"] = np.degrees(np.arcsin(res[\"Active Subspace Distance\"]))\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297500cd",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QfTgwsYWt0JN",
   "metadata": {
    "id": "QfTgwsYWt0JN"
   },
   "source": [
    "##  determine the lambda parameter that gets the best Validation MSE for each (r,n,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RJvhVj2QsOzz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "RJvhVj2QsOzz",
    "outputId": "f5ab0e61-f93e-4d47-c809-af5ef4d06e06",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validationmse_vs_lambda = res.pivot_table(values=\"Validation MSE\",index = (\"r\",\"sigma\",\"n\",\"L\",\"Activations\"),columns=[\"lambda\"])\n",
    "validationmse_vs_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff1bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestlambda = validationmse_vs_lambda.idxmin(axis=1)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(bestlambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K4ItJEBcjtcZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "K4ItJEBcjtcZ",
    "outputId": "a6ab9ade-ea9d-4b83-b14b-29dd2811b57d"
   },
   "outputs": [],
   "source": [
    "mask = [row[\"lambda\"] == bestlambda[row[\"r\"]][row[\"sigma\"]][row[\"n\"]][row[\"L\"]][row[\"Activations\"]] for rowindex,row in res.iterrows()]\n",
    "res = res[mask]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fabcf44",
   "metadata": {
    "id": "QfTgwsYWt0JN"
   },
   "source": [
    "##  determine the L parameter that gets the best validation MSE for each (r,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc2546",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "RJvhVj2QsOzz",
    "outputId": "f5ab0e61-f93e-4d47-c809-af5ef4d06e06",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validationmse_vs_L = res.pivot_table(values=\"Validation MSE\",index = (\"r\",\"sigma\",\"n\",\"Activations\"),columns=[\"L\"])\n",
    "validationmse_vs_L = validationmse_vs_L.iloc[:,1:]\n",
    "bestL = validationmse_vs_L.idxmin(axis=1)\n",
    "pd.concat((validationmse_vs_L,bestL),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb5ccf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701128292779,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "K4ItJEBcjtcZ",
    "outputId": "a6ab9ade-ea9d-4b83-b14b-29dd2811b57d"
   },
   "outputs": [],
   "source": [
    "mask = [row[\"L\"] == bestL[row[\"r\"]][row[\"sigma\"]][row[\"n\"]][row[\"Activations\"]] for rowindex,row in res.iterrows()]\n",
    "bestLres = res[mask]\n",
    "bestLres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daefd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestLres.sort_values(by=['r','n',\"sigma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f1d0c",
   "metadata": {},
   "source": [
    "## What are the chosen lambda and L for each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestres = pd.concat((res[res[\"L\"] == 2],bestLres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(bestres.loc[:,:\"lambda\"].pivot_table(index=[\"r\",\"sigma\",\"n\",\"L\"],values=[\"lambda\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364946f",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d86878",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontname = \"Times New Roman\"\n",
    "import matplotlib\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ecf870",
   "metadata": {},
   "source": [
    "## plots of all the singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cacc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the singular values\n",
    "for std in labelnoise:\n",
    "    f, ax = plt.subplots(nrows=len(res.r.unique()), ncols=len(res.n.unique()), sharex=True, sharey=True, figsize=(10,2.4))\n",
    "    ax = ax[np.newaxis]\n",
    "    for rownum,row in res.iterrows():\n",
    "        if row['sigma'] == std:\n",
    "            whichrow = np.where(row['r'] == res.r.unique())[0][0]\n",
    "            whichcol = np.where(row['n'] == res.n.unique())[0][0]\n",
    "            print(whichrow,whichcol)\n",
    "            print(rf\"{row['r']},{row['n']},{row['L']}\",row[\"Gradient Singular Values\"]/np.sqrt(2048),whichrow,whichcol)\n",
    "            ax[whichrow,whichcol].semilogy(row[\"Gradient Singular Values\"]/np.sqrt(2048),label=rf\"$L={row['L']}$\",linewidth=1,alpha=0.7,marker=\".\")\n",
    "            ax[whichrow,whichcol].set_xticks(list(range(3,20,4)),list(range(4,21,4)))\n",
    "            ax[whichrow,whichcol].set_ylim(10**-9,10**3)\n",
    "            ax[0,whichcol].set_title(rf\"$n={row['n']}$\")\n",
    "            ax[-1,whichcol].set_xlabel(rf\"Index, $k$\")\n",
    "    plt.subplot(2,len(res.n.unique()),1)\n",
    "    leg = plt.legend()\n",
    "    leg = plt.legend(bbox_to_anchor=(-1, 1))\n",
    "    leg.get_frame().set_edgecolor('b')\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "    plt.subplot(2,len(res.n.unique()),1)\n",
    "    plt.ylabel(r\"$r=1$\"+\"\\n\"+r\"$\\sigma_k(\\hat f;\\rho)$\")\n",
    "    plt.yticks([10**p for p in range(-12,3,2)])\n",
    "    plt.subplot(2,len(res.n.unique()),len(res.n.unique())+1)\n",
    "    plt.ylabel(r\"$r=2$\"+\"\\n\"+r\"$\\sigma_k(\\hat f;\\rho)$\")\n",
    "    plt.yticks([10**p for p in range(-12,3,2)])\n",
    "    plt.suptitle(rf\"Singular Values of Trained Networks, $\\sigma =$ {std}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(job_name+f\"_labelnoise{std}/sv.pdf\",dpi=300)#,bbox_extra_artists=(leg,), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vKIELwC_GeZK",
   "metadata": {
    "id": "vKIELwC_GeZK"
   },
   "source": [
    "## Plots of L vs Validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gvOUGojKGWvw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 1932,
     "status": "ok",
     "timestamp": 1701129174191,
     "user": {
      "displayName": "Suzanna Parkinson",
      "userId": "17585917766009932288"
     },
     "user_tz": 360
    },
    "id": "gvOUGojKGWvw",
    "outputId": "378581a5-d4b5-4eed-8e60-29b5134124ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for std in res[\"sigma\"].unique():\n",
    "    f, ax = plt.subplots(ncols=len(res.r.unique()),nrows=1, sharex=True, sharey=False, figsize=(10,4.8))\n",
    "    ax = [ax]\n",
    "    for rnum,r in enumerate(res.r.unique()):\n",
    "        for n in res.n.unique():\n",
    "                res_rnstd = res[(res.r == r) * (res.n == n) * (res[\"sigma\"] == std)]\n",
    "                ax[rnum].scatter(res_rnstd.L,res_rnstd[[\"Validation MSE\"]])\n",
    "                ax[rnum].semilogy(res_rnstd.L,res_rnstd[[\"Validation MSE\"]],label=rf\"$n={n}$\")\n",
    "                for _,model in res_rnstd.iterrows():\n",
    "                    text = rf'$\\lambda = {model[\"lambda\"]:.0e}$' + f'\\nfit {model[\"Final Train MSE\"]:.1e}\\nwd{model[\"Final Weight Decay\"]:.1e}'\n",
    "                    # ax[rnum].annotate(text,[model.L,model[[\"Validation MSE\"]]],fontsize=1)\n",
    "        ax[rnum].set_xlabel(\"$L$ number of layers\")\n",
    "        ax[rnum].set_title(rf\"$r={r}$\")\n",
    "        if std > 0:\n",
    "            ax[rnum].axhline(y=std**2, color='k', linestyle=':',label=\"$\\sigma^2$\")\n",
    "        ax[0].set_ylabel(\"Validation MSE\")\n",
    "        f.suptitle(rf\"Validation MSE for best $\\lambda$ values, $\\sigma$ = {std}\")\n",
    "    ax[0].legend()\n",
    "    f.tight_layout()\n",
    "    if int(std) == std:\n",
    "        std = int(std)\n",
    "    f.savefig(job_name+f\"_labelnoise{std}/ValidationMSE.pdf\",dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad12a9",
   "metadata": {},
   "source": [
    "## Performance metrics with/without linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53158aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columnwidth = 6.17406722223\n",
    "markersize = 4\n",
    "colors = {  \n",
    "    0   :\"C0\",\n",
    "    0.25:\"C1\",\n",
    "    # 0.5 :\"C2\",\n",
    "    # 1   :\"C3\"\n",
    "}\n",
    "labels = {\n",
    "    \"without extra layers\":\"-\",\n",
    "    \"with extra layers\"   :\"--\",\n",
    "    #\"without linear layers\":\"-\",\n",
    "    #\"with linear layers\"   :\"--\",\n",
    "}\n",
    "markers = {\n",
    "    \"without extra layers\":\".\",\n",
    "    \"with extra layers\"   :\"x\",\n",
    "    #\"without linear layers\":\".\",\n",
    "    #\"with linear layers\"   :\"x\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d0a3f",
   "metadata": {},
   "source": [
    "### generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generalization and OOD errors\n",
    "handles = [\n",
    "    Line2D([0], [0], color=color, ls='-', label=rf\"$\\sigma =${sigma}\") for sigma,color in colors.items()\n",
    "]\n",
    "handles += [\n",
    "    Line2D([0], [0], color='k', ls=ls, label=label, marker = markers[label], markersize=markersize) for label,ls in labels.items()\n",
    "] \n",
    "handles += [\n",
    "        Line2D([0], [0], color='k', ls=':', label='$\\sigma^2$, irreducible error'),\n",
    "]\n",
    "\n",
    "for metric in ['In-Distribution Generalization','Out-of-Distribution Generalization']:\n",
    "    standard_errors = metric[:-3] + 'SEM'\n",
    "    f, ax = plt.subplots(ncols=len(res.r.unique()),nrows=2, sharex=True, sharey=\"row\", figsize=(columnwidth,4.25))\n",
    "    ax = ax[:,np.newaxis]\n",
    "    #just the data without label noise in the first row\n",
    "    for col,r in enumerate(res.r.unique()):\n",
    "        for row,sigmas in enumerate([[0],[0.25]]):#[[0],[0.25,0.5,1]]\n",
    "            for sigma in sigmas:\n",
    "                for label,ls in labels.items():\n",
    "                    if label == \"with extra layers\":#\"with linear layers\":\n",
    "                        curr = bestLres[(bestLres.r == r) * (bestLres[\"sigma\"] == sigma)]\n",
    "                    elif label == \"without extra layers\":#\"without linear layers\":\n",
    "                        curr = res[(res.L == 2) * (res.r == r) * (res[\"sigma\"]==sigma)]\n",
    "                    points = curr[[metric]].values[:,0]\n",
    "                    # errorbars = curr[[standard_errors]].values[:,0]\n",
    "                    marker = markers[label]\n",
    "                    ax[row,col].plot(curr.n,points,\n",
    "                                        linestyle=ls,\n",
    "                                        marker=marker,\n",
    "                                        markersize=markersize,\n",
    "                                        color=colors[sigma],\n",
    "                                        alpha=0.8)\n",
    "                    #horizontal dashed line for minimal possible MSE (ie sigma^2) in plots with label noise\n",
    "                    ax[1,col].axhline(y=sigma**2, color=colors[sigma], linestyle=':',alpha=0.3)\n",
    "                    # for (_,model),y in zip(curr.iterrows(),points):\n",
    "                    #     text = rf'$\\lambda = {model[\"lambda\"]:.0e}$' + f'\\nL = {model[\"L\"]}\\nfit {model[\"Final Train MSE\"]:.1e}\\nwd{model[\"Final Weight Decay\"]:.1e}\\nVal{model[\"Validation MSE\"]:.1e}'\n",
    "                    #     ax[row,col].annotate(text,[model[\"n\"],y],fontsize=1)\n",
    "            #plot set up\n",
    "            ax[row,col].set_xscale(\"log\",base=2)\n",
    "            ax[row,col].set_xticks([2**k for k in range(6,12)])\n",
    "            ax[row,col].set_yscale(\"log\",base=10)\n",
    "            ax[row,0].set_ylabel(f\"MSE\",wrap=True)\n",
    "            ax[0,col].set_title(rf\"$r={r}$\") \n",
    "            ax[1,col].set_xlabel(\"Number of training samples ($n$)\")\n",
    "            ax[row,col].minorticks_off()\n",
    "    f.legend(handles=handles, ncol=2, loc = 'upper center', bbox_to_anchor=(0.5,0.03))\n",
    "    plt.suptitle(f\"{metric}\")\n",
    "    plt.tight_layout(pad=0.5,h_pad=1.08, w_pad=1.08)\n",
    "    plt.savefig(job_name+f\"{metric}.pdf\",dpi=300,bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a101f",
   "metadata": {},
   "source": [
    "### singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e301bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = [\n",
    "    Line2D([0], [0], color=color, ls='-', label=rf\"$\\sigma =${sigma}\") for sigma,color in colors.items()\n",
    "]\n",
    "handles += [\n",
    "    Line2D([0], [0], color='k', ls=ls, label=label, marker = markers[label], markersize=markersize) for label,ls in labels.items()\n",
    "] \n",
    "handles += [Line2D([0], [0], color='k', ls=':', label=r\"effective rank tolerance, $\\varepsilon = 10^{-3}$\")]\n",
    "\n",
    "ranktol = 1e-3\n",
    "f, ax = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(columnwidth,5.75))\n",
    "for rnum,r in enumerate(res.r.unique()):\n",
    "        rnum = 0\n",
    "        for nnum,n in enumerate(res.n.unique()):\n",
    "            row = 2*rnum + nnum // 3\n",
    "            col = nnum % 3\n",
    "            print(r,n,row,col)\n",
    "            for sigma in labelnoise:\n",
    "                for label,ls in labels.items():\n",
    "                    if label == \"with extra layers\":#\"with linear layers\":\n",
    "                        curr = bestLres[(bestLres.r == r) * (bestLres.n == n) * (bestLres[\"sigma\"] == sigma)]\n",
    "                    elif label == \"without extra layers\":#\"without linear layers\":\n",
    "                        curr = res[(res.L == 2) * (res.n == n) * (res.r == r) * (res[\"sigma\"]==sigma)]\n",
    "                    marker = markers[label]\n",
    "                    ax[row,col].semilogy(curr[\"Gradient Singular Values\"].values[0]/np.sqrt(2048),\n",
    "                        linestyle=ls,\n",
    "                        linewidth=1,\n",
    "                        alpha=0.7,\n",
    "                        marker=marker,\n",
    "                        markersize=markersize,\n",
    "                        color=colors[sigma])\n",
    "            ax[row,col].axhline(y=ranktol, color='k', linestyle=':',alpha=1, label = r\"effective rank tolerance, $\\varepsilon = 10^{-3}$\")\n",
    "            ax[row,col].set_xticks(list(range(4,20,5)),list(range(5,21,5)))\n",
    "            ax[0,0].set_yticks([10**p for p in range(-9,3,2)])\n",
    "            ax[row,col].set_title(rf\"$r={r},n={n}$\")\n",
    "            ax[-1,col].set_xlabel(rf\"Index, $k$\")\n",
    "            ax[row,col].set_ylim(10**(-9),10**(2.5))\n",
    "f.legend(handles=handles, ncol=2, loc = 'upper center', bbox_to_anchor=(0.5,0.03))\n",
    "plt.suptitle(r\"Singular Values of Trained Networks, $\\sigma_k(\\hat{f};\\rho)$\")\n",
    "plt.tight_layout(pad=0.5,h_pad=1.08, w_pad=1.08)\n",
    "plt.savefig(job_name+f\"_labelnoise_sv.pdf\",dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "handles.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd13005",
   "metadata": {},
   "source": [
    "### active subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = [\n",
    "    Line2D([0], [0], color=color, ls='-', label=rf\"$\\sigma =${sigma}\") for sigma,color in colors.items()\n",
    "]\n",
    "handles += [\n",
    "    Line2D([0], [0], color='k', ls=ls, label=label, marker = markers[label], markersize=markersize) for label,ls in labels.items()\n",
    "] \n",
    "\n",
    "#active subspace error plot\n",
    "f, ax = plt.subplots(ncols=len(res.r.unique()),nrows=2, sharex=True, sharey=False, figsize=(columnwidth,4.25))\n",
    "ax = ax[:,np.newaxis]\n",
    "for row,metric in enumerate([r\"Effective Index Rank, $\\varepsilon = 10^{-3}$\",\"Principal Angle (Degrees)\"]):\n",
    "    for col,r in enumerate(res.r.unique()):\n",
    "        for sigma in [0,0.25]:#,0.5,1]:\n",
    "            for label,ls in labels.items():\n",
    "                if label == \"with extra layers\":#\"with linear layers\":\n",
    "                    curr = bestLres[(bestLres.r == r) * (bestLres[\"sigma\"] == sigma)]\n",
    "                elif label == \"without extra layers\":#\"without linear layers\":\n",
    "                    curr = res[(res.L == 2) * (res.r == r) * (res[\"sigma\"]==sigma)]\n",
    "                if metric == \"Principal Angle (Degrees)\":\n",
    "                    points = curr[[metric]].values[:,0]\n",
    "                elif metric == r\"Effective Index Rank, $\\varepsilon = 10^{-3}$\":\n",
    "                    points = (np.array(curr[\"Gradient Singular Values\"].tolist())/np.sqrt(2048) > ranktol).sum(axis=1)\n",
    "                    ax[row,col].set_yticks(np.arange(0,21,5))\n",
    "                    ax[row,col].set_ylim(0,20.5)\n",
    "                marker = markers[label]\n",
    "                ax[row,col].plot(curr.n,points,\n",
    "                                    linestyle=ls,\n",
    "                                    color=colors[sigma],\n",
    "                                    marker=marker,\n",
    "                                    markersize=markersize,\n",
    "                                    alpha=0.8)\n",
    "                # for (_,model),y in zip(curr.iterrows(),points):\n",
    "                #     text = rf'$\\lambda = {model[\"lambda\"]:.0e}$' + f'\\nL = {model[\"L\"]}\\nfit {model[\"Final Train MSE\"]:.1e}\\nwd{model[\"Final Weight Decay\"]:.1e}\\nVal{model[\"Validation MSE\"]:.1e}'\n",
    "                #     ax[row,col].annotate(text,[model[\"n\"],y],fontsize=1)\n",
    "        #plot set up\n",
    "        ax[row,0].set_ylabel(metric[:15] + '\\n' + metric[16:])\n",
    "        ax[0,col].set_title(rf\"$r={r}$\") \n",
    "        ax[0,col].set_yticks(range(21), minor=True)\n",
    "        ax[row,col].set_xscale(\"log\",base=2)\n",
    "        ax[row,col].set_xticks([2**k for k in range(6,12)])\n",
    "        ax[1,col].set_xlabel(\"Number of training samples ($n$)\")\n",
    "        ax[row,col].minorticks_on()\n",
    "f.legend(handles=handles, ncol=2, loc = 'upper center', bbox_to_anchor=(0.5,0.03))\n",
    "plt.suptitle(f\"Active Subspaces\")\n",
    "plt.tight_layout(pad=0.5,h_pad=0.5, w_pad=0.5)\n",
    "plt.savefig(job_name+f\"Active Subspaces.pdf\",dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc30dc2",
   "metadata": {},
   "source": [
    "## Training v Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.array([0,0.25,0.5,1])\n",
    "for metric in [\"Train MSE\",\"Weight Decay\"]:\n",
    "    f, ax = plt.subplots(nrows=2, ncols=4, sharex=False, sharey='row', figsize=(columnwidth,5))\n",
    "    for rownum,row in bestLres.iterrows():\n",
    "        sigma = row['sigma']\n",
    "        r = row['r']\n",
    "        whichcol = np.where(sigma == sigmas)[0][0]\n",
    "        whichrow = 0 if r == 1 else 1\n",
    "        ax[whichrow,whichcol].semilogy(row[metric],label=rf\"$n = {row['n']}$\",linewidth=1,alpha=0.7)\n",
    "        ax[whichrow,whichcol].set_title(rf\"$r = {row['r']},\\sigma = {sigma}$\")\n",
    "        ax[whichrow,whichcol].set_xlabel(\"Epoch\")\n",
    "        ax[whichrow,whichcol].set_xticks(range(0,60100,20000),labels=[str(i)+\"k\" for i in range(0,61,20)])\n",
    "        handles, labels = ax[whichrow,whichcol].get_legend_handles_labels()\n",
    "        ax[whichrow,whichcol].minorticks_on()\n",
    "    if metric == \"Weight Decay\":\n",
    "        titlestr = rf\"$\\ell_2$-Regularization Term vs. Epoch\"\n",
    "        ystr = rf\"Sum of Squares of Non-Bias Weights\"\n",
    "    else:\n",
    "        titlestr = rf\"{metric} vs. Epoch\"\n",
    "        ystr = \"MSE\"\n",
    "    f.legend(handles, labels, ncol=3, loc = 'upper center', bbox_to_anchor=(0.5,0.03))\n",
    "    f.suptitle(titlestr)\n",
    "    f.supylabel(ystr)\n",
    "    plt.tight_layout(pad=0.5,h_pad=0.5, w_pad=0.5)\n",
    "    f.savefig(job_name+metric+\".pdf\",dpi=300,bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "cluster_startup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
